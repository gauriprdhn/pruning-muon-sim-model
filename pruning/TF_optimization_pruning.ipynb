{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "749722f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-b0bcf385fb82>:29: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. In future versions, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = copy.copy(mpl.cm.get_cmap(\"viridis\"))\n",
      "  my_cmap.set_under('w',1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential, Model,model_from_json\n",
    "from keras.layers import Input,Dense, BatchNormalization, Activation, LeakyReLU\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "from nn_globals import *\n",
    "from nn_encode_displ import nlayers, nvariables\n",
    "from nn_training import train_model\n",
    "from nn_models import load_my_model, update_keras_custom_objects, lr_decay, modelbestcheck_weights, modelbestcheck\n",
    "from keras.models import Model\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Setup matplotlib\n",
    "plt.style.use('tdrstyle.mplstyle')\n",
    "\n",
    "from nn_plotting import (gaus, fit_gaus, np_printoptions, \\\n",
    "                         find_efficiency_errors)\n",
    "\n",
    "eps = 1e-7\n",
    "my_cmap = plt.cm.viridis\n",
    "my_cmap.set_under('w',1)\n",
    "my_palette = (\"#377eb8\", \"#e41a1c\", \"#984ea3\", \"#ff7f00\", \"#4daf4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba50e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluate:\n",
    "    def __init__(self,X_test,y_test):\n",
    "        self.X = X_test\n",
    "        self.y = y_test[0]\n",
    "        self.dxy = y_test[1]\n",
    "    \n",
    "    def compute_data_statistics(self,ctype = \"y\",label=\"data\"):\n",
    "        if ctype == \"y\":\n",
    "            x = self.recalibrate(self.y,reg_pt_scale)\n",
    "            x = x**(-1)\n",
    "        else:\n",
    "            x = self.recalibrate(self.dxy,reg_dxy_scale)\n",
    "        df_describe = pd.DataFrame(x, columns = [label])\n",
    "        print(df_describe.describe())\n",
    "    \n",
    "    def rmse(self,y_true, y_predicted):\n",
    "        assert(y_true.shape[0] == y_predicted.shape[0])\n",
    "        n = y_true.shape[0]\n",
    "        sum_square = np.sum((y_true - y_predicted)**2)\n",
    "        return math.sqrt(sum_square/n)\n",
    "    \n",
    "    def adjusted_r_2(self,y_true, y_predicted):\n",
    "        y_addC = sm.add_constant(y_true)\n",
    "        result = sm.OLS(y_predicted, y_addC).fit()\n",
    "        print(result.rsquared, result.rsquared_adj)\n",
    "\n",
    "    def recalibrate(self,x,scale):\n",
    "        return x/scale\n",
    "    \n",
    "    def inverse(self,arr):\n",
    "        arr_inv = 1./arr\n",
    "        arr_inv[arr_inv == np.inf] = 0.\n",
    "        return arr_inv\n",
    "    \n",
    "    def predict(self,model,batch_size = 256):\n",
    "        y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "        dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "        \n",
    "        y_test = model.predict(self.X,batch_size = 2000)\n",
    "        y_test_meas = y_test[:,0]\n",
    "        dxy_test_meas = y_test[:,1]\n",
    "        y_test_meas = self.recalibrate(y_test_meas,reg_pt_scale)\n",
    "        dxy_test_meas = self.recalibrate(dxy_test_meas,reg_dxy_scale)   \n",
    "    \n",
    "        y_test_meas = y_test_meas.reshape(-1)\n",
    "        dxy_test_meas = dxy_test_meas.reshape(-1)\n",
    "\n",
    "        return y_test_meas, dxy_test_meas\n",
    "class evaluate:\n",
    "    def __init__(self,X_test,y_test):\n",
    "        self.X = X_test\n",
    "        self.y = y_test[0]\n",
    "        self.dxy = y_test[1]\n",
    "    \n",
    "    def compute_data_statistics(self,ctype = \"y\",label=\"data\"):\n",
    "        if ctype == \"y\":\n",
    "            x = self.recalibrate(self.y,reg_pt_scale)\n",
    "            x = x**(-1)\n",
    "        else:\n",
    "            x = self.recalibrate(self.dxy,reg_dxy_scale)\n",
    "        df_describe = pd.DataFrame(x, columns = [label])\n",
    "        print(df_describe.describe())\n",
    "    \n",
    "    def rmse(self,y_true, y_predicted):\n",
    "        assert(y_true.shape[0] == y_predicted.shape[0])\n",
    "        n = y_true.shape[0]\n",
    "        sum_square = np.sum((y_true - y_predicted)**2)\n",
    "        return math.sqrt(sum_square/n)\n",
    "    \n",
    "    def adjusted_r_2(self,y_true, y_predicted):\n",
    "        y_addC = sm.add_constant(y_true)\n",
    "        result = sm.OLS(y_predicted, y_addC).fit()\n",
    "        print(result.rsquared, result.rsquared_adj)\n",
    "\n",
    "    def recalibrate(self,x,scale):\n",
    "        return x/scale\n",
    "    \n",
    "    def inverse(self,arr):\n",
    "        arr_inv = 1./arr\n",
    "        arr_inv[arr_inv == np.inf] = 0.\n",
    "        return arr_inv\n",
    "    \n",
    "    def predict(self,model,batch_size = 256):\n",
    "        y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "        dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "        \n",
    "        y_test = model.predict(self.X,batch_size = 2000)\n",
    "        y_test_meas = y_test[:,0]\n",
    "        dxy_test_meas = y_test[:,1]\n",
    "        y_test_meas = self.recalibrate(y_test_meas,reg_pt_scale)\n",
    "        dxy_test_meas = self.recalibrate(dxy_test_meas,reg_dxy_scale)   \n",
    "    \n",
    "        y_test_meas = y_test_meas.reshape(-1)\n",
    "        dxy_test_meas = dxy_test_meas.reshape(-1)\n",
    "\n",
    "        return y_test_meas, dxy_test_meas\n",
    "    \n",
    "    def compute_error(self,y_predicted,ctype = \"y\"):\n",
    "        if ctype == \"y\":\n",
    "            y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "            print(\"RMSE Error for momentum:\",self.rmse(self.inverse(y_test_true),\\\n",
    "                                                                              self.inverse(y_predicted)))\n",
    "        else:\n",
    "            dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "            print(\"RMSE Error for dxy:\",self.rmse(dxy_test_true,y_predicted))\n",
    "\n",
    "    def get_error(self,y_predicted,ctype = \"y\"):\n",
    "        if ctype == \"y\":\n",
    "            y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "            return self.rmse(self.inverse(y_test_true),self.inverse(y_predicted))\n",
    "        else:\n",
    "            dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "            return self.rmse(dxy_test_true,y_predicted)\n",
    "\n",
    "\n",
    "def k_fold_validation(model, x, y, dxy, folds =10):\n",
    "    x_copy = np.copy(x)\n",
    "    y_copy = np.copy(y)\n",
    "    dxy_copy = np.copy(dxy)\n",
    "    assert x_copy.shape[0] == y_copy.shape[0] == dxy_copy.shape[0]\n",
    "    fold_size = int(x_copy.shape[0] / folds)\n",
    "    x_splits, y_splits, dxy_splits = [], [], []\n",
    "    for i in range(folds):\n",
    "        indices = np.random.choice(x_copy.shape[0],fold_size, replace=False)  \n",
    "        x_splits.append(x_copy[indices])\n",
    "        y_splits.append(y_copy[indices])\n",
    "        dxy_splits.append(dxy_copy[indices])\n",
    "        x_copy = np.delete(x_copy,indices,axis = 0)\n",
    "        y_copy = np.delete(y_copy,indices,axis = 0)\n",
    "        dxy_copy = np.delete(dxy_copy,indices,axis = 0)\n",
    "    rmse_y, rmse_dxy = [],[]\n",
    "    for i in range(folds):\n",
    "        evaluate_obj = evaluate(x_splits[i], tuple([y_splits[i],dxy_splits[i]]))\n",
    "        y_predicted , dxy_predicted = evaluate_obj.predict(model = model)\n",
    "        rmse_y.append(evaluate_obj.get_error(y_predicted,ctype=\"y\"))\n",
    "        rmse_dxy.append(evaluate_obj.get_error(dxy_predicted,ctype=\"dxy\"))\n",
    "    print('Average RMSE for '+ str(folds) + '-fold cv for y:', np.mean(rmse_y))\n",
    "    print('Average RMSE for '+ str(folds) + '-fold cv for dxy:', np.mean(rmse_dxy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb5ab9",
   "metadata": {},
   "source": [
    "### Package Installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80c49ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy \n",
    "# !pip install matplotlib\n",
    "# !pip install -q tensorflow-model-optimization\n",
    "# !pip install pandas\n",
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553cf40",
   "metadata": {},
   "source": [
    "### Preprocessing the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f35cd807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Loading muon data from NN_input_params_FlatXYZ.npz ...\n",
      "[INFO    ] Loaded the variables with shape (19300000, 25)\n",
      "[INFO    ] Loaded the parameters with shape (19300000, 6)\n",
      "[INFO    ] Loaded the encoded variables with shape (3284620, 23)\n",
      "[INFO    ] Loaded the encoded parameters with shape (3284620,)\n",
      "[INFO    ] Loaded # of training and testing events: (2249964, 1034656)\n",
      "[WARNING ] The last batch for training could be too few! (2024967%128)=7. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*0.9) % 128\n",
      "[WARNING ] The last batch for training after mixing could be too few! (4049935%128)=15. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*2*0.9) % 128\n"
     ]
    }
   ],
   "source": [
    "infile_muon_displ = \"NN_input_params_FlatXYZ.npz\"\n",
    "\n",
    "nentries = 100000000\n",
    "\n",
    "def _handle_nan_in_x(x):\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    x[x==-999.0] = 0.0\n",
    "    return x\n",
    "\n",
    "def _zero_out_x(x):\n",
    "    x = 0.0\n",
    "    return x\n",
    "    \n",
    "def _fixME1Ring(x):\n",
    "    for i in range(len(x)):\n",
    "        if (x[i,0] != 0.0): x[i,18] = x[i,18] + 1\n",
    "    return x   \n",
    "\n",
    "def muon_data(filename, reg_pt_scale=1.0, reg_dxy_scale=1.0, correct_for_eta=False):\n",
    "    try:\n",
    "        logger.info('Loading muon data from {0} ...'.format(filename))\n",
    "        loaded = np.load(filename)\n",
    "        the_variables = loaded['variables']\n",
    "        the_parameters = loaded['parameters']\n",
    "        # print(the_variables.shape)\n",
    "        the_variables = the_variables[:nentries]\n",
    "        the_parameters = the_parameters[:nentries]\n",
    "        logger.info('Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "        logger.info('Loaded the parameters with shape {0}'.format(the_parameters.shape))\n",
    "    except:\n",
    "        logger.error('Failed to load data from file: {0}'.format(filename))\n",
    "\n",
    "    assert(the_variables.shape[0] == the_parameters.shape[0])\n",
    "    _handle_nan_in_x(the_variables)\n",
    "      #_fixME1Ring(the_variables)\n",
    "    _handle_nan_in_x(the_parameters)\n",
    "    mask = np.logical_or(np.logical_or( np.logical_or((the_variables[:,23] == 11), (the_variables[:,23] == 13)), (the_variables[:,23] == 14)),(the_variables[:,23] == 15)) \n",
    "\n",
    "    the_variables = the_variables[mask]  \n",
    "    the_parameters = the_parameters[mask]  \n",
    "    assert(the_variables.shape[0] == the_parameters.shape[0])\n",
    "\n",
    "    x = the_variables[:,0:23]\n",
    "    y = reg_pt_scale*the_parameters[:,0]\n",
    "#     print (x[0:30,:], the_variables[0:30,23])\n",
    "#     print (y[0:30])\n",
    "    phi = the_parameters[:,1] \n",
    "    eta = the_parameters[:,2] \n",
    "    vx = the_parameters[:,3] \n",
    "    vy = the_parameters[:,4] \n",
    "    vz = the_parameters[:,5]      \n",
    "    dxy = vy * np.cos(phi) - vx * np.sin(phi) \n",
    "    dz = vz\n",
    "    w = np.abs(y)/0.2 + 1.0\n",
    "    x_mask = the_parameters[:,5]\n",
    "    x_road = the_parameters[:,5] \n",
    "    _zero_out_x(x_mask)\n",
    "    _zero_out_x(x_road)  \n",
    "    logger.info('Loaded the encoded variables with shape {0}'.format(x.shape))\n",
    "    logger.info('Loaded the encoded parameters with shape {0}'.format(y.shape))\n",
    "    #assert(np.isfinite(x).all())\n",
    "    return x, y, dxy, dz, w, x_mask, x_road\n",
    "\n",
    "def muon_data_split(filename, reg_pt_scale=1.0, reg_dxy_scale=1.0, test_size=0.5, correct_for_eta=False):\n",
    "    x, y, dxy, dz, w, x_mask, x_road = muon_data(filename, reg_pt_scale=reg_pt_scale, reg_dxy_scale=reg_dxy_scale, correct_for_eta=correct_for_eta)\n",
    "\n",
    "    # Split dataset in training and testing\n",
    "    x_train, x_test, y_train, y_test, dxy_train, dxy_test, dz_train, dz_test, w_train, w_test, x_mask_train, x_mask_test, x_road_train, x_road_test = train_test_split(x, y, dxy, dz, w, x_mask, x_road, test_size=test_size)\n",
    "    logger.info('Loaded # of training and testing events: {0}'.format((x_train.shape[0], x_test.shape[0])))\n",
    "\n",
    "    # Check for cases where the number of events in the last batch could be too few\n",
    "    validation_split = 0.1\n",
    "    train_num_samples = int(x_train.shape[0] * (1.0-validation_split))\n",
    "    val_num_samples = x_train.shape[0] - train_num_samples\n",
    "    batch_size = 128\n",
    "    if (train_num_samples%batch_size) < 100:\n",
    "        logger.warning('The last batch for training could be too few! ({0}%{1})={2}. Please change test_size.'.format(train_num_samples, batch_size, train_num_samples%batch_size))\n",
    "        logger.warning('Try this formula: int(int({0}*{1})*{2}) % 128'.format(x.shape[0], 1.0-test_size, 1.0-validation_split))\n",
    "    train_num_samples = int(x_train.shape[0] * 2 * (1.0-validation_split))\n",
    "    val_num_samples = x_train.shape[0] - train_num_samples\n",
    "    batch_size = 128\n",
    "    if (train_num_samples%batch_size) < 100:\n",
    "        logger.warning('The last batch for training after mixing could be too few! ({0}%{1})={2}. Please change test_size.'.format(train_num_samples, batch_size, train_num_samples%batch_size))\n",
    "        logger.warning('Try this formula: int(int({0}*{1})*2*{2}) % 128'.format(x.shape[0], 1.0-test_size, 1.0-validation_split))\n",
    "    return x_train, x_test, y_train, y_test, dxy_train, dxy_test, dz_train, dz_test, w_train, w_test, x_mask_train, x_mask_test, x_road_train, x_road_test\n",
    "\n",
    "# Import muon data\n",
    "# 'x' is the array of input variables, 'y' is the q/pT\n",
    "x_train_displ, x_test_displ, y_train_displ, y_test_displ, dxy_train_displ, dxy_test_displ, dz_train_displ, dz_test_displ, \\\n",
    "w_train_displ, w_test_displ, x_mask_train_displ, x_mask_test_displ, x_road_train_displ, x_road_test_displ = \\\n",
    "      muon_data_split(infile_muon_displ, reg_pt_scale=reg_pt_scale, reg_dxy_scale=reg_dxy_scale, test_size=0.315)\n",
    "\n",
    "y_train_displ = np.abs(y_train_displ)\n",
    "y_test_displ = np.abs(y_test_displ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f716a5",
   "metadata": {},
   "source": [
    "### Training Algorithm for the non-pruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13769d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    squared_loss = 0.5*K.square(x)\n",
    "    absolute_loss = delta * (x - 0.5*delta)\n",
    "    #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "    xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "    return K.mean(xx, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c52caf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nvariables, lr=0.001, clipnorm=10., nodes1=64, nodes2=32, nodes3=16, nodes4=0, nodes5=0, \n",
    "                 outnodes=2, l1_reg=0.0, l2_reg=0.0, use_bn=True, kernel_initializer = \"he_uniform\",\n",
    "                 eps = 1e-4, momentum = 0.9, activation = \"tanh\",use_dropout=False):\n",
    "    # Adding 1 BN layer right after the input layer\n",
    "    regularizer = regularizers.L1L2(l1=l1_reg, l2=l2_reg)\n",
    "    model = Sequential()  \n",
    "    if use_bn: \n",
    "        model.add(BatchNormalization(input_shape=(nvariables,), epsilon=eps, momentum=momentum))\n",
    "    model.add(Dense(nodes1, kernel_initializer=kernel_initializer, kernel_regularizer=regularizer, use_bias=False))\n",
    "    if use_bn: \n",
    "        model.add(BatchNormalization(epsilon=eps, momentum=momentum))\n",
    "#     model.add(Activation(activation))\n",
    "    model.add(LeakyReLU(alpha = .25))\n",
    "    if nodes2:\n",
    "        model.add(Dense(nodes2, kernel_initializer=kernel_initializer, kernel_regularizer=regularizer, use_bias=False))\n",
    "        if use_bn: \n",
    "            model.add(BatchNormalization(epsilon=eps, momentum=momentum))\n",
    "#         model.add(Activation(activation))\n",
    "        model.add(LeakyReLU(alpha = .25))\n",
    "        if nodes3:\n",
    "            model.add(Dense(nodes3, kernel_initializer=kernel_initializer, kernel_regularizer=regularizer, use_bias=False))\n",
    "            if use_bn: \n",
    "                model.add(BatchNormalization(epsilon=eps, momentum=momentum))\n",
    "#             model.add(Activation(activation))\n",
    "            model.add(LeakyReLU(alpha = .25))\n",
    "            if nodes4:\n",
    "                model.add(Dense(nodes4, kernel_initializer=kernel_initializer, kernel_regularizer=regularizer, use_bias=False))\n",
    "                if use_bn: \n",
    "                    model.add(BatchNormalization(epsilon=eps, momentum=momentum))\n",
    "#                 model.add(Activation(activation))\n",
    "                model.add(LeakyReLU(alpha = .25))\n",
    "                if nodes5:\n",
    "                    model.add(Dense(nodes5, kernel_initializer=kernel_initializer, kernel_regularizer=regularizer, use_bias=False))\n",
    "                    if use_bn: \n",
    "                        model.add(BatchNormalization(epsilon=eps, momentum=momentum))\n",
    "#                     model.add(Activation(activation))     \n",
    "                    model.add(LeakyReLU(alpha = .25))\n",
    "     \n",
    "    # Output node\n",
    "    model.add(Dense(outnodes, activation='linear', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    # Set loss and optimizers\n",
    "    adam = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=adam, loss=huber_loss, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ad1d70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Training model with l1_reg: 0.0 l2_reg: 0.0\n",
      "[INFO    ] Begin training ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_8 (Batch (None, 23)                92        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                460       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                300       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 15)                60        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                150       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 1,204\n",
      "Trainable params: 1,068\n",
      "Non-trainable params: 136\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 6s - loss: 12.9767 - acc: 0.9271 - val_loss: 10.6366 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 9.73163\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 6s - loss: 11.0218 - acc: 0.9332 - val_loss: 10.6814 - val_acc: 0.9354\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 9.73163\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.8936 - acc: 0.9343 - val_loss: 10.4503 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.73163\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.8200 - acc: 0.9347 - val_loss: 10.3551 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.73163\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.7662 - acc: 0.9351 - val_loss: 10.6358 - val_acc: 0.9352\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9.73163\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.7204 - acc: 0.9355 - val_loss: 10.3578 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.73163\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.7044 - acc: 0.9355 - val_loss: 10.2162 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.73163\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.6736 - acc: 0.9355 - val_loss: 10.3278 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.73163\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 6s - loss: 10.6516 - acc: 0.9357 - val_loss: 10.2051 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.73163\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.006300000008195639.\n",
      "4050/4050 - 5s - loss: 10.6414 - acc: 0.9357 - val_loss: 10.2311 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 9.73163\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.005670000007376075.\n",
      "4050/4050 - 6s - loss: 10.6081 - acc: 0.9356 - val_loss: 10.1742 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 9.73163\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.6011 - acc: 0.9358 - val_loss: 10.2277 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 9.73163\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5810 - acc: 0.9359 - val_loss: 10.1869 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 9.73163\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5916 - acc: 0.9358 - val_loss: 10.0808 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 9.73163\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5574 - acc: 0.9362 - val_loss: 10.0970 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 9.73163\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 6s - loss: 10.5582 - acc: 0.9360 - val_loss: 10.1003 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 9.73163\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5521 - acc: 0.9361 - val_loss: 10.1035 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 9.73163\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5525 - acc: 0.9361 - val_loss: 10.0777 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 9.73163\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5295 - acc: 0.9363 - val_loss: 10.1050 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 9.73163\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.005669999867677689.\n",
      "4050/4050 - 5s - loss: 10.5377 - acc: 0.9362 - val_loss: 10.0942 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 9.73163\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00510299988090992.\n",
      "4050/4050 - 6s - loss: 10.5237 - acc: 0.9363 - val_loss: 10.1292 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 9.73163\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.5086 - acc: 0.9364 - val_loss: 10.0691 - val_acc: 0.9390\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 9.73163\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.5118 - acc: 0.9363 - val_loss: 10.0801 - val_acc: 0.9384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 9.73163\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4886 - acc: 0.9364 - val_loss: 10.0302 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 9.73163\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4940 - acc: 0.9364 - val_loss: 10.0461 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 9.73163\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4912 - acc: 0.9365 - val_loss: 10.0014 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 9.73163\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4874 - acc: 0.9365 - val_loss: 10.0957 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 9.73163\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4867 - acc: 0.9366 - val_loss: 10.0830 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 9.73163\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4890 - acc: 0.9365 - val_loss: 10.1248 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 9.73163\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.005102999974042177.\n",
      "4050/4050 - 5s - loss: 10.4729 - acc: 0.9364 - val_loss: 10.0449 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 9.73163\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.004592699976637959.\n",
      "4050/4050 - 6s - loss: 10.4567 - acc: 0.9366 - val_loss: 9.9839 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 9.73163\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 6s - loss: 10.4607 - acc: 0.9367 - val_loss: 9.9698 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 9.73163\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 6s - loss: 10.4506 - acc: 0.9366 - val_loss: 9.9813 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 9.73163\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 6s - loss: 10.4599 - acc: 0.9366 - val_loss: 10.0143 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 9.73163\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 5s - loss: 10.4481 - acc: 0.9366 - val_loss: 9.9742 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 9.73163\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 5s - loss: 10.4414 - acc: 0.9367 - val_loss: 10.0592 - val_acc: 0.9390\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 9.73163\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 5s - loss: 10.4408 - acc: 0.9366 - val_loss: 10.0325 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 9.73163\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 5s - loss: 10.4360 - acc: 0.9366 - val_loss: 10.2002 - val_acc: 0.9386\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 9.73163\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 5s - loss: 10.4309 - acc: 0.9368 - val_loss: 9.9474 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 9.73163\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.004592699930071831.\n",
      "4050/4050 - 5s - loss: 10.4264 - acc: 0.9368 - val_loss: 9.9759 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 9.73163\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.004133429937064648.\n",
      "4050/4050 - 5s - loss: 10.4189 - acc: 0.9369 - val_loss: 9.9256 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 9.73163\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 6s - loss: 10.4210 - acc: 0.9368 - val_loss: 10.0291 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 9.73163\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4167 - acc: 0.9369 - val_loss: 9.9644 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 9.73163\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4102 - acc: 0.9368 - val_loss: 9.9592 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 9.73163\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4165 - acc: 0.9367 - val_loss: 9.9288 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 9.73163\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4075 - acc: 0.9368 - val_loss: 9.9661 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 9.73163\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4033 - acc: 0.9369 - val_loss: 9.9583 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 9.73163\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4016 - acc: 0.9370 - val_loss: 9.9308 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 9.73163\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4094 - acc: 0.9368 - val_loss: 9.9231 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 9.73163\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.00413342984393239.\n",
      "4050/4050 - 5s - loss: 10.4100 - acc: 0.9368 - val_loss: 9.9444 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 9.73163\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0037200868595391513.\n",
      "4050/4050 - 5s - loss: 10.3994 - acc: 0.9368 - val_loss: 9.9157 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 9.73163\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.3953 - acc: 0.9370 - val_loss: 9.9184 - val_acc: 0.9395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00052: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 9.73163\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.3980 - acc: 0.9368 - val_loss: 9.9303 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 9.73163\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.3925 - acc: 0.9370 - val_loss: 9.9167 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 9.73163\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.4004 - acc: 0.9368 - val_loss: 10.0038 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 9.73163\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.4045 - acc: 0.9370 - val_loss: 9.9345 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 9.73163\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.3941 - acc: 0.9370 - val_loss: 9.9097 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 9.73163\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.3927 - acc: 0.9370 - val_loss: 9.9087 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 9.73163\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 5s - loss: 10.3903 - acc: 0.9369 - val_loss: 9.9255 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 9.73163\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0037200867664068937.\n",
      "4050/4050 - 6s - loss: 10.3937 - acc: 0.9369 - val_loss: 9.9401 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 9.73163\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0033480780897662044.\n",
      "4050/4050 - 6s - loss: 10.3902 - acc: 0.9370 - val_loss: 9.9309 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 9.73163\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 6s - loss: 10.3880 - acc: 0.9370 - val_loss: 9.8915 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 9.73163\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 5s - loss: 10.3849 - acc: 0.9370 - val_loss: 9.9372 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 9.73163\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 5s - loss: 10.3876 - acc: 0.9368 - val_loss: 9.9025 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 9.73163\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 5s - loss: 10.3870 - acc: 0.9370 - val_loss: 9.9127 - val_acc: 0.9393\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 9.73163\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 6s - loss: 10.3913 - acc: 0.9368 - val_loss: 9.9123 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 9.73163\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 6s - loss: 10.3881 - acc: 0.9371 - val_loss: 9.8903 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 9.73163\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 5s - loss: 10.3853 - acc: 0.9370 - val_loss: 9.9172 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 9.73163\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 5s - loss: 10.3849 - acc: 0.9370 - val_loss: 9.9103 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 9.73163\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0033480781130492687.\n",
      "4050/4050 - 5s - loss: 10.3806 - acc: 0.9370 - val_loss: 9.8934 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 9.73163\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.003013270301744342.\n",
      "4050/4050 - 5s - loss: 10.3728 - acc: 0.9371 - val_loss: 9.8721 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 9.73163\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3744 - acc: 0.9371 - val_loss: 9.8956 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 9.73163\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3805 - acc: 0.9371 - val_loss: 9.8913 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 9.73163\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3674 - acc: 0.9369 - val_loss: 9.8963 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 9.73163\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3690 - acc: 0.9371 - val_loss: 9.8792 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 9.73163\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3814 - acc: 0.9370 - val_loss: 9.9066 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 9.73163\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3776 - acc: 0.9371 - val_loss: 9.9043 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 9.73163\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3736 - acc: 0.9370 - val_loss: 9.9075 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 9.73163\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3626 - acc: 0.9372 - val_loss: 9.9009 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 9.73163\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.0030132702086120844.\n",
      "4050/4050 - 5s - loss: 10.3687 - acc: 0.9371 - val_loss: 9.8987 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 9.73163\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.002711943187750876.\n",
      "4050/4050 - 5s - loss: 10.3697 - acc: 0.9371 - val_loss: 9.8831 - val_acc: 0.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00081: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 9.73163\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3663 - acc: 0.9372 - val_loss: 9.8859 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 9.73163\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3658 - acc: 0.9371 - val_loss: 9.8894 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 9.73163\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3592 - acc: 0.9370 - val_loss: 9.8900 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 9.73163\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3739 - acc: 0.9372 - val_loss: 9.8732 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 9.73163\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3661 - acc: 0.9370 - val_loss: 9.9606 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 9.73163\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3725 - acc: 0.9371 - val_loss: 9.8893 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 9.73163\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3745 - acc: 0.9370 - val_loss: 9.9140 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 9.73163\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3545 - acc: 0.9372 - val_loss: 9.8784 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 9.73163\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.002711943117901683.\n",
      "4050/4050 - 5s - loss: 10.3658 - acc: 0.9371 - val_loss: 9.8919 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 9.73163\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.0024407488061115147.\n",
      "4050/4050 - 6s - loss: 10.3532 - acc: 0.9373 - val_loss: 9.9134 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 9.73163\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3570 - acc: 0.9371 - val_loss: 9.8752 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 9.73163\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 5s - loss: 10.3578 - acc: 0.9371 - val_loss: 9.9161 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 9.73163\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3558 - acc: 0.9372 - val_loss: 9.9142 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 9.73163\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3652 - acc: 0.9370 - val_loss: 9.9042 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 9.73163\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3596 - acc: 0.9371 - val_loss: 9.8708 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 9.73163\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3626 - acc: 0.9370 - val_loss: 9.9254 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 9.73163\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3556 - acc: 0.9371 - val_loss: 9.8808 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 9.73163\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 7s - loss: 10.3634 - acc: 0.9371 - val_loss: 9.8876 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 9.73163\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0024407487362623215.\n",
      "4050/4050 - 6s - loss: 10.3544 - acc: 0.9372 - val_loss: 9.8723 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 9.73163\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0021966738626360894.\n",
      "4050/4050 - 6s - loss: 10.3494 - acc: 0.9373 - val_loss: 9.8824 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 9.73163\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 6s - loss: 10.3427 - acc: 0.9372 - val_loss: 9.8803 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 9.73163\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3508 - acc: 0.9372 - val_loss: 9.8838 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 9.73163\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3591 - acc: 0.9372 - val_loss: 9.8764 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 9.73163\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3545 - acc: 0.9370 - val_loss: 9.8656 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 9.73163\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3477 - acc: 0.9372 - val_loss: 9.8887 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 9.73163\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3520 - acc: 0.9371 - val_loss: 9.8910 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 9.73163\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3594 - acc: 0.9371 - val_loss: 9.8644 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 9.73163\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3528 - acc: 0.9370 - val_loss: 9.8874 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 9.73163\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.002196673769503832.\n",
      "4050/4050 - 5s - loss: 10.3547 - acc: 0.9372 - val_loss: 9.9085 - val_acc: 0.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00110: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 9.73163\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001977006392553449.\n",
      "4050/4050 - 5s - loss: 10.3374 - acc: 0.9372 - val_loss: 9.8861 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 9.73163\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3516 - acc: 0.9371 - val_loss: 9.8906 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 9.73163\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3599 - acc: 0.9370 - val_loss: 9.8577 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 9.73163\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3532 - acc: 0.9371 - val_loss: 9.8814 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 9.73163\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3481 - acc: 0.9371 - val_loss: 9.8721 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 9.73163\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3390 - acc: 0.9371 - val_loss: 9.8591 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 9.73163\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3426 - acc: 0.9372 - val_loss: 9.8929 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 9.73163\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3504 - acc: 0.9370 - val_loss: 9.8838 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 9.73163\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 5s - loss: 10.3537 - acc: 0.9372 - val_loss: 9.8682 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 9.73163\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0019770064391195774.\n",
      "4050/4050 - 6s - loss: 10.3456 - acc: 0.9373 - val_loss: 9.8679 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 9.73163\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0017793057952076197.\n",
      "4050/4050 - 6s - loss: 10.3371 - acc: 0.9373 - val_loss: 9.8601 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 9.73163\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 5s - loss: 10.3377 - acc: 0.9373 - val_loss: 9.8635 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 9.73163\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 5s - loss: 10.3481 - acc: 0.9373 - val_loss: 9.8637 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 9.73163\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 7s - loss: 10.3328 - acc: 0.9373 - val_loss: 9.8562 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 9.73163\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 6s - loss: 10.3501 - acc: 0.9373 - val_loss: 9.8597 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 9.73163\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 6s - loss: 10.3443 - acc: 0.9371 - val_loss: 9.8676 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 9.73163\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 6s - loss: 10.3444 - acc: 0.9373 - val_loss: 9.9177 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 9.73163\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 6s - loss: 10.3433 - acc: 0.9371 - val_loss: 9.8570 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 9.73163\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 5s - loss: 10.3434 - acc: 0.9373 - val_loss: 9.8577 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 9.73163\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001779305748641491.\n",
      "4050/4050 - 5s - loss: 10.3447 - acc: 0.9373 - val_loss: 9.8976 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 9.73163\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3438 - acc: 0.9372 - val_loss: 9.8604 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 9.73163\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3416 - acc: 0.9373 - val_loss: 9.8620 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 9.73163\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3412 - acc: 0.9373 - val_loss: 9.8787 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 9.73163\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3366 - acc: 0.9372 - val_loss: 9.8619 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 9.73163\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3295 - acc: 0.9372 - val_loss: 9.8495 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 9.73163\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3452 - acc: 0.9372 - val_loss: 9.8440 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 9.73163\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3401 - acc: 0.9372 - val_loss: 9.8630 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 9.73163\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3379 - acc: 0.9371 - val_loss: 9.8638 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 9.73163\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3313 - acc: 0.9373 - val_loss: 9.8598 - val_acc: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 9.73163\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0016013751737773418.\n",
      "4050/4050 - 5s - loss: 10.3383 - acc: 0.9372 - val_loss: 9.8828 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 9.73163\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0014412376563996078.\n",
      "4050/4050 - 5s - loss: 10.3393 - acc: 0.9372 - val_loss: 9.8481 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 9.73163\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3339 - acc: 0.9374 - val_loss: 9.8487 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 9.73163\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 6s - loss: 10.3292 - acc: 0.9372 - val_loss: 9.8612 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 9.73163\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3427 - acc: 0.9369 - val_loss: 9.8931 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 9.73163\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3374 - acc: 0.9374 - val_loss: 9.8587 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 9.73163\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3289 - acc: 0.9373 - val_loss: 9.8608 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 9.73163\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3303 - acc: 0.9372 - val_loss: 9.9416 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 9.73163\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3281 - acc: 0.9373 - val_loss: 9.8705 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 9.73163\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3317 - acc: 0.9373 - val_loss: 9.8598 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 9.73163\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0014412376331165433.\n",
      "4050/4050 - 5s - loss: 10.3329 - acc: 0.9372 - val_loss: 9.8894 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 9.73163\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001297113869804889.\n",
      "4050/4050 - 5s - loss: 10.3370 - acc: 0.9373 - val_loss: 9.9068 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 9.73163\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 6s - loss: 10.3311 - acc: 0.9372 - val_loss: 9.8643 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 9.73163\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 5s - loss: 10.3249 - acc: 0.9373 - val_loss: 9.8576 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 9.73163\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 5s - loss: 10.3307 - acc: 0.9371 - val_loss: 9.8358 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 9.73163\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 5s - loss: 10.3359 - acc: 0.9373 - val_loss: 9.8548 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 9.73163\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 6s - loss: 10.3351 - acc: 0.9372 - val_loss: 9.8530 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 9.73163\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 6s - loss: 10.3410 - acc: 0.9371 - val_loss: 9.8956 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 9.73163\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 6s - loss: 10.3346 - acc: 0.9371 - val_loss: 9.8657 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 9.73163\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 5s - loss: 10.3366 - acc: 0.9372 - val_loss: 9.8581 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 9.73163\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0012971138348802924.\n",
      "4050/4050 - 6s - loss: 10.3324 - acc: 0.9373 - val_loss: 9.8805 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 9.73163\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0011674024513922633.\n",
      "4050/4050 - 5s - loss: 10.3452 - acc: 0.9372 - val_loss: 9.8861 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 9.73163\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3326 - acc: 0.9373 - val_loss: 9.8599 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 9.73163\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3265 - acc: 0.9372 - val_loss: 9.8520 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 9.73163\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3242 - acc: 0.9372 - val_loss: 9.8562 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 9.73163\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3304 - acc: 0.9373 - val_loss: 9.8515 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 9.73163\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3200 - acc: 0.9373 - val_loss: 9.8543 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 9.73163\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3250 - acc: 0.9373 - val_loss: 9.8518 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 9.73163\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 5s - loss: 10.3327 - acc: 0.9372 - val_loss: 9.8391 - val_acc: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00168: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 9.73163\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 6s - loss: 10.3341 - acc: 0.9375 - val_loss: 9.8456 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 9.73163\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0011674024863168597.\n",
      "4050/4050 - 6s - loss: 10.3271 - acc: 0.9373 - val_loss: 9.8443 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 9.73163\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0010506622376851738.\n",
      "4050/4050 - 7s - loss: 10.3192 - acc: 0.9373 - val_loss: 9.8487 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 9.73163\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 6s - loss: 10.3256 - acc: 0.9373 - val_loss: 9.8494 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 9.73163\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 6s - loss: 10.3303 - acc: 0.9373 - val_loss: 9.8949 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 9.73163\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 6s - loss: 10.3238 - acc: 0.9373 - val_loss: 9.8686 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 9.73163\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 7s - loss: 10.3222 - acc: 0.9373 - val_loss: 9.8608 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 9.73163\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 7s - loss: 10.3252 - acc: 0.9372 - val_loss: 9.8484 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 9.73163\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 5s - loss: 10.3368 - acc: 0.9372 - val_loss: 9.8686 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 9.73163\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 6s - loss: 10.3221 - acc: 0.9374 - val_loss: 9.8439 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 9.73163\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 5s - loss: 10.3243 - acc: 0.9373 - val_loss: 9.8764 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 9.73163\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001050662249326706.\n",
      "4050/4050 - 5s - loss: 10.3337 - acc: 0.9371 - val_loss: 9.8631 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 9.73163\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 7s - loss: 10.3198 - acc: 0.9373 - val_loss: 9.8561 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 9.73163\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 6s - loss: 10.3366 - acc: 0.9373 - val_loss: 9.8779 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 9.73163\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 7s - loss: 10.3274 - acc: 0.9372 - val_loss: 9.8586 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 9.73163\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 7s - loss: 10.3225 - acc: 0.9372 - val_loss: 9.8726 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 9.73163\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 6s - loss: 10.3199 - acc: 0.9373 - val_loss: 9.8540 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 9.73163\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 7s - loss: 10.3267 - acc: 0.9372 - val_loss: 9.8423 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 9.73163\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 6s - loss: 10.3189 - acc: 0.9372 - val_loss: 9.8987 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 9.73163\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 7s - loss: 10.3259 - acc: 0.9371 - val_loss: 9.8513 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 9.73163\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 7s - loss: 10.3200 - acc: 0.9373 - val_loss: 9.8502 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 9.73163\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0009455960243940353.\n",
      "4050/4050 - 6s - loss: 10.3214 - acc: 0.9372 - val_loss: 9.8418 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 9.73163\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0008510364219546318.\n",
      "4050/4050 - 5s - loss: 10.3198 - acc: 0.9374 - val_loss: 9.8480 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 9.73163\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3279 - acc: 0.9371 - val_loss: 9.8597 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 9.73163\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3173 - acc: 0.9373 - val_loss: 9.8453 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 9.73163\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3276 - acc: 0.9372 - val_loss: 9.8614 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 9.73163\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3238 - acc: 0.9372 - val_loss: 9.8560 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 9.73163\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3149 - acc: 0.9373 - val_loss: 9.8379 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 9.73163\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3218 - acc: 0.9372 - val_loss: 9.8491 - val_acc: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00197: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 9.73163\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3227 - acc: 0.9374 - val_loss: 9.8376 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 9.73163\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3201 - acc: 0.9373 - val_loss: 9.8843 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 9.73163\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0008510363986715674.\n",
      "4050/4050 - 5s - loss: 10.3220 - acc: 0.9372 - val_loss: 9.8349 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 9.73163\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0007659327588044107.\n",
      "4050/4050 - 5s - loss: 10.3155 - acc: 0.9375 - val_loss: 9.8374 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 9.73163\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3122 - acc: 0.9373 - val_loss: 9.8242 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 9.73163\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3148 - acc: 0.9373 - val_loss: 9.8482 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 9.73163\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3213 - acc: 0.9373 - val_loss: 9.8310 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 9.73163\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3124 - acc: 0.9374 - val_loss: 9.8455 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 9.73163\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3176 - acc: 0.9374 - val_loss: 9.8523 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 9.73163\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3153 - acc: 0.9371 - val_loss: 9.8696 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 9.73163\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3219 - acc: 0.9373 - val_loss: 9.8440 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 9.73163\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3264 - acc: 0.9372 - val_loss: 9.8577 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 9.73163\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0007659327820874751.\n",
      "4050/4050 - 5s - loss: 10.3160 - acc: 0.9373 - val_loss: 9.8414 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 9.73163\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.0006893395038787276.\n",
      "4050/4050 - 5s - loss: 10.3245 - acc: 0.9373 - val_loss: 9.8687 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 9.73163\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 5s - loss: 10.3105 - acc: 0.9375 - val_loss: 9.9385 - val_acc: 0.9390\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 9.73163\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 6s - loss: 10.3131 - acc: 0.9374 - val_loss: 9.8445 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 9.73163\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 7s - loss: 10.3094 - acc: 0.9374 - val_loss: 9.8451 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 9.73163\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 6s - loss: 10.3215 - acc: 0.9372 - val_loss: 9.8367 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 9.73163\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 7s - loss: 10.3164 - acc: 0.9373 - val_loss: 9.8539 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 9.73163\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 7s - loss: 10.3162 - acc: 0.9373 - val_loss: 9.8436 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 9.73163\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 7s - loss: 10.3112 - acc: 0.9374 - val_loss: 9.8667 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 9.73163\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 6s - loss: 10.3144 - acc: 0.9372 - val_loss: 9.8656 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 9.73163\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0006893394747748971.\n",
      "4050/4050 - 7s - loss: 10.3072 - acc: 0.9373 - val_loss: 9.8336 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 9.73163\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.0006204055272974074.\n",
      "4050/4050 - 6s - loss: 10.3105 - acc: 0.9373 - val_loss: 9.8517 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 9.73163\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 7s - loss: 10.3169 - acc: 0.9372 - val_loss: 9.8779 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 9.73163\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 7s - loss: 10.3137 - acc: 0.9373 - val_loss: 9.8347 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 9.73163\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 7s - loss: 10.3135 - acc: 0.9373 - val_loss: 9.8286 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 9.73163\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 7s - loss: 10.3063 - acc: 0.9374 - val_loss: 9.8272 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 9.73163\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 7s - loss: 10.3068 - acc: 0.9374 - val_loss: 9.8253 - val_acc: 0.9401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00226: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 9.73163\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 5s - loss: 10.3179 - acc: 0.9372 - val_loss: 9.8687 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 9.73163\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 5s - loss: 10.3078 - acc: 0.9374 - val_loss: 9.8230 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 9.73163\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 5s - loss: 10.3152 - acc: 0.9374 - val_loss: 9.8641 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 9.73163\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.0006204055389389396.\n",
      "4050/4050 - 6s - loss: 10.3216 - acc: 0.9373 - val_loss: 9.8524 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 9.73163\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.0005583649850450456.\n",
      "4050/4050 - 7s - loss: 10.3139 - acc: 0.9374 - val_loss: 9.8862 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 9.73163\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 7s - loss: 10.3070 - acc: 0.9374 - val_loss: 9.8562 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 9.73163\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 7s - loss: 10.3109 - acc: 0.9375 - val_loss: 9.8250 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 9.73163\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 7s - loss: 10.3099 - acc: 0.9373 - val_loss: 9.8482 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 9.73163\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 7s - loss: 10.3020 - acc: 0.9375 - val_loss: 9.8400 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 9.73163\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 7s - loss: 10.3143 - acc: 0.9373 - val_loss: 9.8241 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 9.73163\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 6s - loss: 10.3138 - acc: 0.9374 - val_loss: 9.8499 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 9.73163\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 6s - loss: 10.3124 - acc: 0.9373 - val_loss: 9.8259 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 9.73163\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 7s - loss: 10.3145 - acc: 0.9372 - val_loss: 9.8226 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 9.73163\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.0005583649617619812.\n",
      "4050/4050 - 5s - loss: 10.3165 - acc: 0.9372 - val_loss: 9.8368 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 9.73163\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.0005025284655857832.\n",
      "4050/4050 - 5s - loss: 10.3150 - acc: 0.9373 - val_loss: 9.8359 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 9.73163\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 5s - loss: 10.3073 - acc: 0.9374 - val_loss: 9.8260 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 9.73163\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 6s - loss: 10.2959 - acc: 0.9376 - val_loss: 9.8326 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 9.73163\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 7s - loss: 10.3162 - acc: 0.9375 - val_loss: 9.8501 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 9.73163\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 6s - loss: 10.3078 - acc: 0.9374 - val_loss: 9.8441 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 9.73163\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 7s - loss: 10.3075 - acc: 0.9374 - val_loss: 9.8797 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 9.73163\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 6s - loss: 10.3149 - acc: 0.9372 - val_loss: 9.8415 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 9.73163\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 7s - loss: 10.3113 - acc: 0.9373 - val_loss: 9.8782 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 9.73163\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 7s - loss: 10.3131 - acc: 0.9374 - val_loss: 9.8506 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 9.73163\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.0005025284481234848.\n",
      "4050/4050 - 6s - loss: 10.3089 - acc: 0.9374 - val_loss: 9.8499 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 9.73163\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.0004522756033111364.\n",
      "4050/4050 - 7s - loss: 10.3173 - acc: 0.9374 - val_loss: 9.8394 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 9.73163\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 6s - loss: 10.3069 - acc: 0.9374 - val_loss: 9.8409 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 9.73163\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 6s - loss: 10.3028 - acc: 0.9374 - val_loss: 9.8519 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 9.73163\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 6s - loss: 10.3146 - acc: 0.9374 - val_loss: 9.8254 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 9.73163\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 5s - loss: 10.3082 - acc: 0.9374 - val_loss: 9.8341 - val_acc: 0.9402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00255: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 9.73163\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 5s - loss: 10.3023 - acc: 0.9373 - val_loss: 9.8317 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 9.73163\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 5s - loss: 10.2950 - acc: 0.9375 - val_loss: 9.8354 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 9.73163\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 6s - loss: 10.3030 - acc: 0.9373 - val_loss: 9.8229 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 9.73163\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 7s - loss: 10.3107 - acc: 0.9373 - val_loss: 9.8238 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 9.73163\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.0004522755916696042.\n",
      "4050/4050 - 6s - loss: 10.3049 - acc: 0.9374 - val_loss: 9.8522 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 9.73163\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.0004070480325026438.\n",
      "4050/4050 - 7s - loss: 10.3052 - acc: 0.9375 - val_loss: 9.8300 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 9.73163\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3028 - acc: 0.9376 - val_loss: 9.8408 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 9.73163\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3075 - acc: 0.9374 - val_loss: 9.8236 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 9.73163\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.2955 - acc: 0.9376 - val_loss: 9.8405 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 9.73163\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3043 - acc: 0.9374 - val_loss: 9.8464 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 9.73163\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3021 - acc: 0.9375 - val_loss: 9.8242 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 9.73163\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3035 - acc: 0.9375 - val_loss: 9.8679 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 9.73163\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3078 - acc: 0.9373 - val_loss: 9.8497 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 9.73163\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3071 - acc: 0.9374 - val_loss: 9.8329 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 9.73163\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.0004070480354130268.\n",
      "4050/4050 - 6s - loss: 10.3049 - acc: 0.9374 - val_loss: 9.8873 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 9.73163\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.0003663432318717241.\n",
      "4050/4050 - 6s - loss: 10.3157 - acc: 0.9373 - val_loss: 9.8769 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 9.73163\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 6s - loss: 10.3096 - acc: 0.9373 - val_loss: 9.8354 - val_acc: 0.9404\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 9.73163\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 6s - loss: 10.3098 - acc: 0.9373 - val_loss: 9.8198 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 9.73163\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 6s - loss: 10.3013 - acc: 0.9374 - val_loss: 9.8506 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 9.73163\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 5s - loss: 10.3080 - acc: 0.9372 - val_loss: 9.8594 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 9.73163\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 7s - loss: 10.3011 - acc: 0.9373 - val_loss: 9.8463 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 9.73163\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 7s - loss: 10.3028 - acc: 0.9374 - val_loss: 9.8487 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 9.73163\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 6s - loss: 10.3003 - acc: 0.9373 - val_loss: 9.8384 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 9.73163\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 6s - loss: 10.3014 - acc: 0.9374 - val_loss: 9.8341 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 9.73163\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0003663432435132563.\n",
      "4050/4050 - 6s - loss: 10.3010 - acc: 0.9374 - val_loss: 9.8328 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 9.73163\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.0003297089191619307.\n",
      "4050/4050 - 6s - loss: 10.3072 - acc: 0.9374 - val_loss: 9.8525 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 9.73163\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.3103 - acc: 0.9375 - val_loss: 9.8498 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 9.73163\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.3125 - acc: 0.9373 - val_loss: 9.8310 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 9.73163\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.2953 - acc: 0.9375 - val_loss: 9.8246 - val_acc: 0.9401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00284: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 9.73163\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.2953 - acc: 0.9375 - val_loss: 9.8299 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 9.73163\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.3017 - acc: 0.9374 - val_loss: 9.8307 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 9.73163\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 5s - loss: 10.3138 - acc: 0.9373 - val_loss: 9.8247 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 9.73163\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 5s - loss: 10.2997 - acc: 0.9376 - val_loss: 9.8470 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 9.73163\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.2971 - acc: 0.9374 - val_loss: 9.8208 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 9.73163\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0003297089133411646.\n",
      "4050/4050 - 6s - loss: 10.3057 - acc: 0.9374 - val_loss: 9.8236 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 9.73163\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.00029673802200704815.\n",
      "4050/4050 - 6s - loss: 10.2997 - acc: 0.9374 - val_loss: 9.8640 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 9.73163\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 6s - loss: 10.3081 - acc: 0.9373 - val_loss: 9.8167 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 9.73163\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 7s - loss: 10.3097 - acc: 0.9373 - val_loss: 9.8454 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 9.73163\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 6s - loss: 10.2982 - acc: 0.9374 - val_loss: 9.8626 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 9.73163\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 7s - loss: 10.3111 - acc: 0.9374 - val_loss: 9.8242 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 9.73163\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 5s - loss: 10.3064 - acc: 0.9373 - val_loss: 9.8159 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 9.73163\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 6s - loss: 10.3070 - acc: 0.9375 - val_loss: 9.8225 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 9.73163\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 6s - loss: 10.3042 - acc: 0.9375 - val_loss: 9.8248 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 9.73163\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 7s - loss: 10.3059 - acc: 0.9375 - val_loss: 9.8348 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 9.73163\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0002967380278278142.\n",
      "4050/4050 - 6s - loss: 10.3025 - acc: 0.9374 - val_loss: 9.8417 - val_acc: 0.9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Done training. Time elapsed: 0:28:00.330245 sec\n",
      "[INFO    ] Epoch 300/300 - loss: 10.30247688293457 - val_loss: 9.84165096282959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00300: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 9.73163\n"
     ]
    }
   ],
   "source": [
    "assert(keras.backend.backend() == 'tensorflow')\n",
    "\n",
    "lr = 1e-3\n",
    "l1_reg = 0.0\n",
    "l2_reg = 0.0\n",
    "clipnorm = 10.\n",
    "eps = 1e-4\n",
    "momentum = 0.9\n",
    "normal_epochs = 300\n",
    "normal_batch_size = 500\n",
    "\n",
    "model = create_model(nvariables=nvariables, \n",
    "                    lr=learning_rate, \n",
    "                    clipnorm=gradient_clip_norm, \n",
    "                    l1_reg=l1_reg, \n",
    "                    l2_reg=l2_reg,\n",
    "                    nodes1=20, \n",
    "                    nodes2=15, \n",
    "                    nodes3=10, \n",
    "                    outnodes=2, \n",
    "                    activation = \"selu\",\n",
    "                    eps = eps,\n",
    "                    kernel_initializer = \"lecun_uniform\",\n",
    "                    momentum = momentum)\n",
    "\n",
    "logger.info('Training model with l1_reg: {0} l2_reg: {0}'.format(l1_reg, l2_reg))\n",
    "\n",
    "history = train_model(model, \n",
    "                      x_train_displ, \n",
    "                      np.column_stack((y_train_displ, dxy_train_displ)),\n",
    "                      epochs=normal_epochs, \n",
    "                      batch_size=normal_batch_size,\n",
    "                      callbacks=[lr_decay,modelbestcheck,modelbestcheck_weights], \n",
    "                      validation_split=0.1, \n",
    "                      verbose=2)\n",
    "\n",
    "metrics = [len(history.history['loss']), history.history['loss'][-1], history.history['val_loss'][-1]]\n",
    "logger.info('Epoch {0}/{0} - loss: {1} - val_loss: {2}'.format(*metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f18490c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAFOCAYAAAA/9i4MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAArEAAAKxAFmbYLUAABa80lEQVR4nO2dd3hUVfqA3+mTQhJCNRAkgAmKwcAKIkoCSIkgikgRXRXlZwcXK6IuunYUsQGi6FpWFyVrZXEBEUTsYAGFkFANNRDSy/T7++NmhkySSb2TuTc57/PwMHPnzJ13vsx8c+653z1HJ0mShEAgEAiahD7UAgKBQKBlRBIVCASCZiCSqEAgEDQDkUQFAoGgGYgkKhAIBM1AJFGBQCBoBiKJCtoESUlJHDp0qN52H330EampqY1+TNB2EUlUIBAImoFIogKBQNAMRBIVhJxDhw6RlJTEJ598wiWXXMKAAQOYP38+W7duZfz48SQnJzNz5kxKS0t9z3nttddIS0tj4MCB3HDDDezZs8f3WGFhIXfccQfnnHMOqampZGRk+L1efn4+c+bMYcCAAQwZMoQnnngCm83WaO9t27YxdepU+vfvz8UXX+z3OuXl5cybN4/zzjuPAQMGcNttt5Gfn1/vYwLtIZKoQDW8/vrrPPzwwzz55JN88MEH3HHHHdx5552899577Nmzh/fffx+AlStXsnz5cubOncvKlStJTEzkmmuuoaioCID58+fz559/snz5chYtWsR7773n9zp/+9vfMJvNrFixgkWLFvHNN9+wYMGCRrnm5+dzww03kJKSwsqVK7nzzjt59tln+fTTTwFYunQp2dnZvPnmm7z11lvk5eXxxBNP1PuYQINIAkGIOXjwoJSYmCj973//82274IILpOeee853/+6775bmzZsnSZIkXXTRRdKbb77pt4/09HTpzTfflA4dOiQlJSVJu3bt8j22bds2KTExUTp48KC0ZcsWaciQIZLL5fI9/u2330oDBw6UnE6n9OGHH0rDhg2r1bPqY4sXL5YuueQSv8dfffVV6dJLL5UkSZJuvfVW6eabb5bcbrckSZK0Z88e3/ur6zGB9jCGOokLBF569erlu202m+nWrZvvvtFoxOFwUFFRwcGDBxk4cKDfc/v27cuBAwfIzs4mLCyMpKQk32NnnXUWBoMBgKysLAoKCkhJSfE9LkkSTqeTkydPNtg1Ozubc889t4bDkiVLAJgxYwa33347F110EUOGDCE1NZWLLrqo3scE2kMkUYFqsFgsfvf1+pqjTXa7HZCTbFVsNhtdu3bF5XKh0+n8HvN4PHg8HgCcTicJCQksXry4xr47dOjQYFe73Y7JZKqxzWq1AjB48GC++uorvvnmG3744QceffRR3n77bd5///06HxNoDzEmKtAUMTExxMTEsH37dt82t9vNtm3bSEpKolevXpSVlZGdne17fPv27UiVMz4mJCRw7NgxunbtSu/evenduzeHDh3i+eefx2hseJ8iISHBzwFg69atvh7www8/zN69exk7diwPP/wwS5cu5ddff6WwsLDOxwTaQyRRgea45pprePHFF1m/fj07d+7koYcewmg0Mm7cOHr37s3w4cO5//772bp1K1u3buXxxx/39RovvPBCunfvzoMPPsjOnTvZuHEj8+fPp2/fvo1ymD59On/88QcvvPACu3bt4uOPP2bFihVcf/31ABQXF/PEE0/w888/k52dzcqVK+nRowcxMTF1PibQHiKJCjTHzTffzIQJE3jwwQe58sorOXToEMuXL/cd4j/11FN07dqV66+/nrlz53LrrbcSEREBgMFgYOnSpZSUlDBt2jTmz5/PxIkTufXWWxvl0KNHD15++WW++OILJk+ezJIlS7j//vsZMWIEAA888ABdunThtttuY/r06Rw5csQ3XlrXYwLtoZMkMbO9QCAQNBXRExUIBIJmIJKoQCAQNAORRAUCgaAZiCQqEAgEzaDNFNvbbDays7OJjY1tVD2gQCBo27hcLvLz80lMTPRdTFGVNpNNsrOzmTJlSqg1BAKBRsnIyKB///41trf6JLpjxw4A8vLyADkQnTt3rvd53ssLq1+KqCa04Aja8BSOyqAFR2ic5/Hjx5kyZQqxsbG1Pt7qk6gX7wQUnTt3pmvXrvW2916Cp+arSLTgCNrwFI7KoAVHaJpnoGHAVp9E+/XrB8CxY8dCbCIQCFojrT6JNhVvz1XNaMERtOEpHJVBC46grKdIogGoPtWaGtGCI2jDUzgqgxYcQVnPFq0TdTgcpKamUlxcDMDu3bu58sorGTt2LOnp6TWWcfCSmZnJpEmTGDlyJNOmTWPfvn1Bd3U6nTidzqC/TnPQgiNow1M4KoMWHEFZzxZLoosXL2bq1Knk5ub6tt11111cf/31rF27lrfffpuFCxdy/Phxv+d5PB7mzJnD7Nmz2bBhA9OnT2fevHlB93W5XLhcrqC/TnPQgiNow1M4KoMWHEFZzxZLosnJycyaNctv2y233MLIkSMBKCkpwWAw1Ohm79y5E7fb7Zti7JJLLiE7O9svGQsEAkGoaLEx0bS0tBrbxo8fD8DYsWM5cOAA06dPr1FykJOTQ3x8vO++0WikS5cu5Obm0qVLl0Z7FBUVER4ejsViweVy+br0FosFvV5PRUUFIA89mM1mSktLfb9Y3qsVvMvrepO+0+kM2MZoNGIymXA4HLjdbgDCwsLweDy+WjVvG7vd7lvGonobk8mE0Wj0tfEugeF2u33lGmazGYPBgM1mQ5IkdDodVqsVt9uNw+GotY1er683Ft42Vd9n9TZ1xcLbprZYNCVe1WPR0HjVFYuKigp0Oh12u90vFk2JV22xaEy8AsWioqICq9WKzWZTJF5VY9HYeAWKhdvtxmAwUFRU1Ox46XS6Zn/XrFYrkiTV+K41Jl7e4cdAqOLE0tq1azl8+DDXXnst69evZ9SoUb7HdDpdjfosvV5f59m1jIyMGmuNez8IDUWn09VYq6eleeWVV7j66qsDFgRXd3zppZfo3bs3Y8aMaSnFBhHqODYENfy960M4Koeini29vGhiYqJUVFQkHT9+XLrtttv8HnvggQekN954w2/btm3bpPHjx/vuu91uadCgQVJBQUGjXvfo0aNSYmKidPTo0Qa1/+D73dJFCzc06jWUxhsrL1WX+ZUkSSooKGh0HEKBFjyFozJowVGSGudZX+4I2SxO7du35/fff2fTpk0AnDhxgh9++KHGMrTJycnYbDa2bt0KwIcffkhKSkrQr4g4UeLgeEnjeq9KcssttwAwZcoUkpKSWLRoEePGjQPgk08+Yfz48UydOpWrr76aLVu2AHD//ffz1ltvAZCUlMTixYuZPHkyI0eO5OOPPw7J+xAIWjshO5w3Go288MILPP744zz55JOYzWZuu+023wX+6enpPPPMM/Tv359FixYxf/58SktLiYuL49lnnw2q26GCck6WO3G5PWQerXs8pCm0sxrp3j68zjbLli0jKSmJjIwMBg0aRKdOnVi7di0ej4fly5fz1ltvYbVaycjI4LXXXmPQoEG17uc///kP33//PXfddReXX3654u+lIWihAFs4KoMWHEHjxfZZWVm+2wMHDuSjjz6qtd2aNWt8t/v3788nn3wSbDUAisqdDH/2K1weeempi1/crPhrGPU6fn5oNNHhpvobV+I9CafX63njjTfYsGEDf/75J9999x3t2rWr9TnTp08H4KyzziI/P7/54k1ECwXYwlEZtOAIynqq4sSSmogON/HVvcN5+5s9vPvjYT66/QLFX6Od1dioBAqnJj/4888/+etf/8qsWbMYNmwYPXr0YPXq1bU+x7tMcKgH+quelVUrwlEZtOAIynq2+iRafSq8htC9fTidIkyggzNPiwqWWr3o9foaV1VkZ2fTuXNnpkyZwokTJ3xjoGpGK8XXakc4KoeSnmJ5kADodTpCvZZ0ampqjYmkU1NT6dSpE2lpadx0000MGzaM/fv38/nnn4fIUiBo27T6nmhTp8IzmUxIIc6ir776ao1tFouFZcuWAaeKjP/6178C+M7eg//Yc1RUlN/9lqa2JRXUhnBUBi04grKeoicaAJ2OkCdRgUCgfkQSDYDb5UIK+QF93VS9/E/NaMFTOCqDFhxBWU+RRAOgQ/REBQJB/bT6MdGmYjQakFSeRbWy9LMWPIWjMmjBEZT11MY7DgFGowFPqCXqwVsHqna04CkclUELjqCsZ6tPok2pEwXwuN2qP5z3zkyl9sJmLXgKR2XQgiMo69nqk2hTkSrnXZQq5+VUI975ENWOFjyFozJowRGU9Wz1J5b69etHv379SEpKatwTK/Om2nujXkaOHElmZibbt2/n9ttvr7VN1VmeAnHo0CFWrFgBUOe+BAKBTKtPok3FUjlBgZpzqNVqrVE03L9/f5YsWdLkfR4+fJj3339fkX15qc1TbQhHZdCCIyjrKZJoPXhC1BX961//6rf66T/+8Q9eeOEFnn76acaOHcvo0aO56aabaizs9+OPP3LZZZcBUFhYyC233MLo0aO58sorOXz4sK/d119/zWWXXUZ6ejrjxo1j7dq1FBcXM3fuXPbt28edd97pt6/S0lLuv/9+xo4dy5gxY1i6dCkg91xTU1N57LHHmDRpEmPGjOGbb77xc5IkSfWVDsJRGbTgCMp6iiRaG4U5RBZk0leXA8f+UP5fYU69CpdffrlvOkC32826desYPXo02dnZrF69mi+++ILIyEg+/fTTgPtYsGABvXv35osvvuD5558nOzvb99jy5ct5+umnWbNmDXPnzuXFF18kKiqKBQsW0KtXL55//vka+4qMjGTt2rV89tlnfP31177Xzs3NJTk5mY8++ogbb7yRF154we+5drvdt36NWhGOyqAFR1DWU5xYqk5FAbw0gHSPi3QLsDwIr6E3wr17IKx9wCbp6ek8+eSTnDhxgl27dtG7d2/69evHvffey7vvvsuRI0fYtm0bCQkJAfexceNG/vOf/wBw2mmnkZqa6nvspZde4quvvmLNmjX88ssv9V69sWHDBt9YqdVqZcKECfzwww/85S9/wWq1MnHiREAegw7l3KUCQUsjkmh1wtrDHb+y/udMFq7fx6e3X4DFqHCH3RpVZwIFiIiIYNSoUaxdu5Y//viDK664gv/+97+89tpr3HrrrQwZMoSysjL0+sBubrfbr7LAu9JjUVEREydOZMaMGaSmpjJkyBAefPDBOn3sdrtfgXJFRYVvYtuqNXe1VTJooQBbOCqDFhxBFNs3iibVicb0oCJWxy7JhadzPzCHZsmDyy+/nOeee44TJ07wyCOPsHDhQoYOHcrFF19Mbm4uP/30k9/MTdW54IIL+OCDD7jrrrs4fPgwX3/9NcnJyRw+fBiPx8NVV12F0WjkySef9D3HYDDUmMMUYOjQoaxYsYK7776bsrIyPvvsM+6+++4GvQ8tFGALR2XQgiMo6ynGRAPgrpy0NVQnlgDOO+888vPzSU1NxWq1cvXVV/Pdd98xcuRIHnjgASZPnszKlSvJyal9jPWhhx4iMzOTMWPGMH/+fM4//3wAzjzzTFJTU7nooouYOHEiZ555Jjqdjn/+858kJSVRWlpaI0H+/e9/Z8+ePYwePZqpU6cyceJE0tLSGvQ+HA5Ho5esbmmEozJowRGU9dRJWjiVpgDHjh0jLS2NTZs20bVr13rbf/zTXu78aBd//GMskRZ1dtgLCwsBgr7yaXPRgqdwVAYtOELjPOvLHaInGgDv0F4oe6ICgUD9qLOLpQKsldfUqjmHhoWFhVqhQWjBUzgqgxYcQVlPkUQD4B3lUPNoh/dsu9rRgqdwVAYtOIKynuJwPgAul3yGWsU5tE0WNgcL4agMWnAEUWzfKJo6FZ6+clBUxTlUIBCogFafRJuKtxhXzSeW2mJNXrAQjsqgBUcQkzI3iiYvmWyUC+xVnEPb5NUhwUI4KoMWHEFZTzEmGgCX0zsmqt4s2hbHn4KFcFQGLTiCGBNtGbxn50OsURdt8UxosBCOyqAFR9Dw2XmHw0FqairFxcWAPAHwddddx/Dhw0lPT+edd96p9XlLlixh1KhRpKenk56ezn333Rd0V51ePrGk5jFRnU6n2qVLqqIFT+GoDFpwBGU9W6wnunjxYtavX09ubq5v22OPPUZKSgpvv/02BQUFXHHFFaSkpNC/f3+/52ZlZbFw4UJSUlJaSvfUzPbqzaGamEEctOEpHJVBC46grGeL9USTk5OZNWuW37bS0lKmTZsGQPv27enXrx9ZWVk1nnvw4EHi4+NbxNOHBg7n3W63JhYG04KncFQGLTiCsp4t1hOtbcafd99913f7u+++4/vvv691erVDhw4xb948cnJy6NatG/feey99+/ZtkkdRURHh4eFYLBZcLpdv2jeLxYJer6eiogLAN+RQWlpGIfIAtPfXyzuBscFgwGw243Q6cVXO+lS9jdFoxGQy4XA4fH+0sLAwPB6Pb2Db28Zut/vGaqq3MZlMGI1GXxudTofNZsPtdvtmozGbzRgMBmw2m2+VUqvVWmcbvV5fbyy8baq+z+pt6orFiRMnAOjQoUONWDQlXtVj0dB41RUL74QUXbt29YtFU+JVWywaE69AsSgqKvLtR4l4VY1FY+MVKBbl5eW+6RSbGy/vZ7yp8fK2kSSpxnft5MmTeDweoqOj642XNxcEIuQnlnJzc3n22Wf56aefeO655+jZs6ff43a7nQkTJnDNNdeQkJDA6tWrmTlzJhs3bvRNClydjIwMMjIy/LY1dtornW+1TzX3RQUCQciRWpjExESpqKhIkiRJ+vXXX6UhQ4ZITz/9tFRSUlJre7vdLpWVlfltGzx4sJSTk9Oo1z169KiUmJgoHT16tEHtN2celk6f+1/pQF5po16nJSktLZVKS9Xr50ULnsJRGbTgKEmN86wvd4S0TnTevHnMnTuXuXPnEhkZWWubHTt2MHbsWN+6PZs2bSIiIoK4uLiguhk1UmyvheJmLXgKR2XQgiMo6xmyd1tQUMC+fft45ZVXWLZsmW/7nDlzfKVMzzzzDAMGDGDGjBlMnToVk8lEbGwsS5YswWAI7pIdvmL7oL5K8/CO4Vgqp+1TK1rwFI7KoAVHUNazxZNo1bPvtZ2J9+JdLhhg5syZzJw5M6he1ZEqu6BqrhNti4XNwUI4KoMWHEFZT/X3u0OEobLYXsU5VBNFzaANT+GoDFpwBGU9W30SbepUeBbfzPbqzaJtsbA5WAhHZdCCIyjr2eqTaFORKrv76k2haKKoGbThKRyVQQuOoKxnq0+iTZ0KzzuzvZrHRL21r+Hh4SE2qRsteApHZdCCIyjrKabCC4AO9Y+JCgSC0NPqe6JNxWKWZ75WcxINdMWW2tCCp3BUBi04grKeIokGwFuHqubD+WDXyiqFFjyFozJowRGU9RRJNABOZ+OutQ8F3okX1F7YrAVP4agMWnAEZT1FEg2EBort1Vx+VRUteApHZdCCIyjr2eqTaFPrRA16+Zybmj8Ter02zgtqwVM4KoMWHEFZz1afRJuKxSIPPKu5J6r2QyYvWvAUjsqgBUdQ1rPVJ9Gm1ol6Kotx1ZtC8ZvAVs1owVM4KoMWHEFZz1afRJuK2y0HWcUdUd9M4WpHC57CURm04AjKempjACMEeCco0MpAuUAgCA2iJxoAX7F9iD3qoi0WNgcL4agMWnAEUWzfIviK7T3qTaNtsbA5WAhHZdCCI4hi+xbBbpNnvlZvCsW3WqLaB/G14CkclUELjqCsZ6tPok2tE9X7VvtU2kggELQmWn0SbSp6g7fYXr1ZtC0WNgcL4agMWnAEUWzfKJpaJ2r1zmyvuJFyqP2QyYsWPIWjMmjBEZT11MbPRgjw1omq+Yoll8vlKxpWM1rwFI7KoAVHUNaz1fdEm4rHJYrtlUILnsJRGbTgCKLYvmWoLLZXc09UIBCEHtETDYC1cgISNafQtjj+FCyEozJowRHEBCSNoulT4VUW46o4i7bFM6HBQjgqgxYcQZydbxHsdnnmazUfzrfFwuZgIRyVQQuOIIrtG0VTS5wqa+1VfWJJIBCEnlafRJuK0SgWqlMKLXgKR2XQgiOIa+dbBO8sL+pNoW1zxpxgIRyVQQuOoKxni44COxwOUlNTKS4uBuDw4cNcd911DB8+nPT0dN55551an5eZmcmkSZMYOXIk06ZNY9++fUF3dfvqRNWbRp1Opybq8rTgKRyVQQuOoKxniyXRxYsXM3XqVHJzc33bHnvsMVJSUvjqq69YsWIFb731Ftu3b/d7nsfjYc6cOcyePZsNGzYwffp05s2bF3RfLcxs3xavDgkWwlEZtOAIynq2WBJNTk5m1qxZfttKS0uZNm0aAO3bt6dfv35kZWX5tdm5cydut5sRI0YAcMkll5Cdne2XjIOB3juzfVBfRSAQaJ0WGxNNS0urse3dd9/13f7uu+/4/vvvufvuu/3a5OTkEB8f77tvNBrp0qULubm5dOnSpdEeRUVFhIeHY7FYcLlcvi69xWJBr9f7Sh9cldvLKyooLCwEwGq1AmCzyeVPBoMBs9mM0+n0/apVb2M0GjGZTDgcDtyVi9+FhYXh8Xiw2+1+bex2Ox6Pp9Y2JpMJo9Hoa6PT6dDpdLjdbp+f2WzGYDBgs9mQJAmdTofVasXtduNwOGpto9fr642Ft03V91m9TUNiUVpaWiMWTYlX9Vg0NF51xcJms6HT6bDb7X6xaEq8aotFU+JVPRY2mw2r1erzbW68qsaisfEKFAuPx4PBYKCoqKjZ8dLpdM3+rlmtViRJqvFdczqdvu9OffHyDj8GIuQnlnJzc3n22Wf56aefeO655+jZs6ff4zqdDqPRX1Ov19d5di0jI4OMjAy/bd4PQkMR84kKBIKGENIk+ttvv3HrrbcyceJEPv/8cyIjI2u06datG0ePHvXd93g85OXl0a1bt4D7nTJlClOmTPHbduzYMdLS0oiOjiYqKgqovdDWu82VXwCA2WIhJibGr433F7D6cxrbJiwsrFltCgsL0ev1Nfxq2094eHidbeqKRXPbeGPh/fuGKl5eaouF9wSixWIJaiwaE6/qbaoeESkVr/o+Fw1pU/0zKUlSgz6TzYlFY9tUj4XJZMJkMvl5BoqXt6cbiJBeozVv3jzmzp3L3Llza02gII+l2mw2tm7dCsCHH35ISkpKjT+S0ujqbyIQCASh64kWFBSwb98+XnnlFZYtW+bbPmfOHNLT00lPT+eZZ56hf//+LFq0iPnz51NaWkpcXBzPPvts0P1MlcX2aj6cb4uFzcFCOCqDFhxB48X2Vc++Vz8TX5U1a9b4bvfv359PPvkkmFo18Hbl1XzFUlssbA4WwlEZtOAIGi621xJOpxMd6u6JtsXC5mAhHJVBC46grGfIz84Hm6ZOhedyudDp1N0T1UJRM2jDUzgqgxYcQVlP0ROtA71OJ4rtBQJBnbT6nmiTV/u0WisP59WbRquXdqgVLXgKR2XQgiMo6yl6onWhU/eYqEAgCD0iiQbAZrOp/nC+6uV/akYLnsJRGbTgCMp6iiRaBzrUfWJJIBCEnlY/JtpUjEYjOpUfzlefU0CtaMFTOCqDFhxBWU9tvOMQYDKZ0Ol0qj6xZDKZQq3QILTgKRyVQQuOoKxnq0+iTa0TdTgc8tn5IDgphXdmKrWvrKgFT+GoDFpwBGU9W30SbSput1sutveoN41650xUO1rwFI7KoAVHUNaz1SfRptaJAqrviQoEgtDT6pNoUwkLC8Og16v6xFL1+Q/VihY8haMyaMERlPUUSTQA3mUU1Fzi5HVUO1rwFI7KoAVHUNZT1IkGwG63o1P5zMx2u923Loya0YKncFQGLTiCsp4iidaBKLYXCAT1IQ7nAyCK7ZVDC57CURm04Aii2L5RNLVO1GQyoder+9r5tljYHCyEozJowRGU9RSH8wFwOBzoJHUfzjscjkYvBR0KtOApHJVBC46grGer74k2tU7UW2yv4hzaJgubg4VwVAYtOIKynqInWgdyElVxFhUIBCGn1fdEm0pYWBh6nSi2VwIteApHZdCCI4hi+xbB4/FULlQXapPAtMXC5mAhHJVBC44giu1bBLvdDpKEpOLz822xsDlYCEdl0IIjKOvZ6nuiTS1xgsrVPtWbQwUCgQpo9Um0qZhMJnR6MSmzEmjBUzgqgxYcQUzK3CiaWuJkNBpVv1BdW7w6JFgIR2XQgiMo6ynGRAPgHRNVc7F9Wxx/ChbCURm04AhiTLRF8J6dV3EObZNnQoOFcFQGLTiChs/OOxwOUlNTKS4u9tv+3nvv8cQTTwR83pIlSxg1ahTp6emkp6dz3333BVsVnU4nL1QX9FdqOl5HtaMFT+GoDFpwBGU9G9UTlSQJp9OJ2WwmLy+P3377jaSkJOLj4+t97uLFi1m/fj25ubm+bdu3b+e9995jzZo1TJ06NeBzs7KyWLhwISkpKY3RbRZWqxWDXqfqw3mr1RpqhQahBU/hqAxacARlPRvcE/39998ZOXIkn332GYWFhUyYMIH77ruP8ePHs2HDhnqfn5yczKxZs/y2tW/fntGjRzNy5Mg6n3vw4MEGJWolcbvd6EDViyy53W5NXKusBU/hqAxacARlPRvcE3300UcZNmwY6enprFq1itjYWD755BNWrFjBiy++WG8iTEtLq7EtPj6e+Ph4MjMzaxziV+XQoUPMmzePnJwcunXrxr333kvfvn0bqu5HUVER4eHhWCwWXC4XTqcTkJdO1ev1VFRUAFBSUoLH48HmcFBYWAic+vWy2WwAGAwGzGYzTqcTl8tVaxuj0YjJZMLhcPj+aGFhYXg8Ht/AtreN3W73jdVUb2MymTAajb42Op0Om82G2+32zUZjNpsxGAzYbDYkSUKn02G1Wutso9fr642Ft03V91m9TV2xOHHiBAAdOnSoEYumxKt6LBoar7pi4f0bd+3a1S8WTYlXbbFoTLwCxaKoqMi3HyXiVTUWjY1XoFiUl5djMBhwOp3Njpf3M97UeHnbSJJU47t28uRJPB4P0dHR9carrtwEjUiiWVlZPPXUU0RGRvLdd98xZswYTCYTF154Ic8//3xDd9No7HY7EyZM4JprriEhIYHVq1czc+ZMNm7ciNlsrvU5GRkZZGRk+G1ryrRXerHcp0AgqIcGJ9EOHTqQl5dH586d+eGHH7j22msB2LdvHzExMcHyQ6fTcc899xAeHg7A+PHjefTRR8nNzQ14iD9lyhSmTJnit+3YsWOkpaURHR1NVFQUIP/aVce7zWQyYTAYMBhNNd5f9fGU2vbTkDbVJ0FobBtvrVtERES9+/HGL1CbumLR3DYdO3b08wxVvLzUFgtvLC0WS1Bj0ZA2gWLhLRC3Wq2Kxau+z0VD2lS973VsyGeyObFobJvqsYiNja3hGShe3p5uIBo8Jnr11Vcze/ZsLr30Urp27cq5557Lxx9/zCOPPML48eMbuptGs2PHDsaOHUt+fj4AmzZtIiIigri4uKC9JlQttldvV9RoNGqiuFkLnsJRGbTgCMp6Nngv//d//0diYiL79+9n3Lhx6HQ6ysvLmTVrVo1enxKkp6fzzDPPMGDAAGbMmMHUqVMxmUzExsayZMkSDAaD4q9ZFZvNJk9Aot4c6vuFrO2XV01owVM4KoMWHEFZz0al4mHDhjFkyBBfiVOXLl1ISkpqVELLysqqsW327Nk1tq1Zs8Z3e+bMmcycObMxqs1GkiRQ+aTManarihY8haMyaMERlPVssRInraHT6VR/7XxbLGwOFsJRGbTgCCEqtm9uiVOoaOpUeHKxvV4U2yuAFjyFozJowRFCVGyflZXFtddeW2uJU05OjmJCakGuM1P3mGhbLGwOFsJRGbTgCMp6NjiJekuciouL+eGHHxg6dCgQ/BKn5tKvXz/69etHUlJSo54n15VKql4epC0uTxsshKMyaMERQrRksrfEKSIiwq/E6bnnnmPixImKyKgNHaLaXiAQ1I1qS5xCjdlsxqBX9/Igga7YUhta8BSOyqAFR1DWs1ElTqmpqQwYMIADBw5QVFTE5MmTVV8P1lQMBgN6lc/iFOxaWaXQgqdwVAYtOIKyng1Oog6Hg8cee4wPP/zQN4lBeHg406dPZ86cOZpZW6WheCdZUHEObZOFzcFCOCqDFhwhRMX2zz//PH/88QcrVqzgzDPPxGazsWXLFp566ikcDgcPPvhgs2XUhCRJ6FD3uvNtsbA5WAhHZdCCIyjr2eAkumbNGp5//nnOOeccQB5TuOiiiwgLC+Oee+5RbRJtap2oXq+vnNlevR8KvV4bS2RpwVM4KoMWHEFZzwYn0cLCwlpfOCoqirKyMsWE1ILFYsFo0Kv65LzaD5m8aMFTOCqDFhxBWc8Gp+Nhw4bxwgsvUFBQ4NtWWlrK0qVLGThwoGJCStPUOlGXy4Wk8tU+XS6Xb4JaNaMFT+GoDFpwBGU9G9wTnT9/PrNnz2bYsGEkJCRgMpnYv38/Xbp0YdmyZYrIqAmn04m6D+bxzRSudrTgKRyVQQuOoKxnnUn04MGDfvefeeYZcnNzyczMxOFwYLVaSU1NVUxGfehUfWJJIBCEnjqT6OjRo2ud6cR7Zkun0/nW8snMzAyOYYjwjomq+WxjWxx/ChbCURm04AjKetaZRL/88kvFXkhr6PV6eSo89ebQNnkmNFgIR2XQgiO04Nn5bt26KfZCWqOiogKPx43UuIu6WhTvaolq//XXgqdwVAYtOIKynurNEArR1DpR8A5XKG0kEAhaE60+iTYV7+G8mkuc2uKhU7AQjsqgBUcIUbG9VunXrx8gL5ncGE6dWAqGlTKo/ZDJixY8haMyaMERQlRs39Y4VWwfapPAtMXC5mAhHJVBC44QomL7tobT6axc8VO9WbQtFjYHC+GoDFpwBGU9RU+0DvQ6dc/iJBAIQo/oiQbAYrFgMBhwutSbRdvi+FOwEI7KoAVHaMFi+9ZAc6bCM+jVffV8WzwTGiyEozJowRHE2fkWoaKiArfbjUeqedmrWmiLhc3BQjgqgxYcQRTbN4qmljgBlZd9qrcnKhAIQk+rT6JNxbtQnZqTaFtcFCxYCEdl0IIjhGihuraGvGSyAQl3qFUC0haXpw0WwlEZtOAIynq26Ciww+EgNTWV4uJiv+3vvfceTzzxRMDnZWZmMmnSJEaOHMm0adPYt29fsFXlOjLJo+orlpxOpybq8rTgKRyVQQuOoKxniyXRxYsXM3XqVHJzc33btm/fzty5c3nmmWcCPs/j8TBnzhxmz57Nhg0bmD59OvPmzQu6r1geRDm04CkclUELjqCsZ4sl0eTkZGbNmuW3rX379owePZqRI0cGfN7OnTtxu92MGDECgEsuuYTs7Gy/ZBwszEY9Nqcn6K8jEAi0S4uNiaalpdXYFh8fT3x8PJmZmTUO8b3k5OQQHx/vu280GunSpQu5ubl06dKl0R5FRUWEh4djsVhwuVy+Lr3FYkGv1/tKH5xOJx0iTHy/v5DCwkIArFYrADabDZAHp81mM06n0/erVr2N0WjEZDLhcDhwu+Xx1bCwMDweD3a73a+N3W7H4/HU2sZkMmE0Gn1tdDodOp0Ot9vt8zObzRgMBmw2m2/FAavVitvtxuFw1NpGr9fXGwtvm6rvs3qbhsSitLS0RiyaEq/qsWhovOqKhc1mQ6fTYbfb/WLRlHjVFoumxKt6LGw2G1ar1efb3HhVjUVj4xUoFh6PB4PBQFFRUbPjpdPpmv1ds1qtSJJU47vmdDp935364hUoN3lR/YklnU6H0eivqdfr6zy7lpGRQUZGht827wehMbQPN5Ffrv7xHYFAEDpUn0S7devG0aNHffc9Hg95eXl1zro/ZcoUpkyZ4rft2LFjpKWlER0dTVRUFFB7oa13W2FhIe3DTZQ73Fgj2mE1nUra3l/A6s+pSkPahIWFNatNYWEher2emJiYevcTHh5eZ5u6YtHcNt5YREZG1tumrv00N15eaouFt5TNYrEENRaNiVf1NlWPiJSKV32fi4a0qf6ZlCSpQZ/J5sSisW2qx8JkMmEymfw8A8XL29MNhOqv0UpOTsZms7F161YAPvzwQ1JSUmr8kYJB+zATACfLGt+LFQgEbQPVJtH09HS2b9+OTqdj0aJFPP7444waNYpVq1bx2GOPBf31DQYDndrJv0QnS+1Bf72mYDAYNFHcrAVP4agMWnAEZT1b/HA+KyurxrbZs2fX2LZmzRrf7f79+/PJJ58EU6sGZrOZLjHybbX2RNtiYXOwEI7KoAVH0HCxvZZwOp1EmOQ5RU+WqjOJtsXC5mAhHJVBC46grKfqTyw1l6ZOhecto4iNMKv2cF4LRc2gDU/hqAxacARlPUVPtB46RFhUezgvEAhCT6vviTZ1Kjxv2URcjJVDBeWKeylB9dIOtaIFT+GoDFpwBGU9RU+0Hnp2jGDfibJQawgEApUikmgAvJfW9eoYwZ8ny/GocMW6qpf/qRkteApHZdCCIyjrKZJoPSR0jKTC6Sa3RP0fDIFA0PK0+jHRpuK9Xj+hk1xwv/9EGadFh9X1lBan+pwCakULnsJRGbTgCMp6ip5oALzX1p4WZcVq0rPnRGmolWrgdVQ7WvAUjsqgBUdQ1lMbPxvNoKl1ot5ZnywWC2edFsUfh4sUd2suVR3VjBY8haMyaMERlPVs9Um0qXjnIwRI7hbNj/vzQ2hTO1Ud1YwWPIWjMmjBEZT1bPVJtDlLJns5u1s07/6Yg83p9psSTyAQCFp9Em0qVecW7N89BrdHYseRIv5yemwIrfypPv+hWtGCp3BUBi04grKe4sRSADwej28phT6dI2lnNbLlQEGIrfyp6qhmtOApHJVBC46grKdIogGw2+2+NVcMeh3nnt6erQfUNS5a1VHNaMFTOCqDFhxBWU+RRBvIoIRYthwowOVW/6+sQCBoOUQSDYDRaPQryB1zVhfK7C6WbdobQit/qjuqFS14Ckdl0IIjKOup/nfbTJpaJ1q9ELdP53bcktabpV/tZeaFvQgzh/4svRaKmkEbnsJRGbTgCMp6ip5oAOx2O67dG2DHJ75tVw/pQYXTzfrM3NCJVaEtjj8FC+GoDFpwBGU9W31PtKl1oh6Ph6j/XFm5E/lqpdOiwxjcM5aVWw8y4Zw4RT2bghbOgoI2PIWjMmjBEZT1FD3RQLhrn83+hgsT2Lw7j98OFrasj0AgUCUiiQYgvLDaqqQVco3o6DO7cE73aG5992cOF1aEwOwUYWFhmihu1oKncFQGLTiCsp4iiQZAKqk8/A+LhZJceLYPHM9Er9fxzxmDcLo9/PvHP0Pq2BYLm4OFcFQGLTiCKLZvESp6jqY87RFwVkDZCfC4oDKxdoi0kH52V/73+zEkKXQz3rfFQfxgIRyVQQuOIIrtG8WOHTvYsWMHWVlZ9TeuhhQWA64KcFTOJeo8dfg+Lvk09uWV8fSaXSFNpAKBILS0+rPzTcVkMmGwRsl3yk/K/ztPrfp5fq8OPDT+TB5fncmBvDLSEjtz1Xk9WtxRC2jBUzgqgxYcQVnPVp9Em1riZDQa0YVVJtGyE/L/VXqiOp2O/xvWi03ZJ1i7I5e1O3KZPjgenU6niHdDHbWAFjyFozJowRHE8iAtgt1ux4lZvlNLEvXy8vQBPHl5MgBvfLMfu6vlJqVti+NPwUI4KoMWHEGMibYIHo8Ht7GyBKKs8pLRKofzXmLCzVw5KJ64aCuPr87kgqc3MvOtLbhbYInltngmNFgIR2XQgiNo+Oy8w+EgNTWV4uJiAIqKirjlllsYMWIEY8eO5csvv6z1eUuWLGHUqFGkp6eTnp7OfffdF3RXnU4H5gj5Th09UQC9XseaO1NZfceFTBrYjS93HWfUok18tu1I0B1bcvigqWjBUzgqgxYcQVnPFhvAWLx4MevXryc399R15wsWLCAhIYFly5axb98+pk+fzrp164iOjvZ7blZWFgsXLiQlJaWldLFareCpnMXel0Rr9kS9RFlN9IuLpl9cNJlHi9m8O497Vm6jV8cIzu4WHfB5zXbUAFrwFI7KoAVHUNazxXqiycnJzJo1y3ff4/Gwbt06rrvuOgB69epFcnIymzdvrvHcgwcPEh8f31KqgLyQldtQGWjf4XzDrlBacvVAvpk7guTu0dz0zlbuWPErNqfyY6Vut1sTC4NpwVM4KoMWHEFZzxbriaalpfndLygowGaz0bVrV9+2uLg4jh49WuO5hw4dYt68eeTk5NCtWzfuvfde+vbt2ySPoqIiwsPDsVgsuFwunE4nIC+dqtfrqaiQE2VJSQlmk4kwvRGp9Dh6wGUrwWWzYbPZADAYDJjNZpxOJy6XC5B/4cxApM7B4+N6M+ejTD7bdgQjHob1jiE5rh3xnaJxu92+ZVuNRiMmkwm73e4bpwkLC8Pj8fgGv00mE0aj0ddGp9Nhs9n89mM2mzEYDNhsNiRJQqfTYbVa62yj1+vrjYW3TdX3Wb1NoFgAnDgh9+Q7dOiAyWTC4XD4PsDeNt6YemNRtU19sWhovOqKRWFhIQBdu3b1i0VT4lVbLBoTr0CxKCoq8u1HiXhVjUVj4xUoFuXl5RgMBpxOZ7Pj5f2MNzVe3jaSJPli4W1z8uRJPB4P0dHR9cbLO/wYiJDVI+h0OgwG/zk59Xp9jdIDu93OhAkTuOaaa0hISGD16tXMnDmTjRs3Yjaba913RkYGGRkZftu8H4RGSoIpAn253BPVuRp3rfxp0RY+vPFcHlyVxUfbjvHRNrnMKibchEEH7113Dl2j1L0+t0AgqBud1MKX2yQlJbFlyxbatWvHwIED+fLLL4mNlcceb7nlFqZMmcJFF13ka+9wOHC5XISHh/u2nXfeefznP/9p1CH+sWPHSEtLY9OmTX6930CUl8vjn+HLzoXiw/LGxIvhqvcb/Jpeisqd7DhaxBmd2/HbwUKyc0v494859OoUwTs3DG7yALfPsUps1IgWPIWjMmjBERrnWV/uCFmJk06nY+zYsbz/vpyUsrKyyMrKYujQoX7tduzYwdixY8nPlxeJ27RpExEREcTFBXc+T4PBIPeUzZGnNtZxYqkuosNNDO3dkU7tLIw+qwu3j+jDs1P6s3l3HuNf+oZHV+3khre28OfJsqY5qhwteApHZdCCIyjrGdLLC+677z7uu+8+Ro8ejcViYcGCBb7pqdLT03nmmWcYMGAAM2bMYOrUqZhMJmJjY1myZEnQ/1DecRaLt8wJGnxiqSEM7d2Re8cm8cO+k/zz2/2YjXqmv/YDt47ow3kJsSR2addwR4u6hwS04CkclUELjqCsZ4sn0aoTgcTGxvL666/X2m7NmjW+2zNnzmTmzJlBd6uKb5Sj3WmnNiqYRAFuH9GH20f0Yc/xUiIsBv7+yR888tkO3B6JC/t05MbUXqQldqrfUeVowVM4KoMWHEFZT21c6BoCfOOU0d1ObWzi4Xx99OksDxm8ft0g3B6JL3Ye441v9nPdP3+ib9d2dImyctV5PThSWMGkAd2JDj81eYJWCpvVjnBUBi04grKerT6JNnW1T18xblTVJBr8mewNeh3pZ59G+tmnsW7HMb7MPM7+vDJu/tfPAPzz2/1MHhjP6t+PsD+vjDtG9Obm4e2QkMg+Vsrnfxzl6vN60L29egb2tVCALRyVQQuOoKxnq0+iTcVXiBvd/dTGIPVEAzGmX1fG9OuKJEn8cbgYg17Hq1/v5e3vD9CncyRXD2rPc+v38MKGvYSZDJQ5XJj0ejK2HiL1jI7YXG7MBj1De3ek72ntyCu1s/NIMVaTgfN7d6B7TLhfrzZYaKX4Wu0IR+VQ0rPVJ9GmToXnrSsNb+GeaG3odDqSu8uXjr545QDf9sLCQsb0jWVfoZu8Urvv7P8rX+1lx5FijHodDpeHBz7+HVflhChRViNuj8Tjq93odXBJ/zjGJXflo18OM2tkH77dc5Lv9uYxoX8cl6bEYTWdOoEnSRK7j5fSIzac7YeK6N4+jLiY+tep8cVSxWUvwlEZtOAIynq2+iTabKqOibrt4HGDXj0lHGd1jWRo3xi/bQ9P6Od3v8Tm5EBeOd3ahxFpkf/ku44Vk3m0mGfXZvPZtiO0sxr5dk8eZQ43A3rEMO/j33nzuwMUV8hXmSR3iyY6zMQHWw8SaTFSanfRp3Mko87swpRzu9OzQwROt4c3vtnPadFWrCYDndtZsLs89Osov6bHI2F3eQgzy/E7XmKjc7tTh1WHCsqJiw5Dr9fGuJpAACKJBsR3NZTlNP8HnOVgqb/8qCUIdMVWddpZTb6erJf+3WPo3z2GCefEUVzhwmLU8+a3++ndOZLLUrrx28FC5rz/K6PP6oLFpGfnkWLW7jzGxWd3pX2EmUE923PnB9vYc7yUd3/4k3KHCwnQ63Q1pgGc+pc4Lj+nKwve28Efh4uYNLAb5Q43/91+lP7do0nuFk2HCDMvbdjDoJ7teWpSfxI6RrDipxx+3J/PjKE9+cvp7dmfV0aU1chXWSdITexEbIQZva72kwR2lxuLseE/dg2JpdPtwWQI3eyRDf17hxItOIKyniKJBsBXh2owwZS35NsZM6CiUDVJVIla2XCzkXCz/DG4a0ySb3tKfAxf3TvCr+2xIhud21l8PUW9TkeU1cT6zFySK2eqOrtbNJ3aWSgsd7LzaBGldjcvrc9m5c9H6NkhnLvHJPH65n2EWwzcPTqRnUeLee/HHABuHd6bLzNzGffiZnp2DGfviTJ6dghn2qvfM+XceD785RAWo54Sm6vG+7AY9SR0jKB3p0jSEjvx0Kd/MCA+hjH9umI26vn9UCEX9OnIiRI7q38/ypxRiaQldkKSJPJKHfyw5yTtw01cmBTO4cIKftp/kp/25xMbYaZHbDhbDhTwze48PrxtKN2qDGGUO1zsOV5Kn86Rvjh6KbE52bw7j4vP7kp+mYMws4Gvsk4wIqmzrzfeGLRSxK4FlPRs8cs+Q0VjL/ssKioCODUtX+5OeOV8OH8WdOoLA68Jpm6DqOGoUo4cP8kXWSeZNCiBdlaTr0bP24N86n+ZWI0G7hydiMPlYfnmfew7UcZfh/QguVs0L365m8+2HSG5WzTrduRy3dDTGdCjPU63B0kCt0ei3OHiz5PlfLf3JDuPFtMvLoq4mDA2ZZ/A7ZHoFxfF9kNyvM7oHMnu46W0DzdhMRo4VmzzuabEx7DjSBFOt0S3mDAKyh1UON1IEnRuZ8HtkRie1Jnv9+Zx1Xk9ePeHHI4V24iPDWP2yDP47LcjlDlcTBrQjY9/PcwvOYVMGtCNdTtzsRj1nCxz0C8uituG9+G3gwV0bx/OiRI75/WKxaDT8cHWgyR2aceuYyWkJXbinO7R7DhSTOd2FmwV5QyMjyImJoZdx4rp1TESs1HP7twS3vsxh0E9YykodzBtUDwmg57s3BK6RFmJDqv75KF3spq8UjslNhcJHSPqbF8XWvlMNsazvtwhkmgAvLP6xMTEyBtKcuG5RNCbIH4wXP958GQbSA1HlaKk58lSO7ER5oB1fh6PxOrfjzI4IZYuUVaOFFbgdHs4vUMEu3NL+PVgIZMHdmfV9iPkFtsoqnAyIL493SMh+3gZH/2ex7AzOjF9cDxhJgNOt4RHkjhcWEF0mInFG/bw0/58wswGfv6zgLH9unDjsF48vz6bb/ec5Jzu0URajfy0P5+eHSI4Jz6GVduOMOyMTnydfYIJ58SRk1/GlgMFhJsNlDvcWIx67C55diXvtvjYMA7m1zyReVbXCNqFWfhxfz7dYsIosTkpc7gJMxkotcs99GFndKR7+3BWbj2IUa8jPjacfnFRdI220s5iZOXWQ/TsGMGZp7VjSEIH5n64nWFndGLdzmNUONzcOrw3MeFmOkSYKXO4WL8zl/bhZg4WlHOk0EZKfAz3jE3igY9+56IzO1Nmd7PrWDEx4WZwO9m0J5+FU1LYlH0Ci8nArWm9CTMb+O1gISU2J4N6xqLTwfFiO/Gx8okdp9vDH4eLiLQYefXrfeh1cF5CB9KSOmF3efx6/zanm8278/glp4BxZ59Gcvdo/vf7UQ6cLOfW4b2xu9y4PRJfZ+eR0DGCHUeKiIsJo0uUlS5RFgx6HRWlJQ3+TLb5JFq1TvSmm25qcBL1Tn8VFVW5WJ3LAY9XXj3UMRFmbQmKb2Oo4ahStODZWEePR+J4iZ2u0Vbf/YMF5fSIDUen0/l6d3Cqp1f1B+B4iY1ws5GD+eX07BDBsWIbOqBTOwtbDuQzpFcHvt93ktwiG5cP7MaxIht7jpwk49djeNAz7IyObMrOIyU+mo6RFiYO6Manvx2mxObi3z/mYHd5GJwQy4AeMew4XMzu4yXszi3F6fFw6TlxlNndfL37hK/n+efJMv465HS6RFl5cf1ujAYdNqcbjwQX9umI2yMRFxNGtxgrGT8fIq/UjtMtpw6zQU9KfAw2l5sDeWV0ijSzN68cg16HyaCjQ4SF1MSOvL/lIN5sYzHqcXkkzu4WzbGiCiocboorh2msJj2x4WaOVsZEQj56aB9uRqeD/Xll5BbbCTfLJy+nD+7BM2uzcHskxvc/jS925BJpNZJf5sCgrzlGH2428NykM7mgV/sG/b1FEm1iEvXOLeh3be1T8WAvBmsM3P9nMHQbRa2OKkQLnm3BsdzhQq/T+crW8sscHCoo56zTonC4Pb4x3aJyJ2FmA7uOFbPjSDFXDvJfxTa32MZ1//yJiQO6Mfkv3THqdXIvtNLR5fbw9d5Cwi1GeneK4PH/ZrJh13Huv7gvA09vz8H8co6X2Nl2sJBfcgqYNLA7kRYDQ3t3xOZ00zHSQs+OEew9UcpHvxyia3QY+06UUljuxKCXx+FnDO0JwKVLvqHE5uKGC3oSE27mPz8f4ryEWCqcbmaN6MOrX++jb9d2XNCnI0UVTk6U2CmscHJ+z2ji24c1KJZtPol6aezhfFmZPKNSRESV8aEX+kNhZfJ86AQYQ3smslZHFaIFT+HYOKr2tKsSyNHmdPvVHHv3IUk0q6TNVjle3dgTdY2JZX25Q5ydD4B3Fm4/wjucSqLleRAV3On46qNWRxWiBU/h2DgCjUkHcqyeQL37aO4l7LXttyEoGUuxZHJjCO9w6nbp8dB5CAQC1SB6ogGodawkPPbU7e0roeMZ4HZCWEyLeVVFzeN3VdGCp3BUBi04grKeIokGQK+vpZMe3gEMFvnyzx+WwN4vofgI3L3r1Br1oXZUIVrwFI7KoAVHUNZTG+84BFRUVPhWI/TRKQm6Jp+6f2KXfLb+z+9bVq6SWh1ViBY8haMyaMERlPVs9Ul0x44d7Nixw29G/Sbzlxlw45dw9hX+2/dtbP6+q/LKhfDHh8ruUyAQBIVWn0Sbil6vD9zln/xPGP+cfNscCQe+Ue6FJQlOZEL+vuY5qggteApHZdCCIyjr2erHRJs6n2i9A8/d/iL/nzQO9m5oilrtOMrA4wJH/RNAt8VB/GAhHJVBC46grKf6fzJChNPprLuWLG4A3L4Fzhgj14y67Mq8sK2oUqD+JFqvo0rQgqdwVAYtOIKynq2+J9pUXK6a063VoFMilObKt0tzIaZH81/YVij/76h/DfoGOaoALXgKR2XQgiMo6yl6os3Fu6RySS3DBV8/C8d31dzudsKRX2vfXyN6ogKBIPSIJBoAi8XSsHGTdpXX0pYc9d9ecAA2PA7/u7fmc9Y9BK8NB3tpzccqCuX/GzgmqoUxKC14Ckdl0IIjKOvZ6g/nm7pkcoPP3FkiwRIFOT/AyT2Qt1s+6fT5PZUNql0c7CiH7DXy7YID0PVs/8e9h/PO+g/ntXAWFLThKRyVQQuOoKxnq0+iTcVbiNugXyuPC35YCtZoeSxz24pTj1UtVfK44ckqazYV7K8liVYezjegJ9ooxxCiBU/hqAxacARlPbXxs9EM+vXrR79+/UhKSqq/cVMxVv4h7twJZ02Ub1/5b7mWtOggvHyunFyrj5vm76+5L+/hvBgTFQg0geiJBqBRC1n935fyMsqWSBjxAHQ/F/qOh4LKafNO7obDv5xKtv/3JfzvPrknWpXS43B4q3y7AWfn2+KiYMFCOCqDFhxBWU+RRAPQqCVVO/T2v93hVvl2+9Nhzu+wbBjs+OjU/KNd+kH7hJo90RVXwuGf5dsN6Im2xeVpg4VwVAYtOIKyni16OO9wOEhNTfWtZ1NUVMQtt9zCiBEjGDt2LF9++WWtz8vMzGTSpEmMHDmSadOmsW9f/ZdENhfFinFjeshJc+s/5bP14R3AFAaxveDkXv+2VecobcCYaFssbA4WwlEZtOAIynq2WBJdvHgxU6dOJTc317dtwYIFJCQksHHjRl555RUeeOAB31KmXjweD3PmzGH27Nls2LCB6dOnM2/evKD7ulwu5Qpy4887dTusvfx/5zOhKAdsxaceM1QubWtuJ/dEJQn+PQ2erzJzVLAcg4gWPIWjMmjBEZT1bLEkmpyczKxZs3z3PR4P69at47rrrgOgV69eJCcns3nzZr/n7dy5E7fbzYgRIwC45JJLyM7O9kvGqmf4PJj5hXy7uLKetIt8TT/HM+Ue6eLB8pn8CS/BZYsBCZwVcjlUUU7DXsdRLlcACASCFqPFxkTT0tL87hcUFGCz2fwWfoqLi+PoUf+i9ZycHOLj4333jUYjXbp0ITc3ly5dujTao6ioiPDwcCwWCy6Xy9elt1gs6PV6X+mD0+nEbDZTWlrq+8WyWuXlcW02GyAPTpvNZpxOZ8A2RqMRk8mEI7ov7QB3VDdcdjue8DisBgsVB7ZiLD+OOU+eqq/U1AEcTiKBipKTeFfbLjt5GGNkR+x2Ox6Pp3J9Gh1ut9u3rnvU2yNxD76J8rOm+xYSs1qtuN1uHA4HII8FGQwGbDYbkiSh1+vrjYW3TdX3Wb1NQ2JRWloqx8LhwO121x2vKm3CwsLweDy+1S5NJhNGo9EXi4a0qS8WNpsNnU4nr1ZZJRZNiVdtsWhKvKrHwmazYbVafb7NjVfVWDQ2XoFi4fF4MBgMFBUVNTteOp2uad+1ap8vSZJ8sfC2cTqdvu9OffHyDj8GImQnlnQ6XY0zZHq9HqPRWKNd9W16vb7Os2sZGRlkZGT4bfN+EEKGTkfJVZ/jieiIFUBvxB3bB8Px3zEe2epr5omKR1cql0Lpqpxc0udlQWTHwPt32dAX7EU68iucNT1Ib0IgEFQnZEm0ffv26PV68vPziY2V1y46duwYw4YN82vXrVs3v96px+MhLy+Pbt26Bdz3lClTmDJlit8277Kn0dHRREVFAbUX2nq3FRYWIkkSkZGRNdp4fwGrP6feNu0u8G909kSMG58AdGAKB2c5Ud3PhONyb8G6bLCvaVjJn2AajqX4gDzDfqWjXq8nJiZGvlIKMBTsJTo6uoZPeHh4nc51xaK5bbyx8MaywfGqRlhYmCJtaouFd+XwQJcDhiJe1dt4jzisVqti8arvc9GQNlXve783MTEx9e63ObFobJvqsTCZTJhMJj/PQPHy9nQDEbJie51Ox9ixY3n//fcByMrKIisri6FDh/q1S05OxmazsXWr3Fv78MMPSUlJqfFH0iQX3gUDr4PLl8FdO2H6B/Ja9qZa1mva+KR8zf2SwbD2wZqPe5dyzsuWT0gFi9yd8skuMfYqEAAhvmLpvvvu45dffmH06NHcfffdLFiwwPdrkJ6ezvbt29HpdCxatIjHH3+cUaNGsWrVKh577LGguxmNxhrDCIpjMMKlL8E5V8pn7ZPS5e3VF727Ya28ougPS+T73y+BggOE/fQSporKsqjCg/L/tiIoOyHfLvgTyk7Kt5VKegc2yye7vEm7AbRILJvC3o3gPDWepkrHKjTI0eM+ddVbCNBCHEFZzxZ/t1XXOoqNjeX111+vtd2aNWt8t/v3788nn3wSbDU/TCZTi76eH1FxcNlS+PQ2+X7cQBhwDXzxdxjxkJxMXx+Npew4BlcZeKbCty/IPVhnGeT+AVI/eaYog0muDvjiYfjbb/7LPjeFwspKgbw9cq1rIMpOQkQHIMSxDERFAfxrIkx8BVKuUqdjNRrk+Nu/YeMT8gq0IUALcQRlPVv9tfNNxeFwhO5klE4HA66Gc2fK941m6D8VuvaH/lMg9V4oO46EDuNPy+DN8fKMUOZw6NQXfn0XPpsNRivojfIlpvYi+P0/cOQ3uRTq2xeh6HDj3bxJdP8mebloSZLLtPL2wE/L5R7xgW/huSQoOgQEKZbHfoftGfW3C0TBAfn/E/KPekj/3g2kQY5Hf5OnZbQV1d0uSGghjqCsp/r73c2kqVPheUskQsq4Z2HUI/Ltdl3hlsoa2vNvh9MvoLSkmPA1czD0OE9OoJ3PBJ0B/jtHbnfNx5C5Sr5aCk7NbRrTQ06Gm56FWT+duhwVYMcn8HuGvBR0z2HyUEPVGfu9SfT7xXB0GyRPgVV3QIc+8lSAXz0FvUeCxykv4HfOlXXH8pd/Qc73MHFp42Lz/RJ5WCF5spzIGzu1mTeJntwDgPGHl/FEdIGhMxu3n+ZiK5KnUtTp6m3aoM9kXrb8f9EheVaxFkYV35sGoKRnq0+imkZvAGtU7Y/FpeAuLKTkug3+J9k8HvnLY42Sk5mjXE6iZ4yVr4LqMUSecT8hFU7uk3upLrs8QXTRQXlav5Kj8gJ8Xy+ETQvgqpVyjzb3D7mn4+XAZji+U759cg9c8Dc5af9e2UP8/T/Q43zQVfsyH9oqJ8D+V8pDFBUFYC+RfzC88xBseV3+QfjLjNoTzKGt8vNO7IJ/joUpb8nvtz72b5aTtr7yo5+3G3J+JOzbBfL9qkn0v3dChzPg/NvksVNHmW+IIiC/vAO5O+DiBfW72EvguTNh8huQdHH97RtCZZUGRYdOXdDRGCoK5SGg6uPyLYWtSP7MRp1Wf1uV0OqTaFNX+6xeNqFGanXU6+HsSafu9x4B/afBmCcgspOcMI9ug7S58hctQ75ijA5nyMmq5CjcuEGeWNpeAiumw7uT/F+j7yVQfFhe4qT8JHRNlg+vz7xULtX66im53Z4v4PWLiIqOl/dVkQ/D7pYrDRylcsJxVsjPz/wMyvPlk23DH4DVd8v7cJTB0Fn+r19RIM+MBfDDK/IXb8cnYLDIPcwBV/u3X3kt9BgKQ26BzQth/9fyjwTIV4ltXniqbXm+PG7sssvji+Ed5OS8fCSYrHBX5qnZuGrjs9ny/0NuhfY95dt5u+X33/ks+PktSLlKjvWx3+Ux7H1f1Z1Eiw7D0iGE/XUVUqe+gdvZik+tsFA5lFIDpw1+fAXOu1V+P9V5b4p85DH5jcCvU5UT2fDL29DzQjBHYj1tUMOeF4hVc+TJem5YBz3Oq7d5U1Hy+93qk2ibxxwBk147dd9ogasre4pxA+G0FLnnet1ncOwPyPpc3g5gaSf38A58A6edI/dEP/grjHxIHjrY9KycAHU6+cTVaSnyOOxXT0Ha/fLzv18CjjI8CSPQu8ph7QPyvgfdCFuWw5Db5P19vRC+WSQ/tu8r+f+zJ8P6R+Re69lXQFmenNAO/ig/brDIX2CQ/89cJSfYDn3gtP7y2PCf38HOT+Wxz7OvkBOo5IFd/5WTXMEB2L0OZ+r9mL5+Wn7/A/4Kh7aAyyb/WCw9DwxmKC+T99V/au2xdlacur32QRh6B2x+TnZwlMhzze78BL5+RnY/9wa5bc4Pp55Xni/H7dBW6D5I/lE5sBnsxej/3Iy7ehL9bjEc+QWueEPulQPoTf5J9MC38qTh/S6XF1Rc/4j83vtdLv9YvDcZUu+TPQ79BMe2ywk50FFQVba8Dj+9Kv84tDsN/m9zg4Ym/HDZIWMGDL9f/oEH+Yft6gy52sBeIlenNBRHOfy+Ui4fPPILRHSGmPj6n9dERBINgLfAVs090mY76vUwYzXoKscTu55dc6b9iI7Qb6J8OzYB5h2Sv+QAaVXWj0oce2ofN26UE6peDylXUVzuAKNVHnaIS5FnseozSk5gF/xNTvQpV8lJdNQj8kkvc6RcPxuXAjs+lg+twzvAxsfl1xn0f/L43/6v4fQL4c9v5P10PEP+QnYbCNlr5bHZsPZygvnX5XJPufNZcrI49wY5cWR9Tlm/q4k49AumT2+XE+53L0FYLCQMkxPnpYvl//93nzwcEttL/vKfeQmcM11OOt++KLuN+of8o5D1P5CqjL3t/AQiu8qJzGg99V6O/gZb3pC93r1C7gkXHZSX5R63UB4WAaSDP2E7+xqsFccgvKM89+z6h+UhmD8+lHuQUd3lv9OJXXJCXvsAbF8pexzaIj8P4OBPchI9tEWO4Z/fy7ECeSHFzFXyD2XcADlG7br4V2NIktxj/OlV+b6jVD46+OpJbANvwnp4t3wUceYE/8/TL+/I77P7ufJ9ewnk/Cj/eBXmVC6Z01/+4SnPh/evkmuTJzwvj9FHdpZ9dnwsV6xU/byW58vDNDs/hVV/k5Pn+9Pl/XnPJ1Si5PdbJ0nBrMxWD94rljZt2uR3vX4gvFeHqLmoXwuO0AjPw7/IPV6PG1wVp06MeDzyYa8pHHatlnsVcQPkioBj26D3RYAElmh5eOGtcXKCvehhOO9mOdktPV9+bNq7ED9YrqXtmCjv31FKYYUbJA8xn14r9/w695OfO/BauQfe5Wx52GDjE/L+Cg7ISTvrczkhuiqvaunaH27+Wu4tr7xOPml3PBPuyYK9G6DXcHn8ed9X8NkseWzaXionUskDnc6EE5nyWPBv/wa3/xnksvFLidjwoJzg8/fKP0Z71p9qMHyeXDXxy9vyCSt75XXfFz8j/wCA/N50OrjlG3nM2zv8YjDLwz7Za+RE5SyTjwb2fCG/ryvfk5N1pzOh7Lg8TAJy4naUQI+hSPs24onogqGoso741u/lsf2yE/LznkuSE91Zl8qzm62+S25njpR78pJb7lV/OBPOugyy18nJ+/gOaBcnjzWvf0R+71HdYNZW+aSqxwOvDJXbmqyypxeDGR44KvfqCw7Al49ResZluE5PbdB3p77cIZJoAEpL5ZU4a7vsUy1owRFC4OlyyIkwIU3+4oDcu0J36n4gx4JM+PR2uOoDuUdXH9++JCequIHySbuqdbiSJC88mL9PHmOuTvY6uScVFSf3BE/ukXu1RQflw+2flssJteSYnHDWPyw/T2+Ue5/JU2DScvnCB50edq+Tt7nscu/yh6VybzvmdHnM8uke8o/P+bfDv6fKPwzlJ+VhEo8TJi6DlOmw7X34+GZ5SkZHySnfrv0ra5Al+Qetz0Uw+Ca5p1+YA33HUX7gZ0w/voypXSf46TX/99vjfLnn66kyBZ3BAm67PASTdLFcrXHvXngpRY7D2VfA5a/KM5397z65tE5vlA/1V1wFnfvKZXXlAapv+l8J29+HK1dAyRF5PN4aQ9nEfyLF9mnQZ1Ik0Uoam0S9M7qoecEtLTiCNjw14VhRjuHrpzFaI+GM0XLPrraTQwF3UCL3mg0muSe84xO5Z3bmJfDjq/LQTmRnuWf80U0w5jH50NheLCfy3evkMfLDP8vDF6n31DiL7xfHr5+VE173QXJS/3EZDL4ZTj9fvhhj6z9h+r/loZrkqfKQUNkJuZxv9xfyWO1VGZA45tQLFB+Ve/2xCfI+Nz8n/4DZiuQfkszP5PcZHQ95WXDPbnk/R7fJCTtxDFy2BLvOesqzHtp8Eq1aJ3rTTTc1OImWlMi/wO3atQuqX3PQgiNow1M4KoOijt5Kicbg/aGwFcm9Zu/wybYVcvVIuy6N9qwviYoTSwHQQtGwFhxBG57CURkUdWzKJcrek54RHeUECvICkoNv9Gsmiu0bQVPrRAUCgaAhtPok2lSqzy2oRrTgCNrwFI7KoAVHUNZTJNEAeJdRUDNacARteApHZdCCIyjrKWZxCoDdbvedaVQrWnAEbXgKR2XQgiMo6ymSqEAgEDQDcTgfAC1MLqsFR9CGp3BUBi04grKerT6JNnU+Ua0scaAFtOApHJVBC46grKc4nA+AFsZ2tOAI2vAUjsqgBUdQ1lMbPxvNoKl1olo4y6gFR9CGp3BUBi04grKerT6JenG55EkPjh8/3qD2RUXyGjX1rTkdSrTgCNrwFI7KoAVHaJynN2d4c0h12kwSzc/PB2DKlCkhNhEIBFokPz+f7t2719je6icg8WKz2cjOziY2Nhaj0ehbujkpKSngc2655RaWLVtW62MNeX59bZr7eH2OLeHQEMf6PFvCIdh/75b4PDTXUQmHtviZdLlc5Ofnk5iYWOskzm2mJ2q1Wunfv7/v/smTJwHqnNHJbDYHfLwhz6+vTXMfr8+xJRwa4lifZ0s4BPvv3RKfh+Y6KuHQVj+TtfVAfUiCgEyZMiXUCvWiBUdJ0oancFQGLThKknKeosRJIBAImoFIogKBQNAMRBKtAy2cydeCI2jDUzgqgxYcQTnPNnN2XiAQCIKB6IkKBAJBMxBJVCAQCJqBSKICgUDQDEQSrYVVq1YxatQohg8fzl133UVZWVmolQCYOXMm6enpvn/vvvsuRUVF3HLLLYwYMYKxY8fy5ZdfhsTN4XCQmppKcXExQJ1eoYpvdcfvv/+e4cOH+8U01I5vvfWWL2bXX389hw8fVl0sa3NUUywlSeL5558nPT2dsWPHMnPmTHJzc4MXR0WqTVsROTk50tChQ6WDBw9KHo9HmjdvnrRw4cJQa0mSJElpaWmSx+Px2zZv3jzp6aefliRJkvbu3SsNHjxYKiwsbFGvl19+WbrsssukxMREqaioqE6vUMW3Nsd3331XevXVV2u0DZXj9u3bpaFDh0pHjx6VJEmS3njjDemmm25SVSwDOaoplmvXrpUmT54slZeXS5IkSY8//rj097//PWhxFD3RanzxxRcMHz6c7t27o9PpmDZtGuvWrQu1FqWlpYSHh6PT6XzbPB4P69at47rrrgOgV69eJCcns3nz5hZ1S05OZtasWQ3yClV8qzsC5OTk1Ho5X6gcDx48yOWXX+671PCCCy5g165dqoplIEc1xbJXr148/PDDhIWF4XQ6KSkpITIyMmhxbDPXzjeU6h+GuLg4VaxZn5OTQ0VFBVdeeSWFhYWkpKRwzz33YLPZ/K7/jYuL4+jRoy3qlpaW5ne/oKAgoNfhw4dDEt/qjiDHdOfOnSxZsgSLxcLNN9/M2LFjQ/YZGDduHOPGjQOgvLycF198kaFDh7Jq1SrVxDKQo5pi2adPHwBWrlzJU089hdlsZvHixbzzzjtBiaNIotXQ6XR+Swfo9XoMBkMIjU55XHrppcycOZOwsDCefvpp7rzzzhpuer0+5Es06HS6gF5qiu8555xDv379uPDCC8nOzub666/39UZC6bhq1Sqee+45Bg0axB133MHnn3/u97gaYlnV8aGHHuJf//qX6mI5depUrrjiCpYvX84jjzwStM+kSKLVqN6TO3r0KKeffnoIjWROP/10br/9dsxmMwDTpk3jxhtvRK/Xk5+fT2xsLCDP4D9s2LBQqtK+ffuAXi6XSxXx9Xg8XH311bRr1w6Qpzw799xzyczMDNlnQJIk7r33Xnbs2MGzzz7LoEGDkCRJVbGszVFtsVy+fDk9e/Zk9OjRGAwGJk6cyKuvvhq0OIox0Wp4z9qdPHkSSZL417/+xfjx40OtxTvvvMPtt9/um1171apVDB48mLFjx/L+++8D8hyIWVlZDB06NJSq6HS6gF5qia/L5SItLY2ff/4ZgCNHjvDrr78yYMCAkDn+73//Iysriw8//JBBgwYB6otlbY5qi2VkZCRvv/02DocDgM8++6zO70pzHUVPtBo9evTgjjvu4KqrrsLpdHLBBRdw7bXXhlqLGTNmcODAAcaMGYPVaiUpKYm///3vANx3332MHj0ai8XCggULCAsLC7Gt7FSbl1riazabeeGFF/jHP/6B3W7HYrEwb948evfuDRASx59//pnjx48zadIk37ZOnTrx4osvqiaWgRzVFMvJkyezZ88exo0bh8lkonfv3jz66KOYTKagxFFcOy8QCATNQBzOCwQCQTMQSVQgEAiagUiiAoFA0AxEEhUIBIJmIJKoQCAQNAORRAWCJpCUlMR3330Xag2BChBJVCAQCJqBSKICgUDQDEQSFWie/Px85syZw4ABAxgyZAhPPPEENpuNl19+mRkzZrBkyRKGDBnCsGHDWLJkCR6Px/fc1157jbS0NAYOHMgNN9zAnj17fI+VlpYyd+5c337nz5/vu5QQYM+ePUyePJnk5GQmTJjAjh07WvR9C9SBSKICzfO3v/0Ns9nMihUrWLRoEd988w0LFiwAYOvWrezbt49//etfzJ8/nzfffJMPPvgAkKdKW758OXPnzmXlypUkJiZyzTXXUFRUBMD8+fP5888/eeedd3jxxRf55ptvWL58ue91ly5dyowZM1i5ciWdOnXi0Ucfbfk3Lwg9ys0nLRC0PFu2bJGGDBkiuVwu37Zvv/1WGjhwoLRo0SIpJSXFN8O5JEnSokWLpAkTJkiSJEkXXXSR9Oabb/rtLz09XXrzzTelQ4cOSUlJSdKuXbt8j61evVp6/PHHJUmSpMTERGnp0qW+x7744gupf//+wXiLApUjJiARaJqsrCwKCgpISUnxbZMkCafTSX5+PklJSX4TsvTt25d33nmHiooKDh48yMCBA/3217dvXw4cOMDevXsJDw8nKSnJ91jVCYkBEhMTfbfDw8Ox2WxBeIcCtSOSqEDTOJ1OEhISWLx4cY3HPvvsM9/8q9Wx2+0ANR73zshfXl6OyWSq87XVMFuWIPSIMVGBpklISODYsWN07dqV3r1707t3bw4dOsTzzz+P0Whk9+7dOJ1OX/vffvuNPn36EBMTQ0xMDNu3b/c95na72bZtG0lJSSQkJFBYWMjhw4d9jy9fvpw5c+a05NsTaACRRAWa5sILL6R79+48+OCD7Ny5k40bNzJ//nz69u0LyOs9Pfroo+zevZuPPvqI999/n2nTpgFwzTXX8OKLL7J+/Xp27tzJQw89hNFoZNy4cSQlJTF48GAeeugh/vjjD7788kvfmXyBoCricF6gaQwGA0uXLuWRRx5h2rRpxMTEMGnSJG699VaWLl1KSkoKYWFhTJs2jYiICGbPns3kyZMBuPnmmyktLeXBBx+koqKCc845h+XLl/sO8RcuXMjf//53rrrqKmJjY7nxxhu5/PLLQ/l2BSpETMosaLW8/PLLfPfdd6xYsSLUKoJWjDicFwgEgmYgkqhAIBA0A3E4LxAIBM1A9EQFAoGgGYgkKhAIBM1AJFGBQCBoBiKJCgQCQTMQSVQgEAiagUiiAoFA0Az+H+7NOvOhYi2ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 350x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5),dpi = 70)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e004c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for 50-fold cv for y: 79.06990215617502\n",
      "Average RMSE for 50-fold cv for dxy: 16.27243344908982\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate((x_train_displ, x_test_displ),axis = 0)\n",
    "y = np.concatenate((y_train_displ, y_test_displ),axis = 0)\n",
    "dxy = np.concatenate((dxy_train_displ, dxy_test_displ),axis = 0)\n",
    "k_fold_validation(model = model, \n",
    "                  x = x, \n",
    "                  y = y, \n",
    "                  dxy = dxy, \n",
    "                  folds =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cda3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] # of entries: 1034656, mean: -0.03382112115543751, std: 16.23794399552425\n",
      "[INFO    ] gaus fit (a, mu, sig): [6.83766317e+04 1.39188345e-02 1.10113168e+01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 375x375 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] # of entries: 1034656, mean: 0.045670753392419994, std: 0.18357138526457467\n",
      "[INFO    ] gaus fit (a, mu, sig): [3.49308319e+04 4.99603957e-03 2.43344844e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 375x375 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def __generate_delta_plots__(y,y_predicted,dxy,dxy_predicted):\n",
    "    y_test_true = y.copy()\n",
    "    y_test_true /= reg_pt_scale\n",
    "    y_test_sel = (np.abs(1.0/y_test_true) >= 20./reg_pt_scale)\n",
    "    dxy_test_true = dxy.copy()\n",
    "    dxy_test_true /= reg_dxy_scale\n",
    "    dxy_test_sel = (np.abs(dxy_test_true) >= 25)\n",
    "    \n",
    "    # Plot Delta(dxy) - (b)\n",
    "    plt.figure(figsize=(5,5),dpi = 75)\n",
    "    yy = (dxy_predicted - dxy_test_true)[y_test_sel]\n",
    "    hist, edges, _ = plt.hist(yy, bins=120, range=(-120,120), histtype='stepfilled', facecolor='c', alpha=0.6)\n",
    "    plt.xlabel(r'$\\Delta(d_{0})_{\\mathrm{meas-true}}$ [cm]')\n",
    "    plt.ylabel(r'entries')\n",
    "    logger.info('# of entries: {0}, mean: {1}, std: {2}'.format(len(yy), np.mean(yy), np.std(yy)))\n",
    "\n",
    "    popt = fit_gaus(hist, edges, mu=np.mean(yy), sig=np.std(yy))\n",
    "    logger.info('gaus fit (a, mu, sig): {0}'.format(popt))\n",
    "    xdata = (edges[1:] + edges[:-1])/2\n",
    "    plt.plot(xdata, gaus(xdata, popt[0], popt[1], popt[2]), color='c')\n",
    "    plt.show()   \n",
    "    \n",
    "    # Plot Delta(q/pT)/pT\n",
    "    plt.figure(figsize=(5,5),dpi = 75)\n",
    "    yy = ((np.abs(1.0/y_predicted) - np.abs(1.0/y_test_true))/np.abs(1.0/y_test_true))\n",
    "    hist, edges, _ = plt.hist(yy, bins=160, range=(-2.0,2.0-eps), histtype='stepfilled', facecolor='c', alpha=0.6)\n",
    "    plt.xlabel(r'$\\Delta(p_{T})_{\\mathrm{meas-true}}/p_{T}$ [1/GeV]')\n",
    "    plt.ylabel(r'entries')\n",
    "    logger.info('# of entries: {0}, mean: {1}, std: {2}'.format(len(yy), np.mean(yy), np.std(yy[np.abs(yy)<0.4])))\n",
    "\n",
    "    popt = fit_gaus(hist, edges, mu=np.mean(yy), sig=np.std(yy[np.abs(yy)<2.0]))\n",
    "    logger.info('gaus fit (a, mu, sig): {0}'.format(popt))\n",
    "    xdata = (edges[1:] + edges[:-1])/2\n",
    "    plt.plot(xdata, gaus(xdata, popt[0], popt[1], popt[2]), color='c')\n",
    "    plt.show()\n",
    "\n",
    "evaluate_obj = evaluate(x_test_displ, tuple([y_test_displ, dxy_test_displ]))\n",
    "y_predicted , dxy_predicted = evaluate_obj.predict(model = model)\n",
    "__generate_delta_plots__(y_test_displ, y_predicted, dxy_test_displ, dxy_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73232b7f",
   "metadata": {},
   "source": [
    "### Training Algorithm for Pruned Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e70069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2eb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "def loading_trained_model(filepath='', model_filename='model'):\n",
    "    try:\n",
    "        model_path = filepath + model_filename + \".json\"\n",
    "        model_weights_path = filepath + model_filename + \"_weights.h5\"\n",
    "        json_file = open(model_path, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(model_weights_path)\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "        return loaded_model\n",
    "        \n",
    "    except:\n",
    "        print(\"ERROR: The model doesn't exist at the address provided.\")\n",
    "        \n",
    "loaded_model = loading_trained_model(filepath = os.getcwd()+\"/\",model_filename = 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aac1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gpradhan/opt/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_batch_no (None, 23)                93        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 20)                922       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 20)                81        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 20)                1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_1  (None, 15)                602       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 15)                61        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 15)                1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_2  (None, 10)                302       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 10)                41        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 10)                1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_3  (None, 2)                 44        \n",
      "=================================================================\n",
      "Total params: 2,149\n",
      "Trainable params: 1,068\n",
      "Non-trainable params: 1,081\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 1000\n",
    "epochs = 10\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "lr = 1e-4\n",
    "clipnorm = 10.\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(\n",
    "                                                        target_sparsity = 0.50, \n",
    "                                                        begin_step = 0, \n",
    "                                                        end_step = -1, \n",
    "                                                        frequency = 1)\n",
    "        }\n",
    "\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(loaded_model, **pruning_params)\n",
    "\n",
    "# Set loss and optimizers\n",
    "adam = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
    "model_for_pruning.compile(optimizer=adam,\n",
    "              loss=huber_loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe23bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Training model with l1_reg: 0.0 l2_reg: 0.0\n",
      "[INFO    ] Begin training ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 9s - loss: 17.3813 - accuracy: 0.9042 - val_loss: 14.5742 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 9.73163\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 13.8538 - accuracy: 0.9226 - val_loss: 13.0167 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 9.73163\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 12.8407 - accuracy: 0.9263 - val_loss: 12.3552 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.73163\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 12.3379 - accuracy: 0.9276 - val_loss: 11.9869 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.73163\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 12.0553 - accuracy: 0.9286 - val_loss: 11.7497 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9.73163\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 5s - loss: 11.8628 - accuracy: 0.9294 - val_loss: 11.6158 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.73163\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 11.7261 - accuracy: 0.9303 - val_loss: 11.4697 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.73163\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 11.6168 - accuracy: 0.9309 - val_loss: 11.3683 - val_accuracy: 0.9325\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.73163\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 11.5327 - accuracy: 0.9314 - val_loss: 11.2851 - val_accuracy: 0.9325\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.73163\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "2025/2025 - 6s - loss: 11.4570 - accuracy: 0.9320 - val_loss: 11.2134 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Done training. Time elapsed: 0:00:59.428356 sec\n",
      "[INFO    ] Epoch 10/10 - loss: 11.457005500793457 - val_loss: 11.213362693786621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 9.73163\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 9.73163\n"
     ]
    }
   ],
   "source": [
    "logger.info('Training model with l1_reg: {0} l2_reg: {0}'.format(l1_reg, l2_reg))\n",
    "logdir = tempfile.mkdtemp()\n",
    "history = train_model(model_for_pruning, \n",
    "                      x_train_displ, \n",
    "                      np.column_stack((y_train_displ, dxy_train_displ)),\n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size,\n",
    "                      callbacks=[lr_decay,modelbestcheck,modelbestcheck_weights, \n",
    "                                 tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "                                 tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)], \n",
    "                      validation_split=validation_split, \n",
    "                      verbose=2)\n",
    "\n",
    "metrics = [len(history.history['loss']), history.history['loss'][-1], history.history['val_loss'][-1]]\n",
    "logger.info('Epoch {0}/{0} - loss: {1} - val_loss: {2}'.format(*metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8cfa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(weights):\n",
    "    \"\"\"\n",
    "    Code borrowed from https://github.com/google/qkeras/blob/master/qkeras/utils.py#L937\n",
    "    Returns the sparsity as the ratio of non-zero weights to the total weights within the weights matrix.\n",
    "    \"\"\"\n",
    "    return 1.0 - np.count_nonzero(weights) / float(weights.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2df2565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_low_magnitude_dense : 0.5\n",
      "prune_low_magnitude_dense_1 : 0.5\n",
      "prune_low_magnitude_dense_2 : 0.5\n",
      "prune_low_magnitude_dense_3 : 0.5\n"
     ]
    }
   ],
   "source": [
    "for layer in model_for_pruning.layers:\n",
    "    w = layer.get_weights()\n",
    "    if len(w) == 1 or len(w) == 2:\n",
    "        print(layer.name,\":\",get_sparsity(w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d676fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### END ############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
