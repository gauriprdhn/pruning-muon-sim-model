{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfeb29da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Using numpy 1.21.0\n",
      "[INFO    ] Using tensorflow 2.4.1\n",
      "[INFO    ] Using keras 2.4.3\n",
      "[INFO    ] Using scipy 1.7.0\n",
      "[INFO    ] Using sklearn 0.24.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0b946e0a86eb>:31: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. In future versions, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = copy.copy(mpl.cm.get_cmap(\"viridis\"))\n",
      "  my_cmap.set_under('w',1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential, Model,model_from_json\n",
    "from keras.layers import (InputLayer,Dense, Activation, BatchNormalization, \n",
    "                          Conv1D, Flatten, MaxPooling1D,AveragePooling1D)\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "from nn_globals import *\n",
    "from nn_encode_displ import nlayers, nvariables\n",
    "from nn_models import (lr_decay, modelbestcheck, modelbestcheck_weights)\n",
    "from nn_training import train_model\n",
    "from nn_models import load_my_model, update_keras_custom_objects\n",
    "from keras.models import Model\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Setup matplotlib\n",
    "plt.style.use('tdrstyle.mplstyle')\n",
    "\n",
    "from nn_plotting import (gaus, fit_gaus, np_printoptions, \\\n",
    "                         find_efficiency_errors)\n",
    "\n",
    "eps = 1e-7\n",
    "my_cmap = plt.cm.viridis\n",
    "my_cmap.set_under('w',1)\n",
    "my_palette = (\"#377eb8\", \"#e41a1c\", \"#984ea3\", \"#ff7f00\", \"#4daf4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0806372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluate:\n",
    "    def __init__(self,X_test,y_test):\n",
    "        self.X = X_test\n",
    "        self.y = y_test[0]\n",
    "        self.dxy = y_test[1]\n",
    "    \n",
    "    def compute_data_statistics(self,ctype = \"y\",label=\"data\"):\n",
    "        if ctype == \"y\":\n",
    "            x = self.recalibrate(self.y,reg_pt_scale)\n",
    "            x = x**(-1)\n",
    "        else:\n",
    "            x = self.recalibrate(self.dxy,reg_dxy_scale)\n",
    "        df_describe = pd.DataFrame(x, columns = [label])\n",
    "        print(df_describe.describe())\n",
    "    \n",
    "    def rmse(self,y_true, y_predicted):\n",
    "        assert(y_true.shape[0] == y_predicted.shape[0])\n",
    "        n = y_true.shape[0]\n",
    "        sum_square = np.sum((y_true - y_predicted)**2)\n",
    "        return math.sqrt(sum_square/n)\n",
    "    \n",
    "    def adjusted_r_2(self,y_true, y_predicted):\n",
    "        y_addC = sm.add_constant(y_true)\n",
    "        result = sm.OLS(y_predicted, y_addC).fit()\n",
    "        print(result.rsquared, result.rsquared_adj)\n",
    "\n",
    "    def recalibrate(self,x,scale):\n",
    "        return x/scale\n",
    "    \n",
    "    def inverse(self,arr):\n",
    "        arr_inv = 1./arr\n",
    "        arr_inv[arr_inv == np.inf] = 0.\n",
    "        return arr_inv\n",
    "    \n",
    "    def predict(self,model,batch_size = 256):\n",
    "        y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "        dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "        \n",
    "        y_test = model.predict(self.X,batch_size = 2000)\n",
    "        y_test_meas = y_test[:,0]\n",
    "        dxy_test_meas = y_test[:,1]\n",
    "        y_test_meas = self.recalibrate(y_test_meas,reg_pt_scale)\n",
    "        dxy_test_meas = self.recalibrate(dxy_test_meas,reg_dxy_scale)   \n",
    "    \n",
    "        y_test_meas = y_test_meas.reshape(-1)\n",
    "        dxy_test_meas = dxy_test_meas.reshape(-1)\n",
    "\n",
    "        return y_test_meas, dxy_test_meas\n",
    "    \n",
    "    def compute_error(self,y_predicted,ctype = \"y\"):\n",
    "        if ctype == \"y\":\n",
    "            y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "            print(\"RMSE Error for momentum:\",self.rmse(self.inverse(y_test_true),\\\n",
    "                                                                              self.inverse(y_predicted)))\n",
    "        else:\n",
    "            dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "            print(\"RMSE Error for dxy:\",self.rmse(dxy_test_true,y_predicted))\n",
    "\n",
    "    def get_error(self,y_predicted,ctype = \"y\"):\n",
    "        if ctype == \"y\":\n",
    "            y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "            return self.rmse(self.inverse(y_test_true),self.inverse(y_predicted))\n",
    "        else:\n",
    "            dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "            return self.rmse(dxy_test_true,y_predicted)\n",
    "\n",
    "def k_fold_validation(model, x, y, dxy, folds =10):\n",
    "    x_copy = np.copy(x)\n",
    "    y_copy = np.copy(y)\n",
    "    dxy_copy = np.copy(dxy)\n",
    "    assert x_copy.shape[0] == y_copy.shape[0] == dxy_copy.shape[0]\n",
    "    fold_size = int(x_copy.shape[0] / folds)\n",
    "    x_splits, y_splits, dxy_splits = [], [], []\n",
    "    for i in range(folds):\n",
    "        indices = np.random.choice(x_copy.shape[0],fold_size, replace=False)  \n",
    "        x_splits.append(x_copy[indices])\n",
    "        y_splits.append(y_copy[indices])\n",
    "        dxy_splits.append(dxy_copy[indices])\n",
    "        x_copy = np.delete(x_copy,indices,axis = 0)\n",
    "        y_copy = np.delete(y_copy,indices,axis = 0)\n",
    "        dxy_copy = np.delete(dxy_copy,indices,axis = 0)\n",
    "    rmse_y, rmse_dxy = [],[]\n",
    "    for i in range(folds):\n",
    "        evaluate_obj = evaluate(x_splits[i], tuple([y_splits[i],dxy_splits[i]]))\n",
    "        y_predicted , dxy_predicted = evaluate_obj.predict(model = model)\n",
    "        rmse_y.append(evaluate_obj.get_error(y_predicted,ctype=\"y\"))\n",
    "        rmse_dxy.append(evaluate_obj.get_error(dxy_predicted,ctype=\"dxy\"))\n",
    "    print('Average RMSE for '+ str(folds) + '-fold cv for y:', np.mean(rmse_y))\n",
    "    print('Average RMSE for '+ str(folds) + '-fold cv for dxy:', np.mean(rmse_dxy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ebd28",
   "metadata": {},
   "source": [
    "### Preprocessing the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e3ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Loading muon data from NN_input_params_FlatXYZ.npz ...\n",
      "[INFO    ] Loaded the variables with shape (19300000, 25)\n",
      "[INFO    ] Loaded the parameters with shape (19300000, 6)\n",
      "[INFO    ] Loaded the encoded variables with shape (3284620, 23)\n",
      "[INFO    ] Loaded the encoded parameters with shape (3284620,)\n",
      "[INFO    ] Loaded # of training and testing events: (2249964, 1034656)\n",
      "[WARNING ] The last batch for training could be too few! (2024967%128)=7. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*0.9) % 128\n",
      "[WARNING ] The last batch for training after mixing could be too few! (4049935%128)=15. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*2*0.9) % 128\n"
     ]
    }
   ],
   "source": [
    "infile_muon_displ = \"NN_input_params_FlatXYZ.npz\"\n",
    "\n",
    "nentries = 100000000\n",
    "\n",
    "def _handle_nan_in_x(x):\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    x[x==-999.0] = 0.0\n",
    "    return x\n",
    "\n",
    "def _zero_out_x(x):\n",
    "    x = 0.0\n",
    "    return x\n",
    "    \n",
    "def _fixME1Ring(x):\n",
    "    for i in range(len(x)):\n",
    "        if (x[i,0] != 0.0): x[i,18] = x[i,18] + 1\n",
    "    return x   \n",
    "\n",
    "def muon_data(filename, reg_pt_scale=1.0, reg_dxy_scale=1.0, correct_for_eta=False):\n",
    "    try:\n",
    "        logger.info('Loading muon data from {0} ...'.format(filename))\n",
    "        loaded = np.load(filename)\n",
    "        the_variables = loaded['variables']\n",
    "        the_parameters = loaded['parameters']\n",
    "        # print(the_variables.shape)\n",
    "        the_variables = the_variables[:nentries]\n",
    "        the_parameters = the_parameters[:nentries]\n",
    "        logger.info('Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "        logger.info('Loaded the parameters with shape {0}'.format(the_parameters.shape))\n",
    "    except:\n",
    "        logger.error('Failed to load data from file: {0}'.format(filename))\n",
    "\n",
    "    assert(the_variables.shape[0] == the_parameters.shape[0])\n",
    "    _handle_nan_in_x(the_variables)\n",
    "      #_fixME1Ring(the_variables)\n",
    "    _handle_nan_in_x(the_parameters)\n",
    "    mask = np.logical_or(np.logical_or( np.logical_or((the_variables[:,23] == 11), (the_variables[:,23] == 13)), (the_variables[:,23] == 14)),(the_variables[:,23] == 15)) \n",
    "\n",
    "    the_variables = the_variables[mask]  \n",
    "    the_parameters = the_parameters[mask]  \n",
    "    assert(the_variables.shape[0] == the_parameters.shape[0])\n",
    "\n",
    "    x = the_variables[:,0:23]\n",
    "    y = reg_pt_scale*the_parameters[:,0]\n",
    "    phi = the_parameters[:,1] \n",
    "    eta = the_parameters[:,2] \n",
    "    vx = the_parameters[:,3] \n",
    "    vy = the_parameters[:,4] \n",
    "    vz = the_parameters[:,5]      \n",
    "    dxy = vy * np.cos(phi) - vx * np.sin(phi) \n",
    "    dz = vz\n",
    "    w = np.abs(y)/0.2 + 1.0\n",
    "    x_mask = the_parameters[:,5]\n",
    "    x_road = the_parameters[:,5] \n",
    "    _zero_out_x(x_mask)\n",
    "    _zero_out_x(x_road)  \n",
    "    logger.info('Loaded the encoded variables with shape {0}'.format(x.shape))\n",
    "    logger.info('Loaded the encoded parameters with shape {0}'.format(y.shape))\n",
    "    #assert(np.isfinite(x).all())\n",
    "    return x, y, dxy, dz, w, x_mask, x_road\n",
    "\n",
    "def muon_data_split(filename, reg_pt_scale=1.0, reg_dxy_scale=1.0, test_size=0.5, correct_for_eta=False):\n",
    "    x, y, dxy, dz, w, x_mask, x_road = muon_data(filename, reg_pt_scale=reg_pt_scale, reg_dxy_scale=reg_dxy_scale, correct_for_eta=correct_for_eta)\n",
    "\n",
    "    # Split dataset in training and testing\n",
    "    x_train, x_test, y_train, y_test, dxy_train, dxy_test, dz_train, dz_test, w_train, w_test, x_mask_train, x_mask_test, x_road_train, x_road_test = train_test_split(x, y, dxy, dz, w, x_mask, x_road, test_size=test_size)\n",
    "    logger.info('Loaded # of training and testing events: {0}'.format((x_train.shape[0], x_test.shape[0])))\n",
    "\n",
    "    # Check for cases where the number of events in the last batch could be too few\n",
    "    validation_split = 0.1\n",
    "    train_num_samples = int(x_train.shape[0] * (1.0-validation_split))\n",
    "    val_num_samples = x_train.shape[0] - train_num_samples\n",
    "    batch_size = 128\n",
    "    if (train_num_samples%batch_size) < 100:\n",
    "        logger.warning('The last batch for training could be too few! ({0}%{1})={2}. Please change test_size.'.format(train_num_samples, batch_size, train_num_samples%batch_size))\n",
    "        logger.warning('Try this formula: int(int({0}*{1})*{2}) % 128'.format(x.shape[0], 1.0-test_size, 1.0-validation_split))\n",
    "    train_num_samples = int(x_train.shape[0] * 2 * (1.0-validation_split))\n",
    "    val_num_samples = x_train.shape[0] - train_num_samples\n",
    "    batch_size = 128\n",
    "    if (train_num_samples%batch_size) < 100:\n",
    "        logger.warning('The last batch for training after mixing could be too few! ({0}%{1})={2}. Please change test_size.'.format(train_num_samples, batch_size, train_num_samples%batch_size))\n",
    "        logger.warning('Try this formula: int(int({0}*{1})*2*{2}) % 128'.format(x.shape[0], 1.0-test_size, 1.0-validation_split))\n",
    "    return x_train, x_test, y_train, y_test, dxy_train, dxy_test, dz_train, dz_test, w_train, w_test, x_mask_train, x_mask_test, x_road_train, x_road_test\n",
    "\n",
    "# Import muon data\n",
    "# 'x' is the array of input variables, 'y' is the q/pT\n",
    "x_train_displ, x_test_displ, y_train_displ, y_test_displ, dxy_train_displ, dxy_test_displ, dz_train_displ, dz_test_displ, \\\n",
    "w_train_displ, w_test_displ, x_mask_train_displ, x_mask_test_displ, x_road_train_displ, x_road_test_displ = \\\n",
    "      muon_data_split(infile_muon_displ, reg_pt_scale=reg_pt_scale, reg_dxy_scale=reg_dxy_scale, test_size=0.315)\n",
    "\n",
    "y_train_displ = np.abs(y_train_displ)\n",
    "y_test_displ = np.abs(y_test_displ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896cc830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2249964, 23, 1) (1034656, 23, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape to add a channel for convolution\n",
    "x_train_displ = x_train_displ.reshape((x_train_displ.shape[0],23,1))\n",
    "x_test_displ = x_test_displ.reshape((x_test_displ.shape[0],23,1))\n",
    "print(x_train_displ.shape,x_test_displ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb621b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_train_displ, x_test_displ),axis = 0)\n",
    "y = np.concatenate((y_train_displ, y_test_displ),axis = 0)\n",
    "dxy = np.concatenate((dxy_train_displ, dxy_test_displ),axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b6df0",
   "metadata": {},
   "source": [
    "### Pre- quantization model analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2519ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    squared_loss = 0.5*K.square(x)\n",
    "    absolute_loss = delta * (x - 0.5*delta)\n",
    "    #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "    xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "    return K.mean(xx, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8269f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_module(model, \n",
    "                    filters, \n",
    "                    kernel_size = 3, \n",
    "                    pool_size = 3, \n",
    "                    stride = 1,\n",
    "                    padding = \"valid\", \n",
    "                    regularizer = None,\n",
    "                    use_pooling = True, \n",
    "                    use_batchnorm = True, \n",
    "                    kernel_initializer = None):\n",
    "    \n",
    "    model.add(Conv1D(filters=filters, \n",
    "                     kernel_size=kernel_size, \n",
    "                     strides = stride,\n",
    "                     padding = padding, \n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     kernel_regularizer = regularizer))\n",
    "    if use_batchnorm:\n",
    "        model.add(BatchNormalization(epsilon=1e-4, \n",
    "                                     momentum=0.9))    \n",
    "    if use_pooling:\n",
    "        model.add(MaxPooling1D(pool_size = pool_size, \n",
    "                               padding = padding))  \n",
    "    model.add(Activation(\"relu\"))\n",
    "    return model \n",
    "\n",
    "def create_conv_model(nvariables, \n",
    "                      lr=0.001, \n",
    "                      clipnorm=10., \n",
    "                      outnodes=2,\n",
    "                      l1_reg=0.0, \n",
    "                      l2_reg=0.0, \n",
    "                      kernel_initializer = None):\n",
    "    \n",
    "    regularizer = regularizers.L1L2(l1=l1_reg, l2=l2_reg)\n",
    "    \n",
    "    model = Sequential()  \n",
    "    model.add(InputLayer(input_shape=(nvariables,1)))\n",
    "    model = add_conv_module(model,filters = 15, \n",
    "                           kernel_size =2, \n",
    "                           pool_size = 2, \n",
    "                           regularizer = regularizer,\n",
    "                           kernel_initializer = kernel_initializer)\n",
    "    model = add_conv_module(model,filters = 10, \n",
    "                           kernel_size =2, \n",
    "                           pool_size = 2, \n",
    "                           regularizer = regularizer,\n",
    "                           kernel_initializer = kernel_initializer)\n",
    "    model = add_conv_module(model,filters = 5, \n",
    "                           kernel_size =2, \n",
    "                           pool_size = 2, \n",
    "                           regularizer = regularizer,\n",
    "                           kernel_initializer=kernel_initializer)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    # Output node\n",
    "    model.add(Dense(outnodes, activation='linear', kernel_initializer=kernel_initializer))\n",
    "    # Set loss and optimizers\n",
    "    adam = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=adam, loss=huber_loss, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029be2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Training model with l1_reg: 0.0 l2_reg: 0.0\n",
      "[INFO    ] Begin training ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 22, 15)            45        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 22, 15)            60        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 11, 15)            0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 11, 15)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 10, 10)            310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 10)            40        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4, 5)              105       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 5)              20        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2, 5)              0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 5)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 602\n",
      "Trainable params: 542\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 18s - loss: 30.4282 - acc: 0.8029 - val_loss: 17.9458 - val_acc: 0.9124\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 17.94576, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 17.94576, saving model to model_bchk_weights.h5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 16.2850 - acc: 0.9136 - val_loss: 15.9398 - val_acc: 0.9086\n",
      "\n",
      "Epoch 00002: val_loss improved from 17.94576 to 15.93985, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 17.94576 to 15.93985, saving model to model_bchk_weights.h5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 15.4036 - acc: 0.9136 - val_loss: 15.1757 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00003: val_loss improved from 15.93985 to 15.17572, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 15.93985 to 15.17572, saving model to model_bchk_weights.h5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 14.9891 - acc: 0.9140 - val_loss: 14.7176 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00004: val_loss improved from 15.17572 to 14.71758, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 15.17572 to 14.71758, saving model to model_bchk_weights.h5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 14.7829 - acc: 0.9135 - val_loss: 14.6002 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00005: val_loss improved from 14.71758 to 14.60025, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 14.71758 to 14.60025, saving model to model_bchk_weights.h5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 14.6176 - acc: 0.9139 - val_loss: 14.4523 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00006: val_loss improved from 14.60025 to 14.45230, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 14.60025 to 14.45230, saving model to model_bchk_weights.h5\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 14.5049 - acc: 0.9140 - val_loss: 15.5076 - val_acc: 0.9081\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 14.45230\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 14.45230\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 15s - loss: 14.4047 - acc: 0.9146 - val_loss: 14.1713 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00008: val_loss improved from 14.45230 to 14.17134, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 14.45230 to 14.17134, saving model to model_bchk_weights.h5\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 14.3062 - acc: 0.9149 - val_loss: 14.3259 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 14.17134\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 14.17134\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "1800/1800 - 14s - loss: 14.2429 - acc: 0.9151 - val_loss: 14.5060 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 14.17134\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 14.17134\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 14.1598 - acc: 0.9153 - val_loss: 14.4121 - val_acc: 0.9099\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 14.17134\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 14.17134\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 14.1061 - acc: 0.9153 - val_loss: 14.2266 - val_acc: 0.9124\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 14.17134\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 14.17134\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 15s - loss: 14.0694 - acc: 0.9153 - val_loss: 13.8337 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00013: val_loss improved from 14.17134 to 13.83368, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 14.17134 to 13.83368, saving model to model_bchk_weights.h5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 14.0319 - acc: 0.9153 - val_loss: 13.9733 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 13.83368\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 14.0122 - acc: 0.9155 - val_loss: 14.2379 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 13.83368\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 13.9693 - acc: 0.9157 - val_loss: 14.3575 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 13.83368\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 13.9516 - acc: 0.9156 - val_loss: 15.3237 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 13.83368\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 14s - loss: 13.9281 - acc: 0.9155 - val_loss: 13.9286 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 13.83368\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "1800/1800 - 15s - loss: 13.9083 - acc: 0.9154 - val_loss: 14.7555 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 13.83368\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 15s - loss: 13.8846 - acc: 0.9158 - val_loss: 13.9636 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 13.83368\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 13.83368\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0008100000384729356.\n",
      "1800/1800 - 15s - loss: 13.8556 - acc: 0.9159 - val_loss: 13.7705 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00021: val_loss improved from 13.83368 to 13.77052, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 13.83368 to 13.77052, saving model to model_bchk_weights.h5\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.8458 - acc: 0.9160 - val_loss: 14.5008 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 13.77052\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 13.77052\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.8326 - acc: 0.9163 - val_loss: 14.1205 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 13.77052\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 13.77052\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.8139 - acc: 0.9162 - val_loss: 13.6812 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00024: val_loss improved from 13.77052 to 13.68116, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 13.77052 to 13.68116, saving model to model_bchk_weights.h5\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.8011 - acc: 0.9162 - val_loss: 13.7033 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 13.68116\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 13.68116\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.8003 - acc: 0.9164 - val_loss: 13.6471 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00026: val_loss improved from 13.68116 to 13.64713, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 13.68116 to 13.64713, saving model to model_bchk_weights.h5\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.7699 - acc: 0.9166 - val_loss: 13.6557 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 13.64713\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 13.64713\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.7754 - acc: 0.9164 - val_loss: 13.7371 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 13.64713\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 13.64713\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.7664 - acc: 0.9164 - val_loss: 13.7772 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 13.64713\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 13.64713\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "1800/1800 - 15s - loss: 13.7556 - acc: 0.9165 - val_loss: 14.8071 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 13.64713\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 13.64713\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0007290000503417104.\n",
      "1800/1800 - 15s - loss: 13.7447 - acc: 0.9168 - val_loss: 13.5542 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00031: val_loss improved from 13.64713 to 13.55424, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 13.64713 to 13.55424, saving model to model_bchk_weights.h5\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.7247 - acc: 0.9169 - val_loss: 13.6229 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 13.55424\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.7199 - acc: 0.9168 - val_loss: 13.6198 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 13.55424\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.7153 - acc: 0.9169 - val_loss: 13.5998 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 13.55424\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.7211 - acc: 0.9170 - val_loss: 14.0348 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 13.55424\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.7122 - acc: 0.9170 - val_loss: 13.5555 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 13.55424\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.7016 - acc: 0.9170 - val_loss: 13.7075 - val_acc: 0.9161\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 13.55424\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.6976 - acc: 0.9172 - val_loss: 13.6993 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 13.55424\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.6861 - acc: 0.9172 - val_loss: 13.7483 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 13.55424\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "1800/1800 - 15s - loss: 13.6934 - acc: 0.9172 - val_loss: 13.6268 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 13.55424\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0006561000715009868.\n",
      "1800/1800 - 18s - loss: 13.6768 - acc: 0.9174 - val_loss: 13.5733 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 13.55424\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 16s - loss: 13.6760 - acc: 0.9172 - val_loss: 13.5657 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 13.55424\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 13.55424\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6605 - acc: 0.9172 - val_loss: 13.5339 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00043: val_loss improved from 13.55424 to 13.53393, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 13.55424 to 13.53393, saving model to model_bchk_weights.h5\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6582 - acc: 0.9173 - val_loss: 13.4866 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00044: val_loss improved from 13.53393 to 13.48656, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 13.53393 to 13.48656, saving model to model_bchk_weights.h5\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6606 - acc: 0.9173 - val_loss: 13.5983 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 13.48656\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 13.48656\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6489 - acc: 0.9174 - val_loss: 13.5429 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 13.48656\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 13.48656\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 15s - loss: 13.6478 - acc: 0.9174 - val_loss: 13.6066 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 13.48656\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 13.48656\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6456 - acc: 0.9172 - val_loss: 13.7022 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 13.48656\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 13.48656\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6469 - acc: 0.9172 - val_loss: 13.4782 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00049: val_loss improved from 13.48656 to 13.47824, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 13.48656 to 13.47824, saving model to model_bchk_weights.h5\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "1800/1800 - 15s - loss: 13.6380 - acc: 0.9174 - val_loss: 14.1377 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 13.47824\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 13.47824\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0005904900433961303.\n",
      "1800/1800 - 15s - loss: 13.6342 - acc: 0.9174 - val_loss: 13.7859 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 13.47824\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 13.47824\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6305 - acc: 0.9173 - val_loss: 13.5560 - val_acc: 0.9181\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 13.47824\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 13.47824\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6221 - acc: 0.9172 - val_loss: 13.4465 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00053: val_loss improved from 13.47824 to 13.44655, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 13.47824 to 13.44655, saving model to model_bchk_weights.h5\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6166 - acc: 0.9174 - val_loss: 13.4474 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 13.44655\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6176 - acc: 0.9173 - val_loss: 13.7808 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 13.44655\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6194 - acc: 0.9172 - val_loss: 13.4531 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 13.44655\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6223 - acc: 0.9172 - val_loss: 13.5848 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 13.44655\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6159 - acc: 0.9174 - val_loss: 13.4623 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 13.44655\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.5986 - acc: 0.9175 - val_loss: 14.0221 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 13.44655\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "1800/1800 - 15s - loss: 13.6182 - acc: 0.9173 - val_loss: 14.1125 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 13.44655\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0005314410547725857.\n",
      "1800/1800 - 15s - loss: 13.5968 - acc: 0.9174 - val_loss: 13.5000 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 13.44655\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.5948 - acc: 0.9174 - val_loss: 13.5798 - val_acc: 0.9157\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 13.44655\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.5925 - acc: 0.9175 - val_loss: 14.0196 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 13.44655\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.5960 - acc: 0.9175 - val_loss: 13.6963 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 13.44655\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.6022 - acc: 0.9175 - val_loss: 13.9175 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 13.44655\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.5990 - acc: 0.9175 - val_loss: 13.6695 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 13.44655\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.5977 - acc: 0.9175 - val_loss: 13.7665 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 13.44655\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 15s - loss: 13.5930 - acc: 0.9175 - val_loss: 13.5680 - val_acc: 0.9178\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 13.44655\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 16s - loss: 13.5928 - acc: 0.9174 - val_loss: 13.4537 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 13.44655\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "1800/1800 - 17s - loss: 13.5945 - acc: 0.9176 - val_loss: 14.1497 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 13.44655\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 13.44655\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00047829695977270604.\n",
      "1800/1800 - 18s - loss: 13.5796 - acc: 0.9178 - val_loss: 13.3973 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00071: val_loss improved from 13.44655 to 13.39731, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 13.44655 to 13.39731, saving model to model_bchk_weights.h5\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 17s - loss: 13.5835 - acc: 0.9176 - val_loss: 14.1288 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 13.39731\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 13.39731\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 18s - loss: 13.5774 - acc: 0.9179 - val_loss: 13.4359 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 13.39731\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 13.39731\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 18s - loss: 13.5856 - acc: 0.9176 - val_loss: 13.4953 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 13.39731\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 13.39731\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 17s - loss: 13.5742 - acc: 0.9176 - val_loss: 13.5225 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 13.39731\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 13.39731\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 16s - loss: 13.5860 - acc: 0.9177 - val_loss: 13.6039 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 13.39731\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 13.39731\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 16s - loss: 13.5755 - acc: 0.9178 - val_loss: 13.3811 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00077: val_loss improved from 13.39731 to 13.38113, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00077: val_loss improved from 13.39731 to 13.38113, saving model to model_bchk_weights.h5\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 16s - loss: 13.5784 - acc: 0.9177 - val_loss: 13.3959 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 13.38113\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 16s - loss: 13.5761 - acc: 0.9177 - val_loss: 13.4606 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 13.38113\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "1800/1800 - 16s - loss: 13.5720 - acc: 0.9178 - val_loss: 13.4267 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 13.38113\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0004304672533180565.\n",
      "1800/1800 - 16s - loss: 13.5565 - acc: 0.9180 - val_loss: 13.5947 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 13.38113\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 15s - loss: 13.5646 - acc: 0.9178 - val_loss: 13.4084 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 13.38113\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5677 - acc: 0.9179 - val_loss: 13.6627 - val_acc: 0.9179\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 13.38113\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5657 - acc: 0.9179 - val_loss: 13.4252 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 13.38113\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5611 - acc: 0.9179 - val_loss: 13.5314 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 13.38113\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5537 - acc: 0.9182 - val_loss: 13.4511 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 13.38113\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5546 - acc: 0.9181 - val_loss: 13.6322 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 13.38113\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5631 - acc: 0.9179 - val_loss: 13.4568 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 13.38113\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5654 - acc: 0.9179 - val_loss: 13.6560 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 13.38113\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "1800/1800 - 16s - loss: 13.5546 - acc: 0.9181 - val_loss: 13.4419 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 13.38113\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00038742052274756136.\n",
      "1800/1800 - 16s - loss: 13.5603 - acc: 0.9181 - val_loss: 13.4352 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 13.38113\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5525 - acc: 0.9181 - val_loss: 13.3911 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 13.38113\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5511 - acc: 0.9182 - val_loss: 13.5883 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 13.38113\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5528 - acc: 0.9181 - val_loss: 13.4594 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 13.38113\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5606 - acc: 0.9181 - val_loss: 13.6487 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 13.38113\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5423 - acc: 0.9182 - val_loss: 13.3950 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 13.38113\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5506 - acc: 0.9183 - val_loss: 13.3925 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 13.38113\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5469 - acc: 0.9182 - val_loss: 13.4312 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 13.38113\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 13.38113\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5542 - acc: 0.9184 - val_loss: 13.3449 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00099: val_loss improved from 13.38113 to 13.34494, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 13.38113 to 13.34494, saving model to model_bchk_weights.h5\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "1800/1800 - 15s - loss: 13.5427 - acc: 0.9182 - val_loss: 13.4689 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 13.34494\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0003486784757114947.\n",
      "1800/1800 - 15s - loss: 13.5348 - acc: 0.9183 - val_loss: 13.5050 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 13.34494\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5451 - acc: 0.9182 - val_loss: 13.4271 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 13.34494\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 15s - loss: 13.5424 - acc: 0.9181 - val_loss: 13.6130 - val_acc: 0.9161\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 13.34494\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5326 - acc: 0.9184 - val_loss: 13.6038 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 13.34494\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5373 - acc: 0.9181 - val_loss: 14.0882 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 13.34494\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5436 - acc: 0.9182 - val_loss: 13.9708 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 13.34494\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5399 - acc: 0.9183 - val_loss: 13.4955 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 13.34494\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5367 - acc: 0.9184 - val_loss: 13.4342 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 13.34494\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5425 - acc: 0.9183 - val_loss: 13.5080 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 13.34494\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "1800/1800 - 15s - loss: 13.5426 - acc: 0.9183 - val_loss: 13.4176 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 13.34494\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.00031381062290165574.\n",
      "1800/1800 - 15s - loss: 13.5275 - acc: 0.9183 - val_loss: 13.5466 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 13.34494\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5272 - acc: 0.9183 - val_loss: 13.6216 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 13.34494\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5255 - acc: 0.9185 - val_loss: 13.7063 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 13.34494\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5256 - acc: 0.9184 - val_loss: 13.5828 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 13.34494\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5345 - acc: 0.9181 - val_loss: 13.3745 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 13.34494\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5208 - acc: 0.9183 - val_loss: 13.5190 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 13.34494\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5288 - acc: 0.9182 - val_loss: 13.4190 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 13.34494\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5239 - acc: 0.9184 - val_loss: 13.4794 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 13.34494\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5292 - acc: 0.9184 - val_loss: 13.4296 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 13.34494\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "1800/1800 - 15s - loss: 13.5320 - acc: 0.9182 - val_loss: 13.4554 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 13.34494\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002824295632308349.\n",
      "1800/1800 - 15s - loss: 13.5115 - acc: 0.9185 - val_loss: 13.7676 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 13.34494\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5240 - acc: 0.9185 - val_loss: 13.3497 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 13.34494\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5185 - acc: 0.9184 - val_loss: 13.3598 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 13.34494\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5258 - acc: 0.9183 - val_loss: 13.5193 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 13.34494\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5190 - acc: 0.9184 - val_loss: 13.4576 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 13.34494\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5192 - acc: 0.9184 - val_loss: 13.4939 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 13.34494\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5133 - acc: 0.9184 - val_loss: 13.5239 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 13.34494\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5182 - acc: 0.9185 - val_loss: 13.4687 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 13.34494\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 15s - loss: 13.5086 - acc: 0.9185 - val_loss: 13.4577 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 13.34494\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "1800/1800 - 16s - loss: 13.5202 - acc: 0.9184 - val_loss: 13.4348 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 13.34494\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.00025418660952709616.\n",
      "1800/1800 - 15s - loss: 13.5067 - acc: 0.9186 - val_loss: 13.5500 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 13.34494\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 15s - loss: 13.5136 - acc: 0.9184 - val_loss: 13.7063 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 13.34494\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5042 - acc: 0.9187 - val_loss: 13.3547 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 13.34494\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5053 - acc: 0.9187 - val_loss: 13.3665 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 13.34494\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5119 - acc: 0.9186 - val_loss: 13.4342 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 13.34494\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5014 - acc: 0.9186 - val_loss: 13.3799 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 13.34494\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5017 - acc: 0.9187 - val_loss: 13.6325 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 13.34494\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5119 - acc: 0.9186 - val_loss: 13.3496 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 13.34494\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 13.34494\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5138 - acc: 0.9186 - val_loss: 13.3382 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00139: val_loss improved from 13.34494 to 13.33822, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00139: val_loss improved from 13.34494 to 13.33822, saving model to model_bchk_weights.h5\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "1800/1800 - 15s - loss: 13.5016 - acc: 0.9187 - val_loss: 13.3393 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 13.33822\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 13.33822\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 16s - loss: 13.5059 - acc: 0.9186 - val_loss: 13.5578 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 13.33822\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 13.33822\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 16s - loss: 13.5026 - acc: 0.9186 - val_loss: 13.3928 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 13.33822\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 13.33822\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 16s - loss: 13.4969 - acc: 0.9186 - val_loss: 13.3345 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00143: val_loss improved from 13.33822 to 13.33446, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00143: val_loss improved from 13.33822 to 13.33446, saving model to model_bchk_weights.h5\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 16s - loss: 13.5030 - acc: 0.9185 - val_loss: 13.3808 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 13.33446\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 13.33446\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 16s - loss: 13.4954 - acc: 0.9188 - val_loss: 13.3509 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 13.33446\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 13.33446\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 16s - loss: 13.4961 - acc: 0.9186 - val_loss: 13.3228 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00146: val_loss improved from 13.33446 to 13.32281, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00146: val_loss improved from 13.33446 to 13.32281, saving model to model_bchk_weights.h5\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 15s - loss: 13.5083 - acc: 0.9185 - val_loss: 13.4077 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 13.32281\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 15s - loss: 13.5062 - acc: 0.9186 - val_loss: 13.7169 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 13.32281\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 15s - loss: 13.5028 - acc: 0.9186 - val_loss: 13.5705 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 13.32281\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "1800/1800 - 15s - loss: 13.5053 - acc: 0.9186 - val_loss: 13.3913 - val_acc: 0.9181\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 13.32281\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.00020589114428730683.\n",
      "1800/1800 - 15s - loss: 13.4922 - acc: 0.9187 - val_loss: 13.4535 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 13.32281\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4929 - acc: 0.9187 - val_loss: 13.3437 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 13.32281\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4952 - acc: 0.9188 - val_loss: 13.3261 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 13.32281\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4924 - acc: 0.9187 - val_loss: 13.3281 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 13.32281\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.5031 - acc: 0.9186 - val_loss: 13.4228 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 13.32281\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4928 - acc: 0.9188 - val_loss: 13.3607 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 13.32281\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4899 - acc: 0.9187 - val_loss: 13.3256 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 13.32281\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4860 - acc: 0.9188 - val_loss: 13.3845 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 13.32281\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 13.32281\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "1800/1800 - 15s - loss: 13.4965 - acc: 0.9186 - val_loss: 13.3049 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00159: val_loss improved from 13.32281 to 13.30495, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00159: val_loss improved from 13.32281 to 13.30495, saving model to model_bchk_weights.h5\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 15s - loss: 13.4924 - acc: 0.9187 - val_loss: 13.3744 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 13.30495\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.00018530203378759326.\n",
      "1800/1800 - 15s - loss: 13.4895 - acc: 0.9188 - val_loss: 13.3344 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 13.30495\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 17s - loss: 13.4879 - acc: 0.9188 - val_loss: 13.3356 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 13.30495\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 16s - loss: 13.4892 - acc: 0.9187 - val_loss: 13.3948 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 13.30495\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 16s - loss: 13.4973 - acc: 0.9187 - val_loss: 13.3528 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 13.30495\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 15s - loss: 13.4877 - acc: 0.9188 - val_loss: 13.4052 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 13.30495\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 15s - loss: 13.4949 - acc: 0.9188 - val_loss: 13.3889 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 13.30495\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 15s - loss: 13.4866 - acc: 0.9188 - val_loss: 13.4716 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 13.30495\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 15s - loss: 13.4949 - acc: 0.9186 - val_loss: 13.4269 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 13.30495\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 15s - loss: 13.4805 - acc: 0.9188 - val_loss: 13.3213 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 13.30495\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "1800/1800 - 15s - loss: 13.4859 - acc: 0.9189 - val_loss: 13.3883 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 13.30495\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.00016677183302817866.\n",
      "1800/1800 - 15s - loss: 13.4888 - acc: 0.9188 - val_loss: 13.5409 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 13.30495\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4803 - acc: 0.9188 - val_loss: 13.3460 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 13.30495\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4860 - acc: 0.9189 - val_loss: 13.3112 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 13.30495\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 13.30495\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4827 - acc: 0.9187 - val_loss: 13.3032 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00174: val_loss improved from 13.30495 to 13.30322, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 13.30495 to 13.30322, saving model to model_bchk_weights.h5\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4879 - acc: 0.9187 - val_loss: 13.3085 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 13.30322\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4767 - acc: 0.9189 - val_loss: 13.3378 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 13.30322\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4794 - acc: 0.9189 - val_loss: 13.3418 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 13.30322\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4807 - acc: 0.9189 - val_loss: 13.4048 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 13.30322\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4852 - acc: 0.9187 - val_loss: 13.3556 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 13.30322\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "1800/1800 - 15s - loss: 13.4814 - acc: 0.9189 - val_loss: 13.3267 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 13.30322\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.00015009464841568844.\n",
      "1800/1800 - 15s - loss: 13.4775 - acc: 0.9188 - val_loss: 13.3401 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 13.30322\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4801 - acc: 0.9187 - val_loss: 13.4164 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 13.30322\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4820 - acc: 0.9188 - val_loss: 13.3237 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 13.30322\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4796 - acc: 0.9188 - val_loss: 13.3077 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 13.30322\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4796 - acc: 0.9189 - val_loss: 13.3445 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 13.30322\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4750 - acc: 0.9189 - val_loss: 13.4428 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 13.30322\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4798 - acc: 0.9189 - val_loss: 13.3170 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 13.30322\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 15s - loss: 13.4749 - acc: 0.9187 - val_loss: 13.3164 - val_acc: 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00188: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 13.30322\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 16s - loss: 13.4746 - acc: 0.9189 - val_loss: 13.3150 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 13.30322\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "1800/1800 - 16s - loss: 13.4831 - acc: 0.9188 - val_loss: 13.3236 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 13.30322\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001350851875031367.\n",
      "1800/1800 - 15s - loss: 13.4698 - acc: 0.9189 - val_loss: 13.7509 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 13.30322\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 15s - loss: 13.4754 - acc: 0.9190 - val_loss: 13.3266 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 13.30322\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 15s - loss: 13.4686 - acc: 0.9189 - val_loss: 13.3195 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 13.30322\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 17s - loss: 13.4729 - acc: 0.9191 - val_loss: 13.3257 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 13.30322\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 17s - loss: 13.4709 - acc: 0.9188 - val_loss: 13.3985 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 13.30322\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 17s - loss: 13.4689 - acc: 0.9190 - val_loss: 13.3225 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 13.30322\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 16s - loss: 13.4736 - acc: 0.9188 - val_loss: 13.3385 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 13.30322\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 16s - loss: 13.4775 - acc: 0.9189 - val_loss: 13.4268 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 13.30322\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 16s - loss: 13.4707 - acc: 0.9189 - val_loss: 13.3064 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 13.30322\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "1800/1800 - 16s - loss: 13.4741 - acc: 0.9189 - val_loss: 13.3805 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 13.30322\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.00012157666351413355.\n",
      "1800/1800 - 15s - loss: 13.4718 - acc: 0.9188 - val_loss: 13.3178 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 13.30322\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 16s - loss: 13.4643 - acc: 0.9189 - val_loss: 13.3176 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 13.30322\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 16s - loss: 13.4714 - acc: 0.9189 - val_loss: 13.3156 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 13.30322\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 16s - loss: 13.4661 - acc: 0.9190 - val_loss: 13.3062 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 13.30322\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 16s - loss: 13.4749 - acc: 0.9188 - val_loss: 13.3317 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 13.30322\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 16s - loss: 13.4576 - acc: 0.9189 - val_loss: 13.3158 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 13.30322\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 16s - loss: 13.4650 - acc: 0.9189 - val_loss: 13.3231 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 13.30322\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 13.30322\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 15s - loss: 13.4675 - acc: 0.9189 - val_loss: 13.2985 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00208: val_loss improved from 13.30322 to 13.29850, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00208: val_loss improved from 13.30322 to 13.29850, saving model to model_bchk_weights.h5\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 15s - loss: 13.4682 - acc: 0.9188 - val_loss: 13.4552 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 13.29850\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "1800/1800 - 17s - loss: 13.4711 - acc: 0.9189 - val_loss: 13.4769 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 13.29850\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.00010941899454337544.\n",
      "1800/1800 - 17s - loss: 13.4623 - acc: 0.9188 - val_loss: 13.3207 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 13.29850\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 17s - loss: 13.4679 - acc: 0.9190 - val_loss: 13.3469 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 13.29850\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 16s - loss: 13.4614 - acc: 0.9190 - val_loss: 13.3155 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 13.29850\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 16s - loss: 13.4586 - acc: 0.9190 - val_loss: 13.3450 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 13.29850\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 15s - loss: 13.4644 - acc: 0.9189 - val_loss: 13.3100 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 13.29850\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 15s - loss: 13.4656 - acc: 0.9189 - val_loss: 13.3058 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 13.29850\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 16s - loss: 13.4673 - acc: 0.9190 - val_loss: 13.3094 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 13.29850\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 16s - loss: 13.4666 - acc: 0.9187 - val_loss: 13.3046 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 13.29850\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 16s - loss: 13.4703 - acc: 0.9189 - val_loss: 13.3187 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 13.29850\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 13.29850\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "1800/1800 - 17s - loss: 13.4659 - acc: 0.9190 - val_loss: 13.2943 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00220: val_loss improved from 13.29850 to 13.29429, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00220: val_loss improved from 13.29850 to 13.29429, saving model to model_bchk_weights.h5\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 9.847709443420172e-05.\n",
      "1800/1800 - 16s - loss: 13.4653 - acc: 0.9189 - val_loss: 13.2998 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 13.29429\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 13.29429\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 16s - loss: 13.4624 - acc: 0.9189 - val_loss: 13.2895 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00222: val_loss improved from 13.29429 to 13.28947, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00222: val_loss improved from 13.29429 to 13.28947, saving model to model_bchk_weights.h5\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 16s - loss: 13.4622 - acc: 0.9189 - val_loss: 13.3680 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 13.28947\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 16s - loss: 13.4580 - acc: 0.9190 - val_loss: 13.3091 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 13.28947\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 15s - loss: 13.4636 - acc: 0.9189 - val_loss: 13.3077 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 13.28947\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 16s - loss: 13.4582 - acc: 0.9191 - val_loss: 13.4019 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 13.28947\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 16s - loss: 13.4649 - acc: 0.9189 - val_loss: 13.3723 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 13.28947\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 15s - loss: 13.4563 - acc: 0.9189 - val_loss: 13.3104 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 13.28947\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 17s - loss: 13.4660 - acc: 0.9189 - val_loss: 13.3174 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 13.28947\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "1800/1800 - 16s - loss: 13.4574 - acc: 0.9189 - val_loss: 13.3275 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 13.28947\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 8.862938630045391e-05.\n",
      "1800/1800 - 16s - loss: 13.4585 - acc: 0.9190 - val_loss: 13.3126 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 13.28947\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 16s - loss: 13.4583 - acc: 0.9190 - val_loss: 13.3229 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 13.28947\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 17s - loss: 13.4589 - acc: 0.9189 - val_loss: 13.3042 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 13.28947\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 17s - loss: 13.4575 - acc: 0.9189 - val_loss: 13.3266 - val_acc: 0.9203\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 13.28947\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 16s - loss: 13.4550 - acc: 0.9191 - val_loss: 13.4063 - val_acc: 0.9178\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 13.28947\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 17s - loss: 13.4588 - acc: 0.9190 - val_loss: 13.3126 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 13.28947\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 15s - loss: 13.4554 - acc: 0.9189 - val_loss: 13.2997 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 13.28947\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 16s - loss: 13.4590 - acc: 0.9189 - val_loss: 13.3174 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 13.28947\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 16s - loss: 13.4533 - acc: 0.9190 - val_loss: 13.3065 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 13.28947\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "1800/1800 - 16s - loss: 13.4608 - acc: 0.9188 - val_loss: 13.3218 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 13.28947\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 17s - loss: 13.4474 - acc: 0.9189 - val_loss: 13.3042 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 13.28947\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 16s - loss: 13.4538 - acc: 0.9190 - val_loss: 13.3197 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 13.28947\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 16s - loss: 13.4510 - acc: 0.9190 - val_loss: 13.3186 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 13.28947\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 13.28947\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 15s - loss: 13.4525 - acc: 0.9190 - val_loss: 13.2887 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00244: val_loss improved from 13.28947 to 13.28874, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00244: val_loss improved from 13.28947 to 13.28874, saving model to model_bchk_weights.h5\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 15s - loss: 13.4587 - acc: 0.9190 - val_loss: 13.2987 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 13.28874\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 15s - loss: 13.4501 - acc: 0.9190 - val_loss: 13.3147 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 13.28874\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 15s - loss: 13.4529 - acc: 0.9191 - val_loss: 13.3363 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 13.28874\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 15s - loss: 13.4521 - acc: 0.9191 - val_loss: 13.2932 - val_acc: 0.9203\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 13.28874\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 15s - loss: 13.4523 - acc: 0.9191 - val_loss: 13.3220 - val_acc: 0.9203\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 13.28874\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "1800/1800 - 15s - loss: 13.4534 - acc: 0.9188 - val_loss: 13.3229 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 13.28874\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 7.178980231401511e-05.\n",
      "1800/1800 - 15s - loss: 13.4598 - acc: 0.9191 - val_loss: 13.3355 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 13.28874\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 15s - loss: 13.4434 - acc: 0.9191 - val_loss: 13.3772 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 13.28874\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 15s - loss: 13.4474 - acc: 0.9191 - val_loss: 13.3245 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 13.28874\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 13.28874\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 15s - loss: 13.4480 - acc: 0.9191 - val_loss: 13.2808 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00254: val_loss improved from 13.28874 to 13.28083, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00254: val_loss improved from 13.28874 to 13.28083, saving model to model_bchk_weights.h5\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 15s - loss: 13.4510 - acc: 0.9190 - val_loss: 13.2737 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00255: val_loss improved from 13.28083 to 13.27373, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00255: val_loss improved from 13.28083 to 13.27373, saving model to model_bchk_weights.h5\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 15s - loss: 13.4473 - acc: 0.9190 - val_loss: 13.3432 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 13.27373\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 16s - loss: 13.4536 - acc: 0.9189 - val_loss: 13.2910 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 13.27373\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 16s - loss: 13.4517 - acc: 0.9189 - val_loss: 13.2906 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 13.27373\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 17s - loss: 13.4444 - acc: 0.9191 - val_loss: 13.3599 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 13.27373\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "1800/1800 - 17s - loss: 13.4484 - acc: 0.9191 - val_loss: 13.3193 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 13.27373\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 6.461082011810504e-05.\n",
      "1800/1800 - 17s - loss: 13.4464 - acc: 0.9191 - val_loss: 13.3291 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 13.27373\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 17s - loss: 13.4480 - acc: 0.9190 - val_loss: 13.2988 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 13.27373\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 17s - loss: 13.4480 - acc: 0.9190 - val_loss: 13.2804 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 13.27373\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 17s - loss: 13.4480 - acc: 0.9191 - val_loss: 13.3034 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 13.27373\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 17s - loss: 13.4449 - acc: 0.9190 - val_loss: 13.2892 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 13.27373\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 17s - loss: 13.4494 - acc: 0.9190 - val_loss: 13.2758 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 13.27373\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 17s - loss: 13.4485 - acc: 0.9190 - val_loss: 13.3082 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 13.27373\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 16s - loss: 13.4425 - acc: 0.9191 - val_loss: 13.2771 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 13.27373\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 16s - loss: 13.4475 - acc: 0.9190 - val_loss: 13.3180 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 13.27373\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "1800/1800 - 15s - loss: 13.4540 - acc: 0.9189 - val_loss: 13.2946 - val_acc: 0.9203\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 13.27373\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 5.8149741380475466e-05.\n",
      "1800/1800 - 16s - loss: 13.4397 - acc: 0.9191 - val_loss: 13.3186 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 13.27373\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 16s - loss: 13.4407 - acc: 0.9190 - val_loss: 13.2802 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 13.27373\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 13.27373\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 - 16s - loss: 13.4426 - acc: 0.9190 - val_loss: 13.2695 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00273: val_loss improved from 13.27373 to 13.26950, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00273: val_loss improved from 13.27373 to 13.26950, saving model to model_bchk_weights.h5\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 16s - loss: 13.4482 - acc: 0.9191 - val_loss: 13.2828 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 13.26950\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 16s - loss: 13.4433 - acc: 0.9192 - val_loss: 13.3048 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 13.26950\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 16s - loss: 13.4460 - acc: 0.9190 - val_loss: 13.3239 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 13.26950\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 16s - loss: 13.4379 - acc: 0.9191 - val_loss: 13.2957 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 13.26950\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 15s - loss: 13.4489 - acc: 0.9191 - val_loss: 13.3203 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 13.26950\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 17s - loss: 13.4436 - acc: 0.9191 - val_loss: 13.2752 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 13.26950\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "1800/1800 - 16s - loss: 13.4436 - acc: 0.9191 - val_loss: 13.2784 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 13.26950\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 5.233476658759173e-05.\n",
      "1800/1800 - 16s - loss: 13.4489 - acc: 0.9190 - val_loss: 13.2870 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 13.26950\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 15s - loss: 13.4403 - acc: 0.9191 - val_loss: 13.2835 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 13.26950\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 15s - loss: 13.4444 - acc: 0.9191 - val_loss: 13.2796 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 13.26950\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 16s - loss: 13.4434 - acc: 0.9191 - val_loss: 13.2773 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 13.26950\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 17s - loss: 13.4397 - acc: 0.9191 - val_loss: 13.2951 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 13.26950\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 17s - loss: 13.4446 - acc: 0.9191 - val_loss: 13.2997 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 13.26950\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 16s - loss: 13.4490 - acc: 0.9190 - val_loss: 13.3319 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 13.26950\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 15s - loss: 13.4449 - acc: 0.9191 - val_loss: 13.2773 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 13.26950\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 15s - loss: 13.4487 - acc: 0.9189 - val_loss: 13.2923 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 13.26950\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "1800/1800 - 15s - loss: 13.4473 - acc: 0.9190 - val_loss: 13.2697 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 13.26950\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 4.7101289601414466e-05.\n",
      "1800/1800 - 15s - loss: 13.4415 - acc: 0.9191 - val_loss: 13.2820 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 13.26950\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 15s - loss: 13.4395 - acc: 0.9190 - val_loss: 13.3068 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 13.26950\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 15s - loss: 13.4423 - acc: 0.9190 - val_loss: 13.2803 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 13.26950\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 17s - loss: 13.4440 - acc: 0.9190 - val_loss: 13.2889 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 13.26950\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 18s - loss: 13.4392 - acc: 0.9190 - val_loss: 13.2732 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 13.26950\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 18s - loss: 13.4411 - acc: 0.9189 - val_loss: 13.2883 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 13.26950\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 16s - loss: 13.4420 - acc: 0.9191 - val_loss: 13.2740 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 13.26950\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 16s - loss: 13.4454 - acc: 0.9192 - val_loss: 13.2714 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 13.26950\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 17s - loss: 13.4444 - acc: 0.9191 - val_loss: 13.2865 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 13.26950\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 4.7101289965212345e-05.\n",
      "1800/1800 - 17s - loss: 13.4434 - acc: 0.9190 - val_loss: 13.2755 - val_acc: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Done training. Time elapsed: 1:17:20.677787 sec\n",
      "[INFO    ] Epoch 300/300 - loss: 13.443351745605469 - val_loss: 13.275480270385742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00300: val_loss did not improve from 13.26950\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 13.26950\n"
     ]
    }
   ],
   "source": [
    "assert(keras.backend.backend() == 'tensorflow')\n",
    "\n",
    "gradient_clip_norm = 10.\n",
    "l1_reg = 0.0\n",
    "l2_reg = 0.0\n",
    "lr = 1e-3\n",
    "model = create_conv_model(nvariables=nvariables, \n",
    "                          lr=lr, \n",
    "                          clipnorm=gradient_clip_norm,\n",
    "                          l1_reg=l1_reg, \n",
    "                          l2_reg=l2_reg, \n",
    "                          outnodes=2, \n",
    "                          kernel_initializer = \"he_uniform\")\n",
    "\n",
    "logger.info('Training model with l1_reg: {0} l2_reg: {1}'.format(l1_reg, l2_reg))\n",
    "normal_epochs = 300\n",
    "normal_batch_size = 1000\n",
    "\n",
    "history = train_model(model, x_train_displ, np.column_stack((y_train_displ, dxy_train_displ)),\n",
    "                    save_model=False, epochs=normal_epochs, batch_size=normal_batch_size,\n",
    "                    callbacks=[lr_decay,\n",
    "                               modelbestcheck,\n",
    "                               modelbestcheck_weights], validation_split=0.2, verbose=2)\n",
    "\n",
    "metrics = [len(history.history['loss']), history.history['loss'][-1], history.history['val_loss'][-1]]\n",
    "logger.info('Epoch {0}/{0} - loss: {1} - val_loss: {2}'.format(*metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee7e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 375x375 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5),dpi = 75)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss with conv + batchnorm + maxpool layer')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1519197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for 50-fold cv for y: 68.60650785255\n",
      "Average RMSE for 50-fold cv for dxy: 20.98529124930171\n"
     ]
    }
   ],
   "source": [
    "k_fold_validation(model = model, \n",
    "                  x = x, \n",
    "                  y = y, \n",
    "                  dxy = dxy, \n",
    "                  folds =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0390752b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Parameters</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epochs</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch size</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>learning rate</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BN momentum</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BN epsilon</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kernel Initializer</td>\n",
       "      <td>he_uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L1 Reg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L2 Reg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Training Parameters       Value\n",
       "1              epochs         300\n",
       "2          batch size        1000\n",
       "3       learning rate       0.001\n",
       "4         BN momentum         0.7\n",
       "5          BN epsilon      0.0001\n",
       "6  Kernel Initializer  he_uniform\n",
       "7              L1 Reg         0.0\n",
       "8              L2 Reg         0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params = {\"epochs\":normal_epochs, \n",
    "                   \"batch size\":normal_batch_size, \n",
    "                   \"learning rate\":lr,\n",
    "                   \"BN momentum\":0.7,\n",
    "                   \"BN epsilon\":1e-4,\n",
    "                   \"Kernel Initializer\":\"he_uniform\",\n",
    "                   \"L1 Reg\":l1_reg,\n",
    "                   \"L2 Reg\":l2_reg\n",
    "                  }\n",
    "df = pd.DataFrame(training_params.items(), \n",
    "                  columns=['Training Parameters', 'Value'],\n",
    "                  index = [i+1 for i in range(len(training_params))])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e28d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### END ############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
