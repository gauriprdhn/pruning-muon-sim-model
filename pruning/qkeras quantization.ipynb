{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd8afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Using numpy 1.21.0\n",
      "[INFO    ] Using tensorflow 2.4.1\n",
      "[INFO    ] Using keras 2.4.3\n",
      "[INFO    ] Using scipy 1.7.0\n",
      "[INFO    ] Using sklearn 0.24.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-86818f11bff2>:33: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. In future versions, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = copy.copy(mpl.cm.get_cmap(\"viridis\"))\n",
      "  my_cmap.set_under('w',1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "from qkeras import *\n",
    "from qkeras import utils\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential, Model,model_from_json\n",
    "from keras.layers import Input,Dense\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "from nn_globals import *\n",
    "from nn_encode_displ import nlayers, nvariables\n",
    "from nn_models import (create_model, create_model_sequential_bn2, create_model_pruned, \n",
    "                       create_model_sequential, create_model_sequential_bn,\n",
    "                       lr_decay, modelbestcheck, modelbestcheck_weights)\n",
    "from nn_training import train_model\n",
    "from nn_models import load_my_model, update_keras_custom_objects\n",
    "from keras.models import Model\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Setup matplotlib\n",
    "plt.style.use('tdrstyle.mplstyle')\n",
    "\n",
    "from nn_plotting import (gaus, fit_gaus, np_printoptions, \\\n",
    "                         find_efficiency_errors)\n",
    "\n",
    "eps = 1e-7\n",
    "my_cmap = plt.cm.viridis\n",
    "my_cmap.set_under('w',1)\n",
    "my_palette = (\"#377eb8\", \"#e41a1c\", \"#984ea3\", \"#ff7f00\", \"#4daf4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a4508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluate:\n",
    "    def __init__(self,X_test,y_test):\n",
    "        self.X = X_test\n",
    "        self.y = y_test[0]\n",
    "        self.dxy = y_test[1]\n",
    "    \n",
    "    def compute_data_statistics(self,ctype = \"y\",label=\"data\"):\n",
    "        if ctype == \"y\":\n",
    "            x = self.recalibrate(self.y,reg_pt_scale)\n",
    "            x = x**(-1)\n",
    "        else:\n",
    "            x = self.recalibrate(self.dxy,reg_dxy_scale)\n",
    "        df_describe = pd.DataFrame(x, columns = [label])\n",
    "        print(df_describe.describe())\n",
    "    \n",
    "    def rmse(self,y_true, y_predicted):\n",
    "        assert(y_true.shape[0] == y_predicted.shape[0])\n",
    "        n = y_true.shape[0]\n",
    "        sum_square = np.sum((y_true - y_predicted)**2)\n",
    "        return math.sqrt(sum_square/n)\n",
    "    \n",
    "    def adjusted_r_2(self,y_true, y_predicted):\n",
    "        y_addC = sm.add_constant(y_true)\n",
    "        result = sm.OLS(y_predicted, y_addC).fit()\n",
    "        print(result.rsquared, result.rsquared_adj)\n",
    "\n",
    "    def recalibrate(self,x,scale):\n",
    "        return x/scale\n",
    "    \n",
    "    def inverse(self,arr):\n",
    "        arr_inv = 1./arr\n",
    "        arr_inv[arr_inv == np.inf] = 0.\n",
    "        return arr_inv\n",
    "    \n",
    "    def predict(self,model,batch_size = 256):\n",
    "        y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "        dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "        \n",
    "        y_test = model.predict(self.X,batch_size = 2000)\n",
    "        y_test_meas = y_test[:,0]\n",
    "        dxy_test_meas = y_test[:,1]\n",
    "        y_test_meas = self.recalibrate(y_test_meas,reg_pt_scale)\n",
    "        dxy_test_meas = self.recalibrate(dxy_test_meas,reg_dxy_scale)   \n",
    "    \n",
    "        y_test_meas = y_test_meas.reshape(-1)\n",
    "        dxy_test_meas = dxy_test_meas.reshape(-1)\n",
    "\n",
    "        return y_test_meas, dxy_test_meas\n",
    "    \n",
    "    def compute_error(self,y_predicted,ctype = \"y\"):\n",
    "        if ctype == \"y\":\n",
    "            y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "            print(\"RMSE Error for momentum:\",self.rmse(self.inverse(y_test_true),\\\n",
    "                                                                              self.inverse(y_predicted)))\n",
    "        else:\n",
    "            dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "            print(\"RMSE Error for dxy:\",self.rmse(dxy_test_true,y_predicted))\n",
    "\n",
    "    def get_error(self,y_predicted,ctype = \"y\"):\n",
    "        if ctype == \"y\":\n",
    "            y_test_true = self.recalibrate(self.y,reg_pt_scale)\n",
    "            return self.rmse(self.inverse(y_test_true),self.inverse(y_predicted))\n",
    "        else:\n",
    "            dxy_test_true = self.recalibrate(self.dxy, reg_dxy_scale)\n",
    "            return self.rmse(dxy_test_true,y_predicted)\n",
    "\n",
    "def k_fold_validation(model, x, y, dxy, folds =10):\n",
    "    x_copy = np.copy(x)\n",
    "    y_copy = np.copy(y)\n",
    "    dxy_copy = np.copy(dxy)\n",
    "    assert x_copy.shape[0] == y_copy.shape[0] == dxy_copy.shape[0]\n",
    "    fold_size = int(x_copy.shape[0] / folds)\n",
    "    x_splits, y_splits, dxy_splits = [], [], []\n",
    "    for i in range(folds):\n",
    "        indices = np.random.choice(x_copy.shape[0],fold_size, replace=False)  \n",
    "        x_splits.append(x_copy[indices])\n",
    "        y_splits.append(y_copy[indices])\n",
    "        dxy_splits.append(dxy_copy[indices])\n",
    "        x_copy = np.delete(x_copy,indices,axis = 0)\n",
    "        y_copy = np.delete(y_copy,indices,axis = 0)\n",
    "        dxy_copy = np.delete(dxy_copy,indices,axis = 0)\n",
    "    rmse_y, rmse_dxy = [],[]\n",
    "    for i in range(folds):\n",
    "        evaluate_obj = evaluate(x_splits[i], tuple([y_splits[i],dxy_splits[i]]))\n",
    "        y_predicted , dxy_predicted = evaluate_obj.predict(model = model)\n",
    "        rmse_y.append(evaluate_obj.get_error(y_predicted,ctype=\"y\"))\n",
    "        rmse_dxy.append(evaluate_obj.get_error(dxy_predicted,ctype=\"dxy\"))\n",
    "    print('Average RMSE for '+ str(folds) + '-fold cv for y:', np.mean(rmse_y))\n",
    "    print('Average RMSE for '+ str(folds) + '-fold cv for dxy:', np.mean(rmse_dxy))\n",
    "    \n",
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    squared_loss = 0.5*K.square(x)\n",
    "    absolute_loss = delta * (x - 0.5*delta)\n",
    "    #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "    xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "    return K.mean(xx, axis=-1)\n",
    "\n",
    "def get_sparsity(weights):\n",
    "    \"\"\"\n",
    "    Code borrowed from https://github.com/google/qkeras/blob/master/qkeras/utils.py#L937\n",
    "    Returns the sparsity as the ratio of non-zero weights to the total weights within the weights matrix.\n",
    "    \"\"\"\n",
    "    return 1.0 - np.count_nonzero(weights) / float(weights.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc92e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_muon_displ = \"NN_input_params_FlatXYZ.npz\"\n",
    "\n",
    "nentries = 100000000\n",
    "\n",
    "def _handle_nan_in_x(x):\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    x[x==-999.0] = 0.0\n",
    "    return x\n",
    "\n",
    "def _zero_out_x(x):\n",
    "    x = 0.0\n",
    "    return x\n",
    "    \n",
    "def _fixME1Ring(x):\n",
    "    for i in range(len(x)):\n",
    "        if (x[i,0] != 0.0): x[i,18] = x[i,18] + 1\n",
    "    return x   \n",
    "\n",
    "def muon_data(filename, reg_pt_scale=1.0, reg_dxy_scale=1.0, correct_for_eta=False):\n",
    "    try:\n",
    "        logger.info('Loading muon data from {0} ...'.format(filename))\n",
    "        loaded = np.load(filename)\n",
    "        the_variables = loaded['variables']\n",
    "        the_parameters = loaded['parameters']\n",
    "        # print(the_variables.shape)\n",
    "        the_variables = the_variables[:nentries]\n",
    "        the_parameters = the_parameters[:nentries]\n",
    "        logger.info('Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "        logger.info('Loaded the parameters with shape {0}'.format(the_parameters.shape))\n",
    "    except:\n",
    "        logger.error('Failed to load data from file: {0}'.format(filename))\n",
    "\n",
    "    assert(the_variables.shape[0] == the_parameters.shape[0])\n",
    "    _handle_nan_in_x(the_variables)\n",
    "      #_fixME1Ring(the_variables)\n",
    "    _handle_nan_in_x(the_parameters)\n",
    "    mask = np.logical_or(np.logical_or( np.logical_or((the_variables[:,23] == 11), (the_variables[:,23] == 13)), (the_variables[:,23] == 14)),(the_variables[:,23] == 15)) \n",
    "\n",
    "    the_variables = the_variables[mask]  \n",
    "    the_parameters = the_parameters[mask]  \n",
    "    assert(the_variables.shape[0] == the_parameters.shape[0])\n",
    "\n",
    "    x = the_variables[:,0:23]\n",
    "    y = reg_pt_scale*the_parameters[:,0]\n",
    "#    print (x[0:30,:], the_variables[0:30,23])\n",
    "#    print (y[0:30])\n",
    "    phi = the_parameters[:,1] \n",
    "    eta = the_parameters[:,2] \n",
    "    vx = the_parameters[:,3] \n",
    "    vy = the_parameters[:,4] \n",
    "    vz = the_parameters[:,5]      \n",
    "    dxy = vy * np.cos(phi) - vx * np.sin(phi) \n",
    "    dz = vz\n",
    "    w = np.abs(y)/0.2 + 1.0\n",
    "    x_mask = the_parameters[:,5]\n",
    "    x_road = the_parameters[:,5] \n",
    "    _zero_out_x(x_mask)\n",
    "    _zero_out_x(x_road)  \n",
    "    logger.info('Loaded the encoded variables with shape {0}'.format(x.shape))\n",
    "    logger.info('Loaded the encoded parameters with shape {0}'.format(y.shape))\n",
    "\n",
    "    return x, y, dxy, dz, w, x_mask, x_road\n",
    "\n",
    "def muon_data_split(filename, reg_pt_scale=1.0, reg_dxy_scale=1.0, test_size=0.5, correct_for_eta=False):\n",
    "    x, y, dxy, dz, w, x_mask, x_road = muon_data(filename, reg_pt_scale=reg_pt_scale, reg_dxy_scale=reg_dxy_scale, correct_for_eta=correct_for_eta)\n",
    "\n",
    "    # Split dataset in training and testing\n",
    "    x_train, x_test, y_train, y_test, dxy_train, dxy_test, dz_train, dz_test, w_train, w_test, x_mask_train, x_mask_test, x_road_train, x_road_test = train_test_split(x, y, dxy, dz, w, x_mask, x_road, test_size=test_size)\n",
    "    logger.info('Loaded # of training and testing events: {0}'.format((x_train.shape[0], x_test.shape[0])))\n",
    "\n",
    "    # Check for cases where the number of events in the last batch could be too few\n",
    "    validation_split = 0.1\n",
    "    train_num_samples = int(x_train.shape[0] * (1.0-validation_split))\n",
    "    val_num_samples = x_train.shape[0] - train_num_samples\n",
    "    batch_size = 128\n",
    "    if (train_num_samples%batch_size) < 100:\n",
    "        logger.warning('The last batch for training could be too few! ({0}%{1})={2}. Please change test_size.'.format(train_num_samples, batch_size, train_num_samples%batch_size))\n",
    "        logger.warning('Try this formula: int(int({0}*{1})*{2}) % 128'.format(x.shape[0], 1.0-test_size, 1.0-validation_split))\n",
    "    train_num_samples = int(x_train.shape[0] * 2 * (1.0-validation_split))\n",
    "    val_num_samples = x_train.shape[0] - train_num_samples\n",
    "    batch_size = 128\n",
    "    if (train_num_samples%batch_size) < 100:\n",
    "        logger.warning('The last batch for training after mixing could be too few! ({0}%{1})={2}. Please change test_size.'.format(train_num_samples, batch_size, train_num_samples%batch_size))\n",
    "        logger.warning('Try this formula: int(int({0}*{1})*2*{2}) % 128'.format(x.shape[0], 1.0-test_size, 1.0-validation_split))\n",
    "    return x_train, x_test, y_train, y_test, dxy_train, dxy_test, dz_train, dz_test, w_train, w_test, x_mask_train, x_mask_test, x_road_train, x_road_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246d5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Loading muon data from NN_input_params_FlatXYZ.npz ...\n",
      "[INFO    ] Loaded the variables with shape (19300000, 25)\n",
      "[INFO    ] Loaded the parameters with shape (19300000, 6)\n",
      "[INFO    ] Loaded the encoded variables with shape (3284620, 23)\n",
      "[INFO    ] Loaded the encoded parameters with shape (3284620,)\n",
      "[INFO    ] Loaded # of training and testing events: (2249964, 1034656)\n",
      "[WARNING ] The last batch for training could be too few! (2024967%128)=7. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*0.9) % 128\n",
      "[WARNING ] The last batch for training after mixing could be too few! (4049935%128)=15. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*2*0.9) % 128\n"
     ]
    }
   ],
   "source": [
    "# Import muon data\n",
    "# 'x' is the array of input variables, 'y' is the q/pT\n",
    "x_train_displ, x_test_displ, y_train_displ, y_test_displ, dxy_train_displ, dxy_test_displ, dz_train_displ, dz_test_displ, \\\n",
    "w_train_displ, w_test_displ, x_mask_train_displ, x_mask_test_displ, x_road_train_displ, x_road_test_displ = \\\n",
    "      muon_data_split(infile_muon_displ, reg_pt_scale=reg_pt_scale, reg_dxy_scale=reg_dxy_scale, test_size=0.315)\n",
    "\n",
    "y_train_displ = np.abs(y_train_displ)\n",
    "y_test_displ = np.abs(y_test_displ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7af30b",
   "metadata": {},
   "source": [
    "### Pre-Quantization Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6e2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analysing dependencies in the data\n",
    "# x = np.concatenate((x_train_displ,x_test_displ),axis=0)\n",
    "# y = np.concatenate((y_train_displ,y_test_displ),axis=0)\n",
    "# dxy = np.concatenate((dxy_train_displ,dxy_test_displ),axis=0)\n",
    "\n",
    "# df = pd.DataFrame(x, columns = [\"dphi_1\",\"dphi_2\",\"dphi_3\",\"dphi_4\",\"dphi_5\",\"dphi_6\",\n",
    "#                                            \"dtheta_1\",\"dtheta_2\",\"dtheta_3\",\"dtheta_4\",\"dtheta_5\", \"dtheta_6\",\n",
    "#                                            \"bend_1\",\"bend_2\",\"bend_3\",\"bend_4\",\n",
    "#                                            \"FR\",\"track theta\",\"ME11\",\n",
    "#                                            \"RPC_1\",\"RPC_2\",\"RPC_3\",\"RPC_4\"])\n",
    "# df[\"Target momentum\"] = y\n",
    "# df[\"Target dxy\"] = dxy\n",
    "# # C_mat = df[[\"dphi_1\",\"dphi_2\",\"dphi_3\",\"dphi_4\",\"dphi_5\",\"dphi_6\",\n",
    "# #             \"dtheta_1\",\"dtheta_2\",\"dtheta_3\",\"dtheta_4\",\"dtheta_5\", \"dtheta_6\",\n",
    "# #            \"track theta\",\"Target momentum\", \"Target dxy\"]].corr()\n",
    "# C_mat = df.corr()\n",
    "# fig = plt.figure(figsize = (7,7),dpi = 100)\n",
    "# sns.heatmap(C_mat, vmax = .8, square = True,cmap = \"YlGnBu\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccb961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.concatenate((y_train_displ, y_test_displ))\n",
    "# dxy = np.concatenate((dxy_train_displ, dxy_test_displ))\n",
    "# assert(y.shape[0] == dxy.shape[0])\n",
    "# stats = evaluate(x_train_displ,tuple([y,dxy]))\n",
    "# stats.compute_data_statistics(ctype=\"y\",label = \"momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2294c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.compute_data_statistics(ctype=\"dxy\",label=\"dxy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e448f04",
   "metadata": {},
   "source": [
    "### Defining the Quantized Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_quantized(nvariables, lr=0.001, clipnorm=10., initializer = \"he_uniform\", \n",
    "                           nodes1=64, nodes2=32, nodes3=16, outnodes=2):\n",
    "    regularizer = regularizers.L1L2(l1=l1_reg, l2=l2_reg)\n",
    "    bn_momentum = 0.9\n",
    "    eps = 1e-4\n",
    "    x = x_in = Input((nvariables,))\n",
    "    x = QBatchNormalization(epsilon=eps, momentum=bn_momentum,name=\"bn-input\")(x)\n",
    "    \n",
    "    x = QDense(nodes1, \n",
    "               kernel_quantizer=\"quantized_bits(bits=16,integer=6,symmetric=0,use_stochastic_rounding=True,alpha = \\\"auto\\\")\",\n",
    "               kernel_initializer=initializer,\n",
    "               use_bias = True,\n",
    "               kernel_regularizer = regularizer,\n",
    "               name=\"hidden-dense-1\")(x)\n",
    "    x = QBatchNormalization(epsilon=eps, \n",
    "                            momentum=bn_momentum,\n",
    "                            beta_quantizer='quantized_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                            gamma_quantizer='quantized_relu_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                            mean_quantizer='quantized_relu_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                            name=\"bn-1\")(x)\n",
    "    x = QActivation(activation=\"quantized_relu(bits=16,integer=6,use_sigmoid=True,use_stochastic_rounding=True)\",\n",
    "                    name=\"act_1\")(x)\n",
    "    \n",
    "    if nodes2:\n",
    "    \n",
    "        x = QDense(nodes2, \n",
    "                   kernel_quantizer=\"quantized_bits(bits=16,integer=6,symmetric=0,use_stochastic_rounding=True,alpha = \\\"auto\\\")\", \n",
    "                   kernel_initializer=initializer,\n",
    "                   use_bias = True,\n",
    "                   kernel_regularizer = regularizer,\n",
    "                   name=\"hidden-dense-2\")(x)\n",
    "        x = QBatchNormalization(epsilon=eps, \n",
    "                                momentum=bn_momentum,\n",
    "                                beta_quantizer='quantized_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                                gamma_quantizer='quantized_relu_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                                name=\"bn-2\")(x)\n",
    "        x = QActivation(activation=\"quantized_relu(bits=18,integer=9, use_sigmoid=True,use_stochastic_rounding=True)\", \n",
    "                        name=\"act_2\")(x)\n",
    "        \n",
    "        if nodes3:\n",
    "\n",
    "            x = QDense(nodes3, \n",
    "                       kernel_quantizer=\"quantized_bits(bits=16,integer=6,symmetric=0,use_stochastic_rounding=True,alpha = \\\"auto\\\")\", \n",
    "                       kernel_initializer=initializer,\n",
    "                       kernel_regularizer = regularizer,\n",
    "                       use_bias = True,\n",
    "                       name=\"hidden-dense-3\")(x)\n",
    "            x = QBatchNormalization(epsilon=eps, \n",
    "                                    momentum=bn_momentum,\n",
    "                                    beta_quantizer='quantized_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                                    gamma_quantizer='quantized_relu_po2(bits = 6,max_value = 64, use_stochastic_rounding=True)',\n",
    "                                    name=\"bn-3\")(x)\n",
    "            x = QActivation(activation=\"quantized_relu(bits=16,integer=6, use_sigmoid=True,use_stochastic_rounding=True)\", \n",
    "                            name=\"act_3\")(x)\n",
    "\n",
    "    x = QDense(outnodes,\n",
    "                kernel_quantizer=\"quantized_bits(bits=16,integer=6,symmetric=0,keep_negative = False,use_stochastic_rounding=True)\",\n",
    "                kernel_initializer = initializer,\n",
    "                use_bias = True,\n",
    "                name=\"dense-output\")(x)\n",
    "    x = Activation(\"linear\")(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x,name=\"qmodel\")\n",
    "    \n",
    "    adam = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=adam, loss=huber_loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089ee06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Training model with l1_reg: 0.0 l2_reg: 0.0\n",
      "[INFO    ] Begin training ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"qmodel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 23)]              0         \n",
      "_________________________________________________________________\n",
      "bn-input (QBatchNormalizatio (None, 23)                92        \n",
      "_________________________________________________________________\n",
      "hidden-dense-1 (QDense)      (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "bn-1 (QBatchNormalization)   (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "act_1 (QActivation)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "hidden-dense-2 (QDense)      (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "bn-2 (QBatchNormalization)   (None, 15)                60        \n",
      "_________________________________________________________________\n",
      "act_2 (QActivation)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "hidden-dense-3 (QDense)      (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "bn-3 (QBatchNormalization)   (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "act_3 (QActivation)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense-output (QDense)        (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 136\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 8s 3ms/step - loss: 38.1848 - accuracy: 0.8511 - val_loss: 18.2991 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 18.29912, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 18.29912, saving model to model_bchk_weights.h5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 16.1105 - accuracy: 0.9209 - val_loss: 13.1676 - val_accuracy: 0.9297\n",
      "\n",
      "Epoch 00002: val_loss improved from 18.29912 to 13.16763, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 18.29912 to 13.16763, saving model to model_bchk_weights.h5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.5424 - accuracy: 0.9275 - val_loss: 16.1999 - val_accuracy: 0.9215\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 13.16763\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 13.16763\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 13.0582 - accuracy: 0.9276 - val_loss: 11.6426 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00004: val_loss improved from 13.16763 to 11.64262, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 13.16763 to 11.64262, saving model to model_bchk_weights.h5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.3209 - accuracy: 0.9287 - val_loss: 12.1975 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 11.64262\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 11.64262\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.5290 - accuracy: 0.9287 - val_loss: 14.0125 - val_accuracy: 0.9225\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 11.64262\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 11.64262\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.7689 - accuracy: 0.9283 - val_loss: 13.2017 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 11.64262\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 11.64262\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 13.1198 - accuracy: 0.9280 - val_loss: 12.2452 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 11.64262\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 11.64262\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1633 - accuracy: 0.9276 - val_loss: 12.4751 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 11.64262\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 11.64262\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.2470 - accuracy: 0.9279 - val_loss: 11.6014 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00010: val_loss improved from 11.64262 to 11.60143, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 11.64262 to 11.60143, saving model to model_bchk_weights.h5\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8268 - accuracy: 0.9315 - val_loss: 11.5089 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00011: val_loss improved from 11.60143 to 11.50887, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 11.60143 to 11.50887, saving model to model_bchk_weights.h5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6307 - accuracy: 0.9325 - val_loss: 13.9710 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 11.50887\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 11.50887\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8878 - accuracy: 0.9316 - val_loss: 11.2190 - val_accuracy: 0.9347\n",
      "\n",
      "Epoch 00013: val_loss improved from 11.50887 to 11.21898, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 11.50887 to 11.21898, saving model to model_bchk_weights.h5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7808 - accuracy: 0.9310 - val_loss: 13.3142 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 11.21898\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 11.21898\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.0709 - accuracy: 0.9306 - val_loss: 11.6840 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 11.21898\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 11.21898\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.6422 - accuracy: 0.9315 - val_loss: 11.7550 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 11.21898\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 11.21898\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.7455 - accuracy: 0.9311 - val_loss: 11.0344 - val_accuracy: 0.9356\n",
      "\n",
      "Epoch 00017: val_loss improved from 11.21898 to 11.03439, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 11.21898 to 11.03439, saving model to model_bchk_weights.h5\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8106 - accuracy: 0.9322 - val_loss: 12.8093 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 11.03439\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.0369 - accuracy: 0.9292 - val_loss: 11.3131 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 11.03439\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0009000000427477062.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.6203 - accuracy: 0.9309 - val_loss: 11.0632 - val_accuracy: 0.9349\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 11.03439\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0008100000384729356.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.5174 - accuracy: 0.9313 - val_loss: 11.4964 - val_accuracy: 0.9309\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 11.03439\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.6582 - accuracy: 0.9313 - val_loss: 11.1193 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 11.03439\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.7614 - accuracy: 0.9307 - val_loss: 14.4258 - val_accuracy: 0.9295\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 11.03439\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.2984 - accuracy: 0.9288 - val_loss: 12.1284 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 11.03439\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1426 - accuracy: 0.9289 - val_loss: 12.4972 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 11.03439\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.2055 - accuracy: 0.9287 - val_loss: 11.6762 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 11.03439\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1010 - accuracy: 0.9295 - val_loss: 11.2940 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 11.03439\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8856 - accuracy: 0.9311 - val_loss: 12.2051 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 11.03439\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.3092 - accuracy: 0.9295 - val_loss: 12.4925 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 11.03439\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0008100000559352338.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8348 - accuracy: 0.9299 - val_loss: 12.1542 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 11.03439\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0007290000503417104.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9922 - accuracy: 0.9298 - val_loss: 11.3189 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 11.03439\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9836 - accuracy: 0.9302 - val_loss: 11.3022 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 11.03439\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6331 - accuracy: 0.9318 - val_loss: 11.6738 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 11.03439\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5303 - accuracy: 0.9324 - val_loss: 11.0871 - val_accuracy: 0.9351\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 11.03439\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 11.03439\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5646 - accuracy: 0.9321 - val_loss: 10.8942 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00035: val_loss improved from 11.03439 to 10.89422, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 11.03439 to 10.89422, saving model to model_bchk_weights.h5\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6186 - accuracy: 0.9321 - val_loss: 11.1741 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 10.89422\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 10.89422\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.5923 - accuracy: 0.9320 - val_loss: 11.0575 - val_accuracy: 0.9357\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 10.89422\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 10.89422\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.6581 - accuracy: 0.9321 - val_loss: 11.1839 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 10.89422\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 10.89422\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.5409 - accuracy: 0.9322 - val_loss: 11.1696 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 10.89422\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 10.89422\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0007290000794455409.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5067 - accuracy: 0.9323 - val_loss: 11.8356 - val_accuracy: 0.9355\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 10.89422\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 10.89422\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0006561000715009868.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4660 - accuracy: 0.9324 - val_loss: 10.8776 - val_accuracy: 0.9364\n",
      "\n",
      "Epoch 00041: val_loss improved from 10.89422 to 10.87763, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 10.89422 to 10.87763, saving model to model_bchk_weights.h5\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.4575 - accuracy: 0.9327 - val_loss: 10.9436 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 10.87763\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.4424 - accuracy: 0.9329 - val_loss: 10.9910 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 10.87763\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.3860 - accuracy: 0.9334 - val_loss: 10.9900 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 10.87763\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.3369 - accuracy: 0.9333 - val_loss: 11.0634 - val_accuracy: 0.9347\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 10.87763\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.3189 - accuracy: 0.9332 - val_loss: 10.9769 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 10.87763\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.5005 - accuracy: 0.9323 - val_loss: 11.5180 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 10.87763\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9581 - accuracy: 0.9302 - val_loss: 12.3458 - val_accuracy: 0.9315\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 10.87763\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7553 - accuracy: 0.9316 - val_loss: 11.6687 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 10.87763\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0006561000482179224.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8581 - accuracy: 0.9311 - val_loss: 13.7048 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 10.87763\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0005904900433961303.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.4881 - accuracy: 0.9317 - val_loss: 11.6102 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 10.87763\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.4264 - accuracy: 0.9302 - val_loss: 13.0009 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 10.87763\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.5513 - accuracy: 0.9309 - val_loss: 11.9972 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 10.87763\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8096 - accuracy: 0.9319 - val_loss: 11.2532 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 10.87763\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9410 - accuracy: 0.9315 - val_loss: 11.7001 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 10.87763\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9691 - accuracy: 0.9321 - val_loss: 15.2516 - val_accuracy: 0.8980\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 10.87763\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.7970 - accuracy: 0.9327 - val_loss: 11.3511 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 10.87763\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9294 - accuracy: 0.9312 - val_loss: 14.5332 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 10.87763\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.3826 - accuracy: 0.9299 - val_loss: 11.9684 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 10.87763\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0005904900608584285.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.2343 - accuracy: 0.9294 - val_loss: 12.4793 - val_accuracy: 0.9295\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 10.87763\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0005314410547725857.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.2085 - accuracy: 0.9292 - val_loss: 12.8684 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 10.87763\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.6891 - accuracy: 0.9272 - val_loss: 11.1060 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 10.87763\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1355 - accuracy: 0.9314 - val_loss: 14.4933 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 10.87763\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.0409 - accuracy: 0.9313 - val_loss: 12.3179 - val_accuracy: 0.9323\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 10.87763\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9187 - accuracy: 0.9318 - val_loss: 12.2593 - val_accuracy: 0.9262\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 10.87763\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1174 - accuracy: 0.9311 - val_loss: 11.5880 - val_accuracy: 0.9342\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 10.87763\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1257 - accuracy: 0.9312 - val_loss: 16.2555 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 10.87763\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.5681 - accuracy: 0.9294 - val_loss: 14.1417 - val_accuracy: 0.9252\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 10.87763\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.4036 - accuracy: 0.9300 - val_loss: 12.5724 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 10.87763\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0005314410664141178.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1167 - accuracy: 0.9300 - val_loss: 11.7437 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 10.87763\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00047829695977270604.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.2533 - accuracy: 0.9284 - val_loss: 11.8643 - val_accuracy: 0.9334\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 10.87763\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.0646 - accuracy: 0.9299 - val_loss: 11.3643 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 10.87763\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.3581 - accuracy: 0.9273 - val_loss: 11.6147 - val_accuracy: 0.9312\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 10.87763\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.0594 - accuracy: 0.9277 - val_loss: 13.3994 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 10.87763\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 12.1207 - accuracy: 0.9285 - val_loss: 11.9455 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 10.87763\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9857 - accuracy: 0.9301 - val_loss: 12.0522 - val_accuracy: 0.9309\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 10.87763\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8611 - accuracy: 0.9304 - val_loss: 12.4308 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 10.87763\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9521 - accuracy: 0.9298 - val_loss: 12.3055 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 10.87763\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.8956 - accuracy: 0.9300 - val_loss: 10.9672 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 10.87763\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00047829694813117385.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.6765 - accuracy: 0.9310 - val_loss: 12.1494 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 10.87763\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0004304672533180565.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7099 - accuracy: 0.9305 - val_loss: 11.3851 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 10.87763\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.6863 - accuracy: 0.9311 - val_loss: 11.5343 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 10.87763\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.9055 - accuracy: 0.9309 - val_loss: 10.9458 - val_accuracy: 0.9355\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 10.87763\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6923 - accuracy: 0.9320 - val_loss: 11.3116 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 10.87763\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.7090 - accuracy: 0.9319 - val_loss: 11.3409 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 10.87763\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6864 - accuracy: 0.9323 - val_loss: 11.1326 - val_accuracy: 0.9349\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 10.87763\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7400 - accuracy: 0.9318 - val_loss: 11.6671 - val_accuracy: 0.9292\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 10.87763\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7523 - accuracy: 0.9315 - val_loss: 11.7003 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 10.87763\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9159 - accuracy: 0.9300 - val_loss: 11.3910 - val_accuracy: 0.9306\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 10.87763\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.00043046724749729037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6878 - accuracy: 0.9315 - val_loss: 11.3768 - val_accuracy: 0.9314\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 10.87763\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00038742052274756136.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6317 - accuracy: 0.9314 - val_loss: 11.7906 - val_accuracy: 0.9241\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 10.87763\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6385 - accuracy: 0.9314 - val_loss: 11.3187 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 10.87763\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7250 - accuracy: 0.9318 - val_loss: 11.2942 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 10.87763\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 2ms/step - loss: 11.9809 - accuracy: 0.9296 - val_loss: 13.7463 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 10.87763\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.3122 - accuracy: 0.9295 - val_loss: 11.7835 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 10.87763\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.1461 - accuracy: 0.9305 - val_loss: 12.0975 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 10.87763\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0665 - accuracy: 0.9310 - val_loss: 11.4693 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 10.87763\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0626 - accuracy: 0.9302 - val_loss: 13.1746 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 10.87763\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 13.1586 - accuracy: 0.9260 - val_loss: 12.9066 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 10.87763\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0003874205285683274.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.6641 - accuracy: 0.9265 - val_loss: 13.3036 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 10.87763\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0003486784757114947.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 13.0403 - accuracy: 0.9245 - val_loss: 13.4100 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 10.87763\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.5425 - accuracy: 0.9277 - val_loss: 12.8130 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 10.87763\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.2451 - accuracy: 0.9287 - val_loss: 12.1453 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 10.87763\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.1080 - accuracy: 0.9295 - val_loss: 11.6347 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 10.87763\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9505 - accuracy: 0.9305 - val_loss: 11.7382 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 10.87763\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.1534 - accuracy: 0.9290 - val_loss: 12.3216 - val_accuracy: 0.9256\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 10.87763\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.3254 - accuracy: 0.9279 - val_loss: 12.8119 - val_accuracy: 0.9307\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 10.87763\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.2704 - accuracy: 0.9286 - val_loss: 11.6877 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 10.87763\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0497 - accuracy: 0.9298 - val_loss: 12.6735 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 10.87763\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0003486784698907286.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9284 - accuracy: 0.9292 - val_loss: 11.2462 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 10.87763\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.00031381062290165574.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7640 - accuracy: 0.9300 - val_loss: 11.4939 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 10.87763\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8711 - accuracy: 0.9300 - val_loss: 13.5068 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 10.87763\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0103 - accuracy: 0.9302 - val_loss: 11.6526 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 10.87763\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9381 - accuracy: 0.9307 - val_loss: 13.8499 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 10.87763\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0954 - accuracy: 0.9308 - val_loss: 12.6045 - val_accuracy: 0.9272\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 10.87763\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0047 - accuracy: 0.9315 - val_loss: 11.5811 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 10.87763\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0005 - accuracy: 0.9309 - val_loss: 11.8310 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 10.87763\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7980 - accuracy: 0.9318 - val_loss: 11.6405 - val_accuracy: 0.9324\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 10.87763\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7554 - accuracy: 0.9320 - val_loss: 11.4986 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 10.87763\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0003138106258120388.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9049 - accuracy: 0.9300 - val_loss: 11.4002 - val_accuracy: 0.9314\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 10.87763\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002824295632308349.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7876 - accuracy: 0.9306 - val_loss: 11.2525 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 10.87763\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6245 - accuracy: 0.9317 - val_loss: 11.0452 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 10.87763\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5049 - accuracy: 0.9329 - val_loss: 10.9374 - val_accuracy: 0.9362\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 10.87763\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5027 - accuracy: 0.9330 - val_loss: 13.8876 - val_accuracy: 0.9188\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 10.87763\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7135 - accuracy: 0.9314 - val_loss: 11.8690 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 10.87763\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8875 - accuracy: 0.9294 - val_loss: 11.4529 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 10.87763\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8043 - accuracy: 0.9294 - val_loss: 11.7911 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 10.87763\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8287 - accuracy: 0.9303 - val_loss: 12.0893 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 10.87763\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8486 - accuracy: 0.9305 - val_loss: 13.1349 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 10.87763\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.00028242956614121795.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8608 - accuracy: 0.9311 - val_loss: 12.6515 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 10.87763\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.00025418660952709616.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.7558 - accuracy: 0.9315 - val_loss: 11.2788 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 10.87763\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7032 - accuracy: 0.9321 - val_loss: 12.1704 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 10.87763\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8302 - accuracy: 0.9318 - val_loss: 11.8784 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 10.87763\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8238 - accuracy: 0.9321 - val_loss: 11.1055 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 10.87763\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7425 - accuracy: 0.9323 - val_loss: 11.4751 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 10.87763\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5686 - accuracy: 0.9323 - val_loss: 11.1299 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 10.87763\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5356 - accuracy: 0.9331 - val_loss: 11.0403 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 10.87763\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5230 - accuracy: 0.9326 - val_loss: 11.0152 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 10.87763\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5179 - accuracy: 0.9332 - val_loss: 11.2944 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 10.87763\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.00025418659788556397.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5292 - accuracy: 0.9330 - val_loss: 10.9857 - val_accuracy: 0.9353\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 10.87763\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5227 - accuracy: 0.9331 - val_loss: 11.5213 - val_accuracy: 0.9312\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 10.87763\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 10.87763\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5473 - accuracy: 0.9322 - val_loss: 10.8339 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00142: val_loss improved from 10.87763 to 10.83390, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00142: val_loss improved from 10.87763 to 10.83390, saving model to model_bchk_weights.h5\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5448 - accuracy: 0.9322 - val_loss: 11.2504 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 10.83390\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5376 - accuracy: 0.9325 - val_loss: 11.7863 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 10.83390\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5684 - accuracy: 0.9322 - val_loss: 11.0856 - val_accuracy: 0.9357\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 10.83390\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5813 - accuracy: 0.9321 - val_loss: 11.3821 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 10.83390\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5674 - accuracy: 0.9318 - val_loss: 11.3833 - val_accuracy: 0.9347\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 10.83390\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6298 - accuracy: 0.9314 - val_loss: 12.0630 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 10.83390\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6549 - accuracy: 0.9312 - val_loss: 12.1236 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 10.83390\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.00022876793809700757.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8305 - accuracy: 0.9300 - val_loss: 11.3993 - val_accuracy: 0.9351\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 10.83390\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.00020589114428730683.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7458 - accuracy: 0.9307 - val_loss: 11.2119 - val_accuracy: 0.9345\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 10.83390\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6707 - accuracy: 0.9312 - val_loss: 11.4864 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 10.83390\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6692 - accuracy: 0.9312 - val_loss: 11.4575 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 10.83390\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5778 - accuracy: 0.9319 - val_loss: 11.1310 - val_accuracy: 0.9291\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 10.83390\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4877 - accuracy: 0.9329 - val_loss: 10.9105 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 10.83390\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4677 - accuracy: 0.9322 - val_loss: 11.0555 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 10.83390\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5040 - accuracy: 0.9323 - val_loss: 11.3019 - val_accuracy: 0.9351\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 10.83390\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4938 - accuracy: 0.9326 - val_loss: 11.0706 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 10.83390\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 10.83390\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4582 - accuracy: 0.9327 - val_loss: 10.7751 - val_accuracy: 0.9360\n",
      "\n",
      "Epoch 00159: val_loss improved from 10.83390 to 10.77507, saving model to model_bchk.h5\n",
      "\n",
      "Epoch 00159: val_loss improved from 10.83390 to 10.77507, saving model to model_bchk_weights.h5\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.00020589114865288138.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4923 - accuracy: 0.9328 - val_loss: 11.0401 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 10.77507\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.00018530203378759326.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.4823 - accuracy: 0.9325 - val_loss: 11.2631 - val_accuracy: 0.9345\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 10.77507\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5945 - accuracy: 0.9327 - val_loss: 11.3876 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 10.77507\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6270 - accuracy: 0.9318 - val_loss: 11.0414 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 10.77507\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.4488 - accuracy: 0.9321 - val_loss: 11.6486 - val_accuracy: 0.9296\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 10.77507\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.5204 - accuracy: 0.9321 - val_loss: 10.9863 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 10.77507\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 7s 3ms/step - loss: 11.4590 - accuracy: 0.9327 - val_loss: 11.2106 - val_accuracy: 0.9313\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 10.77507\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 7s 3ms/step - loss: 11.5992 - accuracy: 0.9316 - val_loss: 11.0613 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 10.77507\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 9s 4ms/step - loss: 11.6226 - accuracy: 0.9320 - val_loss: 11.7079 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 10.77507\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 7s 4ms/step - loss: 11.6239 - accuracy: 0.9318 - val_loss: 10.8560 - val_accuracy: 0.9355\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 10.77507\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001853020366979763.\n",
      "2025/2025 [==============================] - 7s 4ms/step - loss: 11.5297 - accuracy: 0.9324 - val_loss: 10.8018 - val_accuracy: 0.9361\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 10.77507\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.00016677183302817866.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.4300 - accuracy: 0.9326 - val_loss: 11.0510 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 10.77507\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.4631 - accuracy: 0.9325 - val_loss: 11.5492 - val_accuracy: 0.9309\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 10.77507\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6565 - accuracy: 0.9323 - val_loss: 11.2195 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 10.77507\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5621 - accuracy: 0.9324 - val_loss: 10.8877 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 10.77507\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.7119 - accuracy: 0.9317 - val_loss: 12.4328 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 10.77507\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8471 - accuracy: 0.9308 - val_loss: 11.5892 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 10.77507\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9418 - accuracy: 0.9312 - val_loss: 11.8169 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 10.77507\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.2543 - accuracy: 0.9288 - val_loss: 11.2327 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 10.77507\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8208 - accuracy: 0.9309 - val_loss: 11.9976 - val_accuracy: 0.9191\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 10.77507\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.00016677183157298714.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7651 - accuracy: 0.9306 - val_loss: 11.1726 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 10.77507\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.00015009464841568844.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7242 - accuracy: 0.9300 - val_loss: 11.5224 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 10.77507\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7661 - accuracy: 0.9300 - val_loss: 11.6128 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 10.77507\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7834 - accuracy: 0.9301 - val_loss: 11.1063 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 10.77507\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8361 - accuracy: 0.9303 - val_loss: 11.9812 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 10.77507\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0653 - accuracy: 0.9298 - val_loss: 11.7888 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 10.77507\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9860 - accuracy: 0.9289 - val_loss: 11.6384 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 10.77507\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9599 - accuracy: 0.9286 - val_loss: 11.7764 - val_accuracy: 0.9295\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 10.77507\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9094 - accuracy: 0.9290 - val_loss: 11.8107 - val_accuracy: 0.9271\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 10.77507\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0312 - accuracy: 0.9296 - val_loss: 11.2823 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 10.77507\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.000150094652781263.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0881 - accuracy: 0.9292 - val_loss: 14.1221 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 10.77507\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001350851875031367.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 13.2241 - accuracy: 0.9302 - val_loss: 12.5979 - val_accuracy: 0.9315\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 10.77507\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.5148 - accuracy: 0.9285 - val_loss: 13.1669 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 10.77507\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.3020 - accuracy: 0.9281 - val_loss: 11.4551 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 10.77507\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0723 - accuracy: 0.9287 - val_loss: 12.1566 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 10.77507\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9281 - accuracy: 0.9292 - val_loss: 11.4038 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 10.77507\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8202 - accuracy: 0.9302 - val_loss: 11.2222 - val_accuracy: 0.9341\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 10.77507\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7780 - accuracy: 0.9302 - val_loss: 11.6277 - val_accuracy: 0.9333\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 10.77507\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7303 - accuracy: 0.9305 - val_loss: 11.0590 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 10.77507\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6808 - accuracy: 0.9309 - val_loss: 11.0238 - val_accuracy: 0.9341\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 10.77507\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001350851816823706.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6412 - accuracy: 0.9307 - val_loss: 12.2556 - val_accuracy: 0.9225\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 10.77507\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.00012157666351413355.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.6631 - accuracy: 0.9309 - val_loss: 11.7910 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 10.77507\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6213 - accuracy: 0.9308 - val_loss: 11.7224 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 10.77507\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6209 - accuracy: 0.9305 - val_loss: 11.2609 - val_accuracy: 0.9323\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 10.77507\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6164 - accuracy: 0.9315 - val_loss: 11.2299 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 10.77507\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6093 - accuracy: 0.9315 - val_loss: 11.6168 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 10.77507\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6294 - accuracy: 0.9314 - val_loss: 12.4363 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 10.77507\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6384 - accuracy: 0.9303 - val_loss: 10.9651 - val_accuracy: 0.9335\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 10.77507\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6794 - accuracy: 0.9302 - val_loss: 11.6482 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 10.77507\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6634 - accuracy: 0.9307 - val_loss: 11.5623 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 10.77507\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0001215766606037505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5864 - accuracy: 0.9305 - val_loss: 11.1222 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 10.77507\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.00010941899454337544.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6282 - accuracy: 0.9309 - val_loss: 13.4399 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 10.77507\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7434 - accuracy: 0.9299 - val_loss: 11.1436 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 10.77507\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8177 - accuracy: 0.9302 - val_loss: 11.8827 - val_accuracy: 0.9252\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 10.77507\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8185 - accuracy: 0.9301 - val_loss: 11.4339 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 10.77507\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8276 - accuracy: 0.9302 - val_loss: 11.1955 - val_accuracy: 0.9345\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 10.77507\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8626 - accuracy: 0.9303 - val_loss: 11.1575 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 10.77507\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8497 - accuracy: 0.9304 - val_loss: 11.5593 - val_accuracy: 0.9312\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 10.77507\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8460 - accuracy: 0.9307 - val_loss: 11.7842 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 10.77507\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8179 - accuracy: 0.9308 - val_loss: 11.1852 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 10.77507\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.00010941899381577969.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7808 - accuracy: 0.9308 - val_loss: 11.3293 - val_accuracy: 0.9349\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 10.77507\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 9.847709443420172e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7604 - accuracy: 0.9312 - val_loss: 11.4109 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 10.77507\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7422 - accuracy: 0.9312 - val_loss: 11.9791 - val_accuracy: 0.9324\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 10.77507\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7687 - accuracy: 0.9314 - val_loss: 11.9248 - val_accuracy: 0.9296\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 10.77507\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8104 - accuracy: 0.9314 - val_loss: 11.4643 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 10.77507\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8168 - accuracy: 0.9314 - val_loss: 11.2976 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 10.77507\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8466 - accuracy: 0.9310 - val_loss: 11.4110 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 10.77507\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7689 - accuracy: 0.9312 - val_loss: 11.2264 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 10.77507\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7735 - accuracy: 0.9309 - val_loss: 12.0136 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 10.77507\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7543 - accuracy: 0.9311 - val_loss: 12.3890 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 10.77507\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 9.847709588939324e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6428 - accuracy: 0.9311 - val_loss: 11.0569 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 10.77507\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 8.862938630045391e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5938 - accuracy: 0.9315 - val_loss: 12.1221 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 10.77507\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5625 - accuracy: 0.9313 - val_loss: 11.3665 - val_accuracy: 0.9312\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 10.77507\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5455 - accuracy: 0.9316 - val_loss: 11.4472 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 10.77507\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5013 - accuracy: 0.9320 - val_loss: 11.2710 - val_accuracy: 0.9311\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 10.77507\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4944 - accuracy: 0.9317 - val_loss: 10.7779 - val_accuracy: 0.9356\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 10.77507\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.5091 - accuracy: 0.9313 - val_loss: 11.5777 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 10.77507\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4617 - accuracy: 0.9320 - val_loss: 10.9419 - val_accuracy: 0.9353\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 10.77507\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.4881 - accuracy: 0.9316 - val_loss: 11.4942 - val_accuracy: 0.9314\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 10.77507\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.4296 - accuracy: 0.9319 - val_loss: 11.4073 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 10.77507\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 8.862938557285815e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.2712 - accuracy: 0.9297 - val_loss: 11.8135 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 10.77507\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.3738 - accuracy: 0.9295 - val_loss: 11.6225 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 10.77507\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.2630 - accuracy: 0.9295 - val_loss: 12.1378 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 10.77507\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.1808 - accuracy: 0.9302 - val_loss: 11.5907 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 10.77507\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0904 - accuracy: 0.9305 - val_loss: 11.6123 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 10.77507\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 12.0883 - accuracy: 0.9308 - val_loss: 11.8626 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 10.77507\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9951 - accuracy: 0.9308 - val_loss: 11.5407 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 10.77507\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9519 - accuracy: 0.9305 - val_loss: 11.5044 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 10.77507\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.9144 - accuracy: 0.9303 - val_loss: 11.1816 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 10.77507\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8338 - accuracy: 0.9303 - val_loss: 11.6733 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 10.77507\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 7.976644701557234e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8362 - accuracy: 0.9306 - val_loss: 11.1025 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 10.77507\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 7.178980231401511e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8138 - accuracy: 0.9302 - val_loss: 11.4769 - val_accuracy: 0.9270\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 10.77507\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7968 - accuracy: 0.9310 - val_loss: 12.0840 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 10.77507\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8285 - accuracy: 0.9313 - val_loss: 11.7134 - val_accuracy: 0.9323\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 10.77507\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7759 - accuracy: 0.9303 - val_loss: 11.1263 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 10.77507\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7268 - accuracy: 0.9308 - val_loss: 11.0480 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 10.77507\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7140 - accuracy: 0.9309 - val_loss: 10.9276 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 10.77507\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6586 - accuracy: 0.9311 - val_loss: 11.4583 - val_accuracy: 0.9307\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 10.77507\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6718 - accuracy: 0.9308 - val_loss: 11.2280 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 10.77507\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.6912 - accuracy: 0.9309 - val_loss: 11.1433 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 10.77507\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 7.178980013122782e-05.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.6445 - accuracy: 0.9312 - val_loss: 11.0005 - val_accuracy: 0.9338\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 10.77507\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 6.461082011810504e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6468 - accuracy: 0.9311 - val_loss: 11.0679 - val_accuracy: 0.9341\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 10.77507\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6793 - accuracy: 0.9312 - val_loss: 11.7716 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 10.77507\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6341 - accuracy: 0.9315 - val_loss: 11.4810 - val_accuracy: 0.9313\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 10.77507\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 7s 3ms/step - loss: 11.7850 - accuracy: 0.9309 - val_loss: 11.4994 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 10.77507\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.8771 - accuracy: 0.9307 - val_loss: 11.8117 - val_accuracy: 0.9282\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 10.77507\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8359 - accuracy: 0.9311 - val_loss: 11.6234 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 10.77507\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7397 - accuracy: 0.9315 - val_loss: 11.2850 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 10.77507\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7925 - accuracy: 0.9310 - val_loss: 11.3438 - val_accuracy: 0.9325\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 10.77507\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7135 - accuracy: 0.9314 - val_loss: 11.0615 - val_accuracy: 0.9333\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 10.77507\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 6.461082375608385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.8116 - accuracy: 0.9304 - val_loss: 11.4240 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 10.77507\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 5.8149741380475466e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7630 - accuracy: 0.9309 - val_loss: 11.1387 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 10.77507\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7219 - accuracy: 0.9313 - val_loss: 11.6250 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 10.77507\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6700 - accuracy: 0.9317 - val_loss: 11.2853 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 10.77507\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.7621 - accuracy: 0.9311 - val_loss: 11.1551 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 10.77507\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6999 - accuracy: 0.9312 - val_loss: 11.1589 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 10.77507\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6783 - accuracy: 0.9317 - val_loss: 11.1271 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 10.77507\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6816 - accuracy: 0.9315 - val_loss: 10.9605 - val_accuracy: 0.9353\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 10.77507\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6513 - accuracy: 0.9322 - val_loss: 10.8818 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 10.77507\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6889 - accuracy: 0.9317 - val_loss: 11.3084 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 10.77507\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 5.81497406528797e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6464 - accuracy: 0.9314 - val_loss: 10.9680 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 10.77507\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 5.233476658759173e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6305 - accuracy: 0.9320 - val_loss: 10.9411 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 10.77507\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6627 - accuracy: 0.9316 - val_loss: 11.2324 - val_accuracy: 0.9333\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 10.77507\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "2025/2025 [==============================] - 5s 3ms/step - loss: 11.6431 - accuracy: 0.9315 - val_loss: 12.0531 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 10.77507\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      "2025/2025 [==============================] - 6s 3ms/step - loss: 11.6471 - accuracy: 0.9317 - val_loss: 11.8441 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 10.77507\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 10.77507\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 5.233476622379385e-05.\n",
      " 221/2025 [==>...........................] - ETA: 4s - loss: 11.6026 - accuracy: 0.9322"
     ]
    }
   ],
   "source": [
    "assert(keras.backend.backend() == 'tensorflow')\n",
    "\n",
    "# Bits = 24, Int = 12\n",
    "# normal_epochs = 300\n",
    "# normal_batch_size = 1000\n",
    "# l1_reg = 1e-5\n",
    "# l2_reg = 0.0\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# Bits = 18, Int = 12\n",
    "# normal_epochs = 300\n",
    "# normal_batch_size = 500\n",
    "# l1_reg = 1e-2\n",
    "# l2_reg = 0.0\n",
    "# learning_rate = 5e-3\n",
    "\n",
    "# Bits = 18, Int = 6\n",
    "# normal_epochs = 300\n",
    "# normal_batch_size = 500\n",
    "# l1_reg = 1e-5\n",
    "# l2_reg = 0.0\n",
    "# learning_rate = 5e-3\n",
    "\n",
    "# Bits = 12, Int = 6\n",
    "# normal_epochs = 300\n",
    "# normal_batch_size = 1000\n",
    "# l1_reg = 1e-3\n",
    "# l2_reg = 0.0\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# Bits = 10, Int = 5\n",
    "normal_epochs = 300\n",
    "normal_batch_size = 1000\n",
    "l1_reg = 0.0\n",
    "l2_reg = 0.0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "qmodel = create_model_quantized(nvariables=nvariables, lr=learning_rate, clipnorm=gradient_clip_norm, \n",
    "                              nodes1=20, nodes2=15, nodes3=10, outnodes=2)\n",
    "logger.info('Training model with l1_reg: {0} l2_reg: {1}'.format(l1_reg, l2_reg))\n",
    "\n",
    "history = train_model(qmodel, x_train_displ, np.column_stack((y_train_displ, dxy_train_displ)),\n",
    "                    save_model=False, epochs=normal_epochs, batch_size=normal_batch_size,\n",
    "#                     callbacks=[lr_decay,modelbestcheck,modelbestcheck_weights], \n",
    "                    validation_split=0.1, verbose=True)\n",
    "\n",
    "metrics = [len(history.history['loss']), history.history['loss'][-1], history.history['val_loss'][-1]]\n",
    "logger.info('Epoch {0}/{0} - loss: {1} - val_loss: {2}'.format(*metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_train_displ, x_test_displ),axis = 0)\n",
    "y = np.concatenate((y_train_displ, y_test_displ),axis = 0)\n",
    "dxy = np.concatenate((dxy_train_displ, dxy_test_displ),axis = 0)\n",
    "k_fold_validation(model = qmodel, \n",
    "                  x = x,  \n",
    "                  y = y, \n",
    "                  dxy = dxy, \n",
    "                  folds =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e883e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5),dpi = 70)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('quantized model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\"retrain epochs\":normal_epochs, \n",
    "                   \"batch size\"    :normal_batch_size, \n",
    "                   \"learning rate\" :learning_rate,\n",
    "                   \"BN momentum\"   :0.9,\n",
    "                   \"BN epsilon\"    :1e-4,\n",
    "                   \"L1 Reg\"        :l1_reg,\n",
    "                   \"L2 Reg\"        :l2_reg\n",
    "                  }\n",
    "df = pd.DataFrame(training_params.items(), \n",
    "                  columns=['Training Parameters', 'Value'],\n",
    "                  index = [i+1 for i in range(len(training_params))])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e882086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __generate_delta_plots__(model,\n",
    "                             x,\n",
    "                             y,\n",
    "                             dxy,batch_size = 4096):\n",
    "    \n",
    "    # Predictions\n",
    "    y_test_true = y.copy()\n",
    "    y_test_true /= reg_pt_scale\n",
    "\n",
    "    y_test_sel = (np.abs(1.0/y) >= 20./reg_pt_scale)\n",
    "\n",
    "    y_test_meas_ = model.predict(x, batch_size=4096)\n",
    "    y_test_meas = y_test_meas_[:,0]\n",
    "    y_test_meas /= reg_pt_scale\n",
    "    y_test_meas = y_test_meas.reshape(-1)\n",
    "\n",
    "    dxy_test_true = dxy.copy()\n",
    "    dxy_test_true /= reg_dxy_scale\n",
    "\n",
    "    dxy_test_sel = (np.abs(dxy_test_true) >= 25)\n",
    "\n",
    "    dxy_test_meas = y_test_meas_[:,1]\n",
    "    dxy_test_meas /= reg_dxy_scale\n",
    "    dxy_test_meas = dxy_test_meas.reshape(-1)\n",
    "    \n",
    "    # Plot Delta(q/pT)/pT\n",
    "    plt.figure(figsize=(5,5),dpi = 75)\n",
    "    yy = ((np.abs(1.0/y_test_meas) - np.abs(1.0/y_test_true))/np.abs(1.0/y_test_true))\n",
    "    hist, edges, _ = plt.hist(yy, bins=100, range=(-2.0,2.0-eps), histtype='stepfilled', facecolor='c', alpha=0.6)\n",
    "    plt.xlabel(r'$\\Delta(p_{T})_{\\mathrm{meas-true}}/{(p_{T})}_{true}$ [1/GeV]')\n",
    "    plt.ylabel(r'entries')\n",
    "    logger.info('# of entries: {0}, mean: {1}, std: {2}'.format(len(yy), np.mean(yy), np.std(yy[np.abs(yy)<0.4])))\n",
    "\n",
    "    popt = fit_gaus(hist, edges, mu=np.mean(yy), sig=np.std(yy[np.abs(yy)<2.0]))\n",
    "    logger.info('gaus fit (a, mu, sig): {0}'.format(popt))\n",
    "    xdata = (edges[1:] + edges[:-1])/2\n",
    "    plt.plot(xdata, gaus(xdata, popt[0], popt[1], popt[2]), color='c')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Delta(dxy)\n",
    "    plt.figure(figsize=(5,5),dpi = 75)\n",
    "    yy = (dxy_test_meas - dxy_test_true)[y_test_sel]\n",
    "    hist, edges, _ = plt.hist(yy, bins=100, range=(-50,50), histtype='stepfilled', facecolor='c', alpha=0.6)\n",
    "    plt.xlabel(r'$\\Delta(d_{0})_{\\mathrm{meas-true}}$ [cm]')\n",
    "    plt.ylabel(r'entries')\n",
    "    logger.info('# of entries: {0}, mean: {1}, std: {2}'.format(len(yy), np.mean(yy), np.std(yy)))\n",
    "\n",
    "    popt = fit_gaus(hist, edges, mu=np.mean(yy), sig=np.std(yy))\n",
    "    logger.info('gaus fit (a, mu, sig): {0}'.format(popt))\n",
    "    xdata = (edges[1:] + edges[:-1])/2\n",
    "    plt.plot(xdata, gaus(xdata, popt[0], popt[1], popt[2]), color='c')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b141db",
   "metadata": {},
   "outputs": [],
   "source": [
    "__generate_delta_plots__(model = qmodel,\n",
    "                        x = x_test_displ,\n",
    "                        y = y_test_displ,\n",
    "                        dxy = dxy_test_displ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff86047",
   "metadata": {},
   "source": [
    "### Saving the optimal quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_for_export = qmodel\n",
    "# qkeras_file = \"model/quantized_DNN.h5\"\n",
    "# weights_file = \"model/quantized_DNN_weights\"\n",
    "# # save quantized weights\n",
    "# utils.model_save_quantized_weights(model_for_export, weights_file)\n",
    "# # save keras.save() h5 version of quantized model\n",
    "# tf.keras.models.save_model(model_for_export, qkeras_file, include_optimizer=False)\n",
    "# print('Saved quantized Keras model to:', qkeras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb56ad",
   "metadata": {},
   "source": [
    "### Loading the quantized model for assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qkeras_file = \"model/quantized_DNN.h5\"\n",
    "# weights_file = \"model/quantized_DNN_weights\"\n",
    "# loaded_qmodel = utils.load_qmodel(filepath=qkeras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_qmodel.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39601728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_qmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f38429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.get_model_sparsity(model=loaded_qmodel,per_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "## END ############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
